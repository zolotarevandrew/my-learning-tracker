# Learning log

|Date |                                        |
|:---:|:---------------------------------------|
|     |Learnt, thoughts, progress, ideas, links|
---------------------------------------------------------
## 13 may 23
**Team lead - прозрачность**
Связь с командой и окружающим миром – руководством, соседними отделами, всей компанией. 
Это не значит, что члены команды не должны самостоятельно общаться с кем-то из перечисленных, 
это значит, что на плечах тимлида лежит обязанность любым способом обеспечить нужную прозрачность. 
Все направление по обеспечению прозрачности можно разбить на три большие части:
- Отчётность перед руководством
- Обеспечение видимости достижений членов команды
- Информирование команды об изменениях и новостях

Предоставление отчётности может принимать очень разный вид: 
формальные письменные репорты, регулярные демо команды, быстрый статус-чек на one-on-one встречах, дэшборды в Jira. 
Самый простой способ определиться с форматом – обсудить его со своим руководителем, выделить волнующие его вопросы и определить удобную для всех периодичность.

Форматов обеспечения видимости достижений команды тоже может быть много – открытые демо, 
регулярные письма со списком достижений на всю компанию, статьи в Интранете. 
Главное – охватить максимальное количество релевантной аудитории и предоставить информацию в понятном для всех виде.

Информирование команды об изменениях и новостях проще всего проводить на регулярных встречах со всей командой – тимлид рассказывает о новостях и изменениях и даёт время задать волнующие вопросы.
Дисциплинируешь себя регулярно смотреть на результаты команды со стороны заинтересованного лица.

Обеспечение видимости достижений:
- Повышение ценности каждого члена команды внутри компании
- Получение сотрудниками признания их достижений
- Хорошие результаты – хороший тимли

Информирование команды:
- Облегчает циркуляцию информации в компании
- Проще проводить организационные изменения
- Повышает лояльность сотрудников к компании

- Снижает вероятность повторения технических или управленческих ошибок, пройденных другими командами. Кто-то извне может сообщать важную информацию
- Разбирать критические ситуации легче при высокой степени прозрачности и доверия. Понимать их причины и пути предотвращения

Все составляющие обеспечения прозрачности могут быть легко делегированы на ответственного участника команды. 
Хорошим вариантом может быть подключить к этому своего заместителя.

- Организовывать демонстрации (Show and Tell) внутри команды.
- Делать регулярные рассылки заинтересованным лицам с информацией о проделанной работе и планами.
- Демонстрировать прозрачность в своей работе, объяснять её плюсы. Не все люди понимают что такое прозрачность, не все осознают её преимущества, не все умеют быть прозрачными.
- Проводить регулярные встречи с командой, на которых рассказывать о последних новостя

Прозрачность наружу
- Определите всех потенциальных стейкхолдеров вашей команды. В этом может помочь техника stakeholder mapping (https://www.smartsheet.com/what-stakeholder-analysis-and-mapping-and-how-do-you-do-it-effectively)  (https://www.smartsheet.com/what-stakeholder-analysis-and-mapping-and-how-do-you-do-it-effectively). Убедитесь, что в этом списке есть и ваш непосредственный руководитель.
- Проведите опрос внутри технического отдела или всей компании о том, кому интересно следить за вашими новостями. Сразу приготовьтесь к тому, что в реальности заинтересована в этом будет лишь малая доля ответивших "да".
- Пройдитесь по всем стейкхолдерам и проговорите, с какой частотой и в каком формате они хотят получать информацию о работе вашей команды и её результатах.
- С учётом требований всех стейкхолдеров определитесь с форматом информирования их и других заинтересованных групп внутри компании. Это, например, могут быть демо, письменные отчёты, краткие дайджесты в Slack. Постарайтесь сделать так, чтобы выбранные вами форматы не задевали команду и не съедали её время.
- Для каждого формата определитесь с тем, как будете контролировать его эффективность – это могут быть количество просмотров ваших отчётов, качественная оценка удовлетворённости заказчиков.

Прозрачность внутрь
- Назначьте с командой регулярные встречи или встройтесь в текущую рутину.
-  Подготовьте небольшой рассказ о том, что происходило в компании последнее время – это могут быть новости из соседних отделов, стратегические планы, интересные слухи.
- Соберите с команды запросы того, о чем ещё им хотелось бы узнать, и договоритесь насчёт периодичности

**Плюсы**
- Собрать запросы с команды и внедрить регулярные встречи и рассылки для улучшения прозрачности.

---------------------------------------------------------
---------------------------------------------------------
## 13 may 23
**Team lead - зрелость команды**
Субъективная оценка отношения команды к повседневным задачам, способам их выполнения, особенностям командной динамики. Есть несколько моделей: Херси – Бланшара (https://psy.wikireading.ru/98751)  (https://psy.wikireading.ru/98751), пяти пороков Ленсиони (https://blog.mann-ivanov-ferber.ru/2016/07/07/5-porokov-komandy-i-sposoby-ix-ustraneniya/)  (https://blog.mann-ivanov-ferber.ru/2016/07/07/5-porokov-komandy-i-sposoby-ix-ustraneniya/). Эта оценка опирается:
- Как команда хочет получать задачи: инструкция vs конечная цель
- Какие есть отношения и ожидания друг от друга в команде

Команда большее, чем совокупность её членов, а значит необходимо работать и оценивать не только сотрудников в отдельности, но и команду в целом.
Целенаправленная работа над зрелостью команды позволит:
- Увеличить продуктивность команды
- Получать результат лучше, чем было запланировано
- Работа в команде, которая любит своё дело – дополнительный фактор мотивации сотрудников

---------------------------------------------------------
---------------------------------------------------------
## 12 may 23
**System design - google maps**
Improvements


WebSocket is a communication protocol that allows users and servers to have a two-way, interactive communication session. 
This helps in the real-time transfer of data between user and server.

The load balancer balances the connection load between different servers since there is a limit on the number of WebSocket connections per server. 
It connects some devices to server 1, some to server 2, and so on.

A pub-sub system collects the location data streams (device, time, location) from all servers. 
The location data from pub-sub is read by a data analytics engine like Apache Spark.
 The data analytics engine uses data science techniques—such as machine learning, clustering, and so on—to measure and predict traffic on the roads, 
 identify gatherings, hotspots, events, find out new roads, and so on. 
 These analytics help our system improve ETAs

The data analytics engine publishes the analytics data to a new pub-sub topic

The map update service listens to the updates from the pub-sub topic for the analytics. 
It updates the segment graphs if there is a new road identified or if there is a change in the weight (average speed (traffic, road condition)) on the edges of the graph. 
Depending on the location, we know which segment the update belongs to. We find the routing server on which that segment is placed from the key-value store and update the graph on that server.

The graph preprocessing service recalculates the new paths on the updated segment graph. We’ve seen how the paths are updated continuously in the background based on the live data.

With a large road network graph hosted on a single server, we ran into these issues:
- We couldn’t process user queries, since it was impossible to load such a large graph into the memory, making the system unavailable to the users.
- It wasn’t possible to make a persistent two-way connection (for navigation) between the server and millions of users per second.
- It was also a single point of failure.

We solved the above problems by dividing the world into small segments. 
Each small segment consists of a graph that can be easily loaded into a server’s memory. 
With segments, we completed these objectives:
- We hosted each segment on a separate server, mitigating the issue of loading a large, global graph.
- The load balancer divides the request load across different segment servers depending on the user’s area of search. 
It mitigates the issue of putting the burden on a single server, which was affecting the system’s availability.
-  We didn’t discuss replication, but we can replicate each segment, which will help deal with a segment server as a single point of failure and distribute the request load for a segment to replicas

Lazy loading reduces initial load time by reducing the amount of content to load, saves bandwidth by delivering content to users when needed, and preserves server and client resources by rendering only some of the content

We scaled our system for large road networks. Scalability can be seen in two ways:
- The ability of the system to handle an increasing amount of user requests.
- The ability of the system to work with more data (segments).
We divided the world into small segments. 
Each segment is hosted on a different server in the distributed system. 
The user requests for different routes are served from the different segment servers. 
In this way, we can serve millions of user requests

We’re running the user requests on small subgraphs. 
Processing a small subgraph of hundreds of vertices is far faster than a graph of millions of vertices. 
We can cache the processed small subgraph in the main memory and quickly respond to user requests. 
This is how our system responds to the user in less time.

The key-value store helps different services to get the required information quickly.
- The graph processing service checks for the relevant segments in which the source and the destination latitude/longitude lie by querying the key-value store for the segmentID values.
- For load-balancing user requests among different segment servers, the key-value store is queried for the serverID against the segment on which the graph processing should run for a specific request

---------------------------------------------------------
---------------------------------------------------------
## 11 may 23
**System design - google maps**

Each segment has a unique name and boundary coordinates. 
We can easily identify which location (latitude, longitude) lies in which segment. 
Given the source and the destination, we can find the segments in which they lie. For each segment, there are some boundary edges, which we call exit points. 

Let’s summarize how we met the challenge of scalability. 
We divided our problem so that instead of working on a large road network as a whole, we worked on parts (segments) of it. 
The queries for a specific part of the road network are processed on that part only, and for the queries that require processing more than one part of the network, 
we connect those parts, as we have shown above.

For computing the ETA with reasonable accuracy, we collect the live location data ((userID, timestamp,(latitude, longitude))) from the navigation service through a pub-sub system. 
With location data streams, we can calculate and predict traffic patterns on different roads. Some of the things that we can calculate are:
- Traffic (high/medium/low) on different routes or roads.
- The average speed of a vehicle on different roads.
- The time intervals during which a similar traffic pattern repeats itself on a route or road. For example, highway X will have high traffic between 8 to 10 AM.

We store the following information for each segment:
Key-value store:
- The segment’s ID.
- The serverID on which the segment is hosted.
- In reality, each segment is a polygon, so we store boundary coordinates (latitude/longitude), possibly as a list.
- A list of segment IDs of the neighbors segments

We store the information to determine whether, at a particular hour of the day, the roads are congested. This later helps us decide whether or not to update the graph (weights) based on the live data.
- edgeID identifies the edge.
- hourRange tells us which hour of the day it is when there are typical road conditions (non-rush hour) on the road.
- rush is a Boolean value that depicts whether there is congestion or not on a specific road at a specific time.

Each segment has its latitude/longitude boundary coordinates and the graph of its road network.
The segment adder processes the request to add the segment along with the segment information. 
The segment adder assigns a unique ID to each segment using a unique ID generator system.
After assigning the ID to the segment, the segment adder forwards the segment information to the server allocator.
The server allocator assigns a server to the segment, hosts that segment graph on that server, and returns the serverID to the segment adder.
After the segment is assigned to the server, the segment adder stores the segment to server mapping in the key-value store. 
It helps in finding the appropriate servers to process user requests. It also stores each segment’s boundary latitude/longitude coordinates in a separate key-value object.

The user provides the source and the destination so that our service can find the path between them.
The latitude and longitude of the source and the destination are determined through a distributed search.
The latitude/longitude for the source and the destination are passed to the graph processing service that finds the segments in which the source and the destination latitude/longitude lie.
After finding the segment IDs, the graph processing service finds the servers that are hosting these segments from the key-value store.
The graph processing service connects to the relevant servers to find the shortest path between the source and the destination. 
If the source and the destination belong to the same segment, the graph processing service returns the shortest path by running the query only on a single segment. 
Otherwise, it will connect the segments from different servers, as we have seen in the previous lesson.

---------------------------------------------------------
---------------------------------------------------------
## 10 may 23
**System design - google maps**

In a maps system, the user has to enter their starting point and their destination to create a path between the two. For the starting source point, the user uses the current location service.
The location finder determines the current location by maintaining a persistent connection with the user. The user will provide an updated location using GPS, Wi-Fi, and cellular technology. This will be the user’s source point.
For the destination point, the user types an address in text format. It’s a good idea to make use of our Typeahead service here to provide useful suggestions and avoid spelling mistakes.
After entering the source and the destination points, the user requests the optimal path.
The user’s path request is forwarded to the route-finder service.
The route finder forwards the requests to an area search service with the source and the destination points.
The area search service uses the distributed search to find the latitude/longitude for the source and the destination. It then calculates the area on the map spanning the two (source’s and destination’s) latitude/longitude points.
After finding the area, the area search service asks the graph processing service to process part of the graph, depending on the area to find the optimal path.
The graph processing service fetches the edges and nodes within that specified area from the database, finds the shortest path, and returns it to the route-finder service that visualizes the optimal path with the distance and time necessary to comeplete the route. It also displays the steps the user should follow for navigation.
Now that the user can visualize the shortest path on the map, they also want to get directions towards the destination. The direction request is handled by the navigator.
The navigator tracks that the user is following the correct path, which it has from the route-finder service. It updates the user’s location on the map while the user is moving, and shows where to turn with the distance. If a user deviates from the path, it generates an event that is fed to Kafka.
Upon receiving the event from the navigator, Kafka updates the subscribed topic of the area search service, which in turn recalculates the optimal path and suggests it to the user. The navigator also provides a stream of live location data to the graph, building it through the pub-sub system. Later, this data can be used to improve route suggestions provided to the users.

Scalability is about the ability to efficiently process a huge road network graph. 
We have a graph with billions of vertices and edges, and the key challenges are inefficient loading, updating, and performing computations. 
For example, we have to traverse the whole graph to find the shortest path. 
This results in increased query time for the user. So, what could be the solution to this problem.

The idea is to break down a large graph into smaller subgraphs, or partitions. 
The subgraphs can be processed and queried in parallel. As a result, the graph construction and query processing time will be greatly decreased. 
So, we divide the globe into small parts called segments. Each segment corresponds to a subgraph.

A segment is a small area on which we can work easily. 
Finding paths within these segments works because the segment’s road network graph is small and can be loaded easily in the main memory, updated, and traversed. 
A city, for example, can be divided into hundreds of segments.

Each segment has four coordinates that help determine which segment the user is in. 
Each coordinate consists of two values, the latitude and longitude.

Let’s talk about finding paths between two locations within a segment.
 We have a graph representing the road network in that segment. 
 Each intersection/junction acts as a vertex and each road acts as an edge. The graph is weighted, and there 
 could be multiple weights on each edge—such as distance, time, and traffic—to find the optimal path. 
 For a given source and destination, there can be multiple paths. 
 We can use any of the graph algorithms on that segment’s graph to find the shortest paths. 
 The most common shortest path algorithm is the Dijkstra’s algorithm.

After running the shortest path algorithm on the segment’s graph, we store the algorithm’s output in a distributed storage to avoid recalculation and cache the most requested routes. 
The algorithm’s output is the shortest distance in meters or miles between every two vertices in the graph, 
the time it takes to travel via the shortest path, and the list of vertices along every shortest path. 
All of the above processing (running the shortest path algorithm on the segment graph) is done offline (not on a user’s critical path).

What if we have to find the path between two points that lie on the edges? 
What we do is find the vertices of the edge on which the points lie, calculate the distance of the point from the identified vertices, 
and choose the vertices that make the shorter total distance between the source and the destination. 
The distance from the source (and destination) to the nearest vertices is approximated using latitude/longitude values.

---------------------------------------------------------
---------------------------------------------------------
## 9 may 23
**System design - google maps**

We’ll use the following components in our design:

- Location finder: The location finder is a service used to find the user’s current location and show it on the map since we can’t possibly personally remember the latitude and longitude for every place in the world.
- Route finder: For the people who are new to a place, it’s difficult to travel because they don’t know the correct routes. 

The route finder is a service used to find the paths between two locations, or points.
- Navigator: Suggesting a route through the route finder is not enough. 
A user may deviate from the optimal path. In that case, a navigator service is used. 
This service keeps track of users’ journeys and sends updated directions and notifications to the users as soon as they deviate from the suggested route.

- GPS/Wi-Fi/Cellular technology: These are the technologies that we used to find the user’s ground position.

- Distributed search: For converting place names to latitude/longitude values, we need a conversion system behind the source and destination fields. 
A distributed search maintains an index consisting of places names and their mapping to latitude and longitude.
 User’s entered keywords are searched using this service to find a location on the map.
 
- Area search service: The area search service coordinates between the distributed search and graph processing service to obtain the shortest path against a user query.
 The area search service will request the distributed search to obtain the locations of the source and destination on the map. 
 Then, it will use the graph processing service to find the optimal path from the source to the destination.
 
- Graph processing service: There can be multiple paths from one place to another. 
The graph processing service runs the shortest path algorithm on a shorter graph based on the area spanning the source and destination points and helps us determine which path to follow.

- Database: As discussed in the previous lesson, we have the road data from various sources stored in the form of a graph. 
We’ll map this data to a database to develop the road network graph. We’re using a graph database like DataStax Graph to store the graph for our design.

- Pub-sub system: Users might deviate from the first suggested path. In that case, they’ll need information on a new path to their destination. 
Pub-sub is a system that listens to various events from a service and triggers another service accordingly. 
For example, when a user deviates from the suggested path, it pings the area search service to find a new route from the user’s current location to their destination point. 
It also collects the streams of location data for different users from the navigator. 
This data can be processed later to find traffic patterns on different roads at different times. 
We’ll use Kafka as a pub-sub system in our design.

- Third-party road data: How can we build a map system if we don’t have the road networks data? 
We need to collect the road data from third-party resources and preprocess the collected data to bring it into a single format that can be utilized to build the graph.

- Graph building: We’ll use a service that builds the graph from the given data, either collected from the third-party resources or from the users.
- User: This refers to a person or a program that uses the services of the map system.
- Load balancer: This is a system that is used to distribute user requests among different servers and services.

---------------------------------------------------------
---------------------------------------------------------
## 8 may 23
**System design - google maps**

Let’s introduce the problem by assuming that we want to travel from one place to another. Here are the possible things that we might want to know:
- What are the best possible paths that take us to our destination, depending on the vehicle type we’re using?
- How long in miles is each path?
- How much time does each path take to get us to our destination?
 
The functional requirements of our system are as follows.
- Identify the current location: Users should be able to approximate their current location (latitude and longitude in decimal values) on the world map.
- Recommend the fastest route: Given the source and destination (place names in text), the system should recommend the optimal route by distance and time, depending on the type of transportation.
- Give directions: Once the user has chosen the route, the system should list directions in text format, where each item in the list guides the user to turn or continue in a specific direction to reach the destination.

The non-functional requirements of our system are as follows.
- Availability: The system should be highly available.
- Scalability: It should be scalable because both individuals and other enterprise applications like Uber and Lyft use Google Maps to find appropriate routes.
- Less response time: It shouldn’t take more than two or three seconds to calculate the ETA and the route, given the source and the destination points.
- Accuracy: The ETA we predict should not deviate too much from the actual travel time

Scalability: Serving millions of queries for different routes in a second, given a graph with billions of nodes and edges spanning over 194 countries, 
requires robust scalability measures. 
A simple approach, given the latitude and longitude of the source and destination, would be to apply an algorithm like Dijkstra to find the shortest path between the source and the destination. 
However, this approach wouldn’t scale well for billions of users sending millions of queries per second. 
This is because running any path-finding algorithm on a graph with billions of nodes running a million times per second is inefficient in terms of time and cost, 
ultimately leading to a bad user experience. Therefore, our solution needs to find alternative techniques to scale well.

ETA computation: In an ideal situation with empty roads, it’s straightforward to compute ETA using the distance and the speed of the vehicle we want to ride on. 
However, we cannot ignore factors like the amount of traffic on the roads and road conditions, which affect the ETA directly. 
For example, a road under construction, collisions, and rush hours all might slow down traffic. 
Quantifying the factors above to design our system is not trivial. 
Therefore, we’ll, categorize the factors above in terms of traffic load to complete our design

To estimate the number of servers, we need to know how many daily active users are using Google Maps and how many requests per second a single Google Maps server can handle. 
We assume the following numbers:
- Daily active users who use Google Maps: 32 million (about 1 billion monthly users).
- Number of requests a single server can handle per second: 8,000.
4000 servers

---------------------------------------------------------
---------------------------------------------------------
## 6 may 23
**Team lead - знание пользователей**

Product Owner является защитником интересов тех, кто использует его продукт. Если упростить, то эта ветка заключается в следующем:
- Определение целевой аудитории продукта и её сегментирование
- Сбор, анализ и решение болей пользователей


Плюсы
- Команда регулярно общается с пользователями своего продукта.
- Команда в курсе пользовательских исследований и инсайтов.
- При разработке фичи всегда учитывается, для какого сегмента пользователей она разрабатывается.
- Регулярно проводятся маркетинговые исследования для обновления информации о целевой аудитории.

1) Максимально широко определите, кто является вашей целевой аудиторией. Вариант "все люди", конечно, возможен, но лучше все-таки быть более конкретным.
2) Вместе с командой определите значимые критерии сегментации – те, разделение по которым влияет на боли, запросы и потребности пользователей. 
Например, для текстового редактора нет смысла сегментировать пользователей на основе их пола, но есть смысл на основе географии или целей использования.
3) Опишите ваши гипотезы о сегментах, их целях и объёмах. Максимально дешёвым способом провалидируйте их на существующих данных, с помощью пользовательских интервью или маркетинговых исследований.
4) Приоритизируйте ваши сегменты с использованием взвешенной оценки. Можно учитывать такие параметры, как размер сегмента, лёгкость выхода на его представителей, потенциальная прибыль.
5) Опишите полученные сегменты и используйте их в дальнейшем для планирования стратегии
6) Проведите брейншторм с командой способы выйти на релевантных для вас пользователей. Это может быть размещение объявления на Авито, сообщение в релевантном сообществе, профильное оффлайн-мероприятие.
7) Соберите контакты пользователей с использованием выбранных каналов.
8) Договоритесь о постоянном проведении пользовательских интервью, на которых вы сможете валидировать ваши гипотезы о проблемах, собирать обратную связь по фичам.
9) Позовите пользователей на демо команды – это даст отличную возможность сократить цикл получения обратной связи
10) Постройте канал сбора болей пользователей. Стоит посмотреть на социальные сети, данные от службы поддержки, отзывы в магазинах приложений.
11) Регулярно разбирайте этот канал и релевантные боли переводите в отдельный бэклог. В качестве критерия приоритизации можно использовать количество обращений.
12) Сохраняйте контакты тех, кого касалась боль, чтобы сообщить им о релевантном запуске

**Плюсы**
- Пообщаться с продуктологм над построением каналом сбора болей пользователей, подумать можно ли как-то найти пользователей и напрямую с ними общаться.

---------------------------------------------------------
---------------------------------------------------------
## 5 may 23
**Team lead - знание продукта**

Хороший менеджер должен, с одной стороны, знать свой продукт лучше любого из его пользователей, 
а с другой – быть способным поставить себя на место новичка и посмотреть на продукт его глазами. 
Это знание заключается в понимании следующих вещей:
- Сценарии использования продукта пользователями
- Миссия и видение продукта
- Пирамида продуктовых метрик
- Представление о value streams

Плюсы
- Регулярно используете свой продукт
- Строите и содержите актуальными customer journey maps и продуктовую документацию
- Проводите пользовательские интервью и UX сессии, на которых изучаете, как пользователи взаимодействуют с вашим продуктом
- Постоянно изучаете продуктовую аналитику
- Всегда можете объяснить, почему какая-то фича в вашем продукте сделана иначе, чем у конкурентов, и почему ваш вариант лучше

Поговорите с командой продакт-менеджеров или представителей бизнеса, и спросите их про видение будущего продукта и про то, какую ценность он на текущий момент представляет.
Найдите существующую документацию и изучите её. Даже если она устарела, вы можете найти полезные инсайты.
Получите доступ к системе аналитики и изучите то, что там доступно. 
Посмотрите на типичные сценарии использования продукта, показатели отказов для различных сценариев, retention и показатели длины сессии

Самое главное правило – перестроить свою рабочую рутину таким образом, чтобы вы регулярно использовали свой продукт как его обычный пользователь. 
Если вы делаете площадку объявлений – вы должны регулярно покупать и продавать на ней свои товары. 
Если сервис стриминга музыки – использовать его как основной. 
Вы должны, с одной стороны, постоянно чувствовать на себе все существующие боли и проблемы, а с другой – оставаться главным фанатом своего продукта.


Регулярно рассказывайте команде о своём опыте работы с продуктом. 
Это можно делать на стендапах, либо в виде дайджестов. 
Интересная практика – вести полноценный дневник, в котором вы фиксируете весь ваш опыт работы с продуктом, а потом показываете его остальной команде.

**Плюсы**
- Пообщаться с продуктологом попросить рассказать описанные моменты, устаканить в голове (расписать где в документации по продукту);

---------------------------------------------------------
---------------------------------------------------------
## 4 may 23
**System design Quora**
Quora uses a technique called long polling, where if a client requests for an update, the server may not respond for as long as 60 seconds if there are no updates. 
However, if there is an update, the server will reply immediately and allow the client to make new requests.

Lastly, Memcached can employ multiget() to obtain multiple keys from the cache shards to reduce the retrieval latency of multiple keys

Scalability: 
Our system is highly scalable for several reasons. 
The updated design uses powerful and homogeneous service hosts. 
Quora uses powerful machines because service hosts use an in-memory cache, some level of queueing, maintain manager, worker, and routing library. 
The horizontal scaling of these service hosts is convenient because they are homogeneous.
On the database end, our design shards the MySQL databases vertically, which avoids issues in scalability because of overloaded MySQL servers. 
To reduce complex join queries, tables anticipating join operations are placed in the same shard or partition

Consistency: 
Due to the variety of functionalities offered by Quora, different consistency schemes may be selected for different types of data. 
For example, certain critical data like questions and answers should be stored synchronously. 
In this case, performance can take a hit because users don’t expect instantaneous responses to their questions. 
It means that a user may get a reply in five minutes, one hour, one day, or no response at all, depending on the user’s question and the availability of would-be respondents

Availability: 
Some of the main ideas to improve availability include isolation between different components, keeping redundant instances, using CDN, using configuration services like ZooKeeper, 
and load balancers to hide failures from users

Performance: 
This design has a strong performance because we have employed the right technology for the right feature. 
For example, we have used several datastores for different reasons. 
On top of that, we used different distributed caches depending upon the use case and access frequency. 
Also, we employed Kafka to queue similar tasks and assign them to cron jobs that otherwise take a long time if executed via API

---------------------------------------------------------
---------------------------------------------------------
## 3 may 23
**System design Quora**

API:
- Post a question
- Post an answer
- Upvote or downvote a question or answer
- Comment on an answer
- Search

The proposed design serves all the functional requirements. 
However, it has a number of serious drawbacks that emerge as we scale. 
This means that we are unable to fulfill the non-functional requirements.

- Limitations of web and application servers: 
To entertain the user’s request, payloads are transferred between web and application servers, which increases latency because of network I/O between these two types of servers. 
Even if we achieve parallel computation by separating the web from application servers (that is, the manager and worker processes), the added latency due to an additional 
network link erodes a user’s experience. Apart from data transfer, control communication between the router library with manager and worker processes also imposes additional performance penalties

- In-memory queue failure: The internal architecture of application servers log tasks and forward them to the in-memory queues, which serve them to the workers. 
These in-memory queues of different priorities can be subject to failures. 
For instance, if a queue gets lost, all the tasks in that queue are lost as well, and manual engineering is required to recover those tasks. 
This greatly reduces the performance of the system. On the other hand, replicating these queues requires increasing RAM size. 
Also, with the number of features (functional requirements) that our system offers, many tasks can get assembled, which results in insufficient memory. 
At the same time, it is not desirable to choke application servers with not-so-urgent tasks. 
For example, application servers should not be burdened with tasks like storing view counts for answers, adding statistics to the database for later analysis, and so on.

- Latency of HBase: Even though HBase allows high real-time throughput, its P99 latency is not among the best. 
A number of Quora features require the ML engine that has a latency of its own. 
Due to the addition of the higher latency of HBase, the overall performance of the system degrades over time

Service hosts
- We combine the web and application servers within a single powerful machine that can handle all the processes at once. This technique eliminates the network I/O and the latency introduced due to the network hops required between the manager, worker, and routing library processes. The illustration below provides an abstract view of the updated web server architecture:

Tables in the MySQL server are converted to separate shards that we refer to as partitions. 
A partition has a single primary server and multiple replica servers.
The goal is to improve performance and reduce the load due to an increasing number of queries on a single database table. To achieve that, we do vertical sharding in two ways

- We split tables of a single database into multiple partitions. The concept is depicted in Partitions 2 and 3, which embed Tables 4 and 3, respectively.
- We combine multiple tables into a single partition, where join operations are anticipated. The concept is depicted in Partition 1, which embeds Tables 1 and 2
require two types of mappings or metadata to complete our scaling process:
  Which partitions contain which tables and columns?
  Which hosts are primary and replicas of a particular partition?
Both of these mappings are maintained by a service like ZooKeeper

The new design embeds MyRocks as the key-value store instead of HBase. We use the MyRocks version of RocksDB for two main reasons:
- MyRocks has a lower p99 latency instead of HBase. Quora claims to have reduced P99 latency from 80 ms to 4 ms using MyRocks.
- There are operational tools that can transfer data between MyRocks and MySQL

Our updated design reduces the request load on service hosts by separating not-so-urgent tasks from the regular API calls. 
For this purpose, we use Kafka, which can disseminate jobs among various queues for tasks such as the view counter.

---------------------------------------------------------
---------------------------------------------------------
## 2 may 23
**System design Quora**
The initial design of Quora will be composed of the following building blocks and components:
- Web and application servers: 
A typical Quora page is generated by various services. 
The web and application servers maintain various processes to generate a webpage. 
The web servers have manager processes and the application servers have worker processes for handling various requests. 
The manager processes distribute work among the worker processes using a router library. 
The router library is enqueued with tasks by the manager processes and dequeued by worker processes. 
Each application server maintains several in-memory queues to handle different user requests. 
The following illustration provides an abstract view of web and application servers

- Data stores: Different types of data require storage in different data stores. 
We can use critical data like questions, answers, comments, and upvotes/downvotes in a relational database like MySQL because it offers a higher degree of consistency. 
NoSQL databases like HBase can be used to store the number of views of a page, scores used to rank answers, and the extracted features from data to be used for recommendations later on. 
Because recomputing features is an expensive operation, HBase can be a good option to store and retrieve data at high bandwidth. 
We require high read/write throughput because big data processing systems use high parallelism to efficiently get the required statistics. 
Also, blob storage is required to store videos and images posted in questions and answers.
- Distributed cache: For performance improvement, two distributed cache systems are used: Memcached and Redis. 
Memcached is primarily used to store frequently accessed critical data that is otherwise stored in MySQL. 
On the other hand, Redis is mainly used to store an online view counter of answers because it allows in-store increments.
Therefore, two cache systems are employed according to their use case. Apart from these two, CDNs serve frequently accessed videos and images.
- Compute servers: A set of compute servers are required to facilitate features like recommendations and ranking based on a set of attributes. 
These features can be computed in online or offline mode. 
The compute servers use machine learning (ML) technology to provide effective recommendations. 
Naturally, these compute servers have a substantially high amount of RAM and processing power


Posting question, answers, comments: 
The web servers receive user requests through the load balancer and direct them to the application servers. 
Meanwhile, the web servers generate part of the web page and let the worker process in the application servers do the rest of the page generation. 
The questions and answers data is stored in a MySQL database, whereas any videos and images are stored in the blob storage.
 A similar approach is used to post comments and upvote or downvote answers. 
 Task prioritization is performed by employing different queues for different tasks. 
 We perform prioritization because certain tasks require immediate attention—for example, fetching data from the database 
 for a user request—while others are not so urgent—for example, sending a weekly email digest. 
 The worker processes will perform tasks by fetching from these queues.

- Answer ranking system: Answers to questions can be sorted based on date. 
Although it is convenient to develop a ranking system on the basis of date (using time stamps), users prefer to see the most appropriate answer at the top. 
Therefore, Quora uses ML to rank answers. Different features are extracted over time and stored in the HBase for each type of question. 
These features are forwarded to the ML engine to rank the most useful answer at the top. 
We cannot use the number of upvotes as the only metric for ranking answers because a good number of answers can be jokes—and such answers also get a lot of upvotes. 
It is good to implement the ranking system offline because good answers get upvotes and views over time. Also, the offline mode poses a lesser burden on the infrastructure. 
Implementing the ranking system offline and the need for special ML hardware makes it suitable to use some public cloud elastic services.

- Recommendation system: The recommendation system is responsible for several features. 
For example, we might need to develop a user feed, find related questions and ads, recommend questions to potential respondents, 
and even highlight duplicate content and content in violation of the service’s terms of use. Unlike the answer ranking system, 
the recommendation system must provide both online and offline services. This system receives requests from the application server and forwards selected features to the ML engine.

- Search feature: Over time, as questions and answers are fed to the Quora system, it is possible to build an index in the HBase.
 User search queries are matched against the index, and related content is suggested to the user. Frequently accessed indexes can be served from cache for low latency. 
 The index can be constructed from questions, answers, topics labels, and usernames. 

---------------------------------------------------------
---------------------------------------------------------
## 1 may 23
**System design Quora**
Quora is a social question-and-answer service that allows users to ask questions to other users. 
Quora was created because of the issue that asking questions from search engines results in fast answers but shallow information. 
Instead, we can ask the general public, which feels more conversational and can result in deeper understanding, even if it’s slower. 
Quora enables anyone to ask questions, and anyone can reply. 
Furthermore, there are domain experts that have in-depth knowledge of a specific topic who occasionally share their expertise by answering questions.

User should be able to perform the following functionalities:
- Questions and answers: Users can ask questions and give answers. Questions and answers can include images and videos.
- Upvote/downvote and comment: It is possible for users to upvote, downvote, and comment on answers.
- Search: Users should have a search feature to find questions already asked on the platform by other users.
- Recommendation system: A user can view their feed, which includes topics they’re interested in.
- Ranking answers: We enhance user experience by ranking answers according to their usefulness.

- Scalability: The system should scale well as the number of features and users grow with time. 
- Consistency: The design should ensure that different users’ views of the same content should be consistent.
- Availability: The system should have high availability. This applies to cases where servers receive a large number of concurrent requests.
- Performance: The system should provide a smooth experience to the user without a noticeable delay

Assumptions: It is important to base our estimation on some underlying assumptions. We, therefore, assume the following:
- There are a total of 1 billion users, out of which 300 million are daily active users.
- Assume 15% of questions have an image, and 5% of questions have a video embedded in them. A question cannot have both at the same time.
- We’ll assume an image is estimated to be 250 KBs, and a video is considered 5 MBs.

Let’s estimate our requests per second (RPS) for our design. 
If there are an average of 300 million daily active users and each user can generate 20 requests per day, then the total number of requests in a day will be
69500 rps
37500 servers
Storage 85tb per day

---------------------------------------------------------
---------------------------------------------------------
## 27 apr 23
**System design Youtube**

When talking about providing effective service to end users, the following three steps are important:
- Encode: The raw videos uploaded to YouTube have significant storage requirements. 
It’s possible to use various encoding schemes to reduce the size of these raw video files. 
Apart from compression, the choice of encoding scheme will also depend on the types of end devices used to stream the video content.
 Since multiple devices could be used to stream the same video, we may have to encode the same video using different encoding schemes resulting in one raw video 
 file being converted into multiple files each encoded differently. 
 This strategy will result in a good user-perceived experience because of two reasons: users will save bandwidth because the video file will be encoded and compressed to some limit, 
 and the encoded video file will be appropriate for the client for a smooth playback experience.
 
- Deploy: For low latency, content must be intelligently deployed so that it is closer to a large number of end users. 
Not only will this reduce latency, but it will also put less burden on the networks as well as YouTube’s core servers.

- Deliver: Delivering to the client requires knowledge about the client or device used for playing the video. 
This knowledge will help in adapting to the client and network conditions. T
herefore, we’ll enable ourselves to serve content efficiently.

Until now, we’ve considered encoding one video with different encoding schemes. 
However, if we encode videos on a per-shot basis, we’ll divide the video into smaller time frames and encode them individually. 
We can divide videos into shorter time frames and refer to them as segments. 
Each segment will be encoded using multiple encoding schemes to generate different files called chunks. 
The choice of encoding scheme for a segment will be based on the detail within the segment to get optimized quality with lesser storage requirements. 
Eventually, each shot will be encoded into multiple chunk sizes depending on the segment’s content and encoding scheme used. 
As we divide the raw video into segments, we’ll see its advantages during the deployment and delivery phase.

For any video with dynamic colors and high depth, we’ll encode it differently from a video with fewer colors. 
This means that a not-so-dynamic segment will be encoded such that it’s compressed more to save additional storage space. 
Eventually, we’ll have to transfer smaller file sizes and save bandwidth during the deployment and streaming phases.

Using the strategy above, we’ll have to encode individual shots of a video in various formats. 
However, the alternative to this would be storing an entire video (using no segmenting) after encoding it in various formats. 
If we encode on a per-shot basis, we would be able to optimally reduce the size of the entire video by doing the encoding on a granular level. 
We can also encode audio in various formats to optimally allow streaming for various clients like TVs, mobile phones, and desktop machines. 
Specifically, for services like Netflix, audio encoding is more useful because audios are offered in various languages.

We have to bring the content closer to the user. This has three main advantages:
- Users will be able to stream videos quickly.
- There will be a reduced burden on the origin servers.
- Internet service providers (ISPs) will have spare bandwidth.

So, instead of streaming from our data centers directly, we can deploy chunks of popular videos in CDNs and point of presence (PoPs) of ISPs. 
In places where there is no collaboration possible with the ISPs, our content can be placed in internet exchange point (IXPs). 
We can put content in IXPs that will not only be closer to users, but can also be helpful in filling the cache of ISP PoPs.

YouTube recommends videos to users based on their profile, taking into account factors such as their interests, 
view and search history, subscribed channels, related topics to already viewed content, and activities on content such as comments and likes.

An approximation of the recommendation engine of YouTube is provided below. YouTube filters videos in two phases:

Candidate generation: During this phase, millions of YouTube videos are filtered down to hundreds based on the user’s history and current context.
Ranking: The ranking phase rates videos based on their features and according to the user’s interests and history. 
Hundreds of videos are filtered and ranked down to a few dozen videos during this phase.

How the end user gets the content on their device. 
Since we have the chunks of videos already deployed near users, we redirect users to the nearest available chunks. 
As shown below, whenever a user requests content that YouTube has recognized as popular, YouTube redirects the user to the nearest CDN.

However, in the case of non-popular content, the user is served from colocation sites or YouTube’s data center where the content is stored initially. 
We have already learned how YouTube can reduce latency times by having distributed caches at different design layers.

While the content is being served, the bandwidth of the user is also being monitored at all times. 
Since the video is divided into chunks of different qualities, each of the same time frame, the chunks are provided to clients based on changing network conditions.
As shown below, when the bandwidth is high, a higher quality chunk is sent to the client and vice versa.

---------------------------------------------------------
---------------------------------------------------------
## 26 apr 23
**System design Youtube**

Since YouTube is one of the most visited websites, 
a large number of users will be using the search feature.

Each new video uploaded to YouTube will be processed for data extraction. 
We can use a JSON file to store extracted data, which includes the following:
- Title of the video
- Channel name.
- Description of the video.
- The content of the video, possibly extracted from the transcripts.
- Video length.
- Categories.

Each of the JSON files can be referred to as a document. 
Next, keywords will be extracted from the documents and stored in a key-value store. 
The key in the key-value store will hold all the keywords searched by the users, 
while the value in the key-value store will contain the occurrence of each key, its frequency, and the location of the occurrence in the different documents. 
When a user searches for a keyword, the videos with the most relevant keywords will be returned.

Low latency/Smooth streaming can be achieved through these strategies:
- Geographically distributed cache servers at the ISP level to keep the most viewed content.
- Choosing appropriate storage systems for different types of data. For example, we’ll can use Bigtable for thumbnails, blob storage for videos, and so on.
- Using caching at various layers via a distributed cache management system.
- Utilizing content delivery networks (CDNs) that make heavy use of caching and mostly serve videos out of memory. A CDN deploys its services in close vicinity to the end users for low-latency services.
- Scalability: We’ve taken various steps to ensure scalability in our design as depicted in the table below. 
The horizontal scalability of web and application servers will not be a problem as the users grow. 
However, MySQL storage cannot scale beyond a certain point.

- Availablity: The system can be made available through redundancy by replicating data to as many servers as possible to avoid a single point of failure. 
Replicating data across data centers will ensure high availability, even if an entire data center fails because of power or network issues. 
Furthermore, local load balancers can exclude any dead servers, and global load balancers can steer traffic to a different region if the need arises.
- Reliability: YouTube’s system can be made reliable by using data partitioning and fault-tolerance techniques.
 Through data partitioning, the non-availability of one type of data will not affect others. 
 We can use redundant hardware and software components for fault tolerance. 
 Furthermore, we can use the heartbeat protocol to monitor the health of servers and omit servers that are faulty and erroneous. 
 We can use a variant of consistent hashing to add or remove servers seamlessly and reduce the burden on specific servers in case of non-uniform load.
 
 
*Consistency*
Our solution prefers high availability and low latency. 
However, strong consistency can take a hit because of high availability (see the CAP theorem). 
Nonetheless, for a system like YouTube, we can afford to let go of strong consistency. 
This is because we don’t need to show a consistent feed to all the users. 
For example, different users subscribed to the same channel may not see a newly uploaded video at the same time. 
It’s important to mention that we’ll maintain strong consistency of user data. 
This is another reason why we’ve decoupled user data from video metadata.

*Distributed cache*
We prefer a distributed cache over a centralized cache in our YouTube design. 
This is because the factors of scalability, availability, and fault-tolerance, which are needed to run YouTube,
 require a cache that is not a single point of failure. This is why we use a distributed cache. 

*Bigtable versus MySQL*
Another interesting aspect of our design is the use of different storage technologies for different data sets.
The primary reason for the choice is performance and flexibility. 
The number of users in YouTube may not scale as much as the number of videos and thumbnails do. 
Moreover, we require storing the user and metadata in structured form for convenient searching. 
Therefore, MySQL is a suitable choice for such cases.
However, the number of videos uploaded and the thumbnails for each video would be very large in number. Scalability needs would force us to use a custom or NoSQL type of design for that storage. One could use alternatives to GFS and Bigtable, such as HDFS and Cassandra.


*Duplicate videos*
The current YouTube design doesn’t handle duplicate videos that have been uploaded by a user or spammers. 
Duplicated videos take extra space, which leads to a trade-off. As a result, we either waste storage space or face an additional complexity to the upload process for handling duplicate videos.
Let’s perform some calculations to resolve this problem. 
Assume that 50 out of 500 hours of videos uploaded to YouTube are duplicates. 
Considering that one minute of video requires 6 MB of storage space, the duplicated content will take up the following storage space:
- (50 x 60) minutes x 6 MB/min = 18 GB
If we avoid video duplication, we can save up to 9.5 petabytes of storage space in a year.

Storage space being wasted, and other computational costs are not the only issues with duplicate videos. 

*Future scaling*
So far, we’ve focused on the design and analysis of the proposed design for YouTube. 
In reality, the design of YouTube is quite complex and requires advanced systems.

Any infrastructure mentioned above requires some modifications and adaptation to the application-level logic. 
For example, if we continue to increase our data in MySQL servers, it can become a choke point. 
To effectively use a sharded database, we might have to make changes to our database client to achieve a good level of performance and maintain the ACID (atomicity, consistency, isolation, durability)
properties. However, even if we continue to change to the database client as we scale, its complexity may reach a point where it is no longer manageable.
Also note that we haven’t incorporated a disaster recovery mechanism into our design yet.
To resolve the problems above, YouTube has developed a solution called Vitess.



---------------------------------------------------------
---------------------------------------------------------
## 25 apr 23
**System design Youtube**

The workflow for the abstract design:
- The user uploads a video to the server.
- The server stores the metadata and the accompanying user data to the database and, at the same time, hands over the video to the encoder for encoding.
- The encoder, along with the transcoder, compresses the video and transforms it into multiple resolutions (like 2160p, 1440p, 1080p, and so on). 
The videos are stored on blob storage (similar to GFS or S3).
- Some popular videos may be forwarded to the CDN, which acts as a cache.
- The CDN, because of its vicinity to the user, lets the user stream the video with low latency. 
However, CDN is not the only infrastructure for serving videos to the end user.

APIs for each of the following features:
- Upload videos
- Stream videos
-  Search videos
- View thumbnails
- Like or dislike videos
-  Comment on videos

- Component integration: We’ll cover some interconnections between the servers and storage components to better understand how the system will work.
- Thumbnails: It’s important for users to see some parts of the video through thumbnails. 
Therefore, we’ll add thumbnail generation and storage to the detailed design.
- Database structure: Our estimation showed that we require massive storage space. 
We also require storing varying types of data, such as videos, video metadata, and thumbnails, each of which demands specialized data storage for performance reasons. 
Understanding the database details will enable us to design a system with the least possible lag

- Load balancers: To divide a large number of user requests among the web servers, we require load balancers.
- Web servers: Web servers take in user requests and respond to them. These can be considered the interface to our API servers that entertain user requests.
- Application server: The application and business logic resides in application servers. They prepare the data needed by the web servers to handle the end users’ queries.
- User and metadata storage: Since we have a large number of users and videos, the storage required to hold the metadata of videos and the content related to users must be stored in different storage clusters. This is because a large amount of not-so-related data should be decoupled for scalability purposes.
- Bigtable: For each video, we’ll require multiple thumbnails. 
Bigtable is a good choice for storing thumbnails because of its high throughput and scalability for storing key-value data. 
Bigtable is optimal for storing a large number of data items each below 10 MB. 
Therefore, it is the ideal choice for YouTube’s thumbnails.
- Upload storage: The upload storage is temporary storage that can store user-uploaded videos.
- Encoders: Each uploaded video requires compression and transcoding into various formats. 
Thumbnail generation service is also obtained from the encoders.
- CDN and colocation sites: CDNs and colocation sites store popular and moderately popular content that is closer to the user for easy access. 
Colocation centers are used where it’s not possible to invest in a data center facility due to business reasons.


1) The user can upload a video by connecting to the web servers. 
The web server can run Apache or Lighttpd. 
Lighttpd is preferable because it can serve static pages and videos due to its fast speed.

2) Requests from the web servers are passed onto application servers that can contact various data stores to read or write user, videos, or videos’ metadata. 
There are separate web and application servers because we want to decouple clients’ services from the application and business logic. 
Different programming languages can be used on this layer to perform different tasks efficiently. 
For example, the C programming language can be used for encryption. 
Moreover, this gives us an additional layer of caching, where the most requested objects are stored on the application server while the most 
frequently requested pages will be stored on the web servers

3) Multiple storage units are used. Let’s go through each of these:
- Upload storage is used to store user-uploaded videos before they are temporarily encoded.
- User account data is stored in a separate database, whereas videos metadata is stored separately. 
The idea is to separate the more frequently and less frequently accessed storage clusters from each other for optimal access time. 
We can use MySQL if there are a limited number of concurrent reads and writes. 
However, as the number of users—and therefore the number of concurrent reads and writes—grows, we can move towards NoSQL types of data management systems.

- Since Bigtable is based on Google File System (GFS), it is designed to store a large number of small files with low retrieval latency.
It is a reasonable choice for storing thumbnails.

4) The encoders generate thumbnails and also store additional metadata related to videos in the metadata database. I
t will also provide popular and moderately popular content to CDNs and colocation servers, respectively.
5) The user can finally stream videos from any available site.


---------------------------------------------------------
---------------------------------------------------------
## 24 apr 23
**System design Youtube**

We require that our system is able to perform the following functions:
- Stream videos
- Upload videos
- Search videos according to titles
-  Like and dislike videos
- Add comments to videos
-  View thumbnails

It’s important that our system also meets the following requirements:
- High availability: The system should be highly available. High availability requires a good percentage of uptime. 
Generally, an uptime of 99% and above is considered good.
- Scalability: As the number of users grows, these issues should not become bottlenecks: storage for uploading content, 
the bandwidth required for simultaneous viewing, and the number of concurrent user requests should not overwhelm our application/web server.
- Good performance: A smooth streaming experience leads to better performance overall.
- Reliability: Content uploaded to the system should not be lost or damaged.

We don’t require strong consistency for YouTube’s design. 
Consider an example where a creator uploads a video. Not all users subscribed to the creator’s channel should immediately get the notification for uploaded content

To summarize, the functional requirements are the features and functionalities that the user will get, 
whereas the non-functional requirements are the expectations in terms of performance from the system

Estimation requires the identification of important resources that we’ll need in the system.
Hundreds of minutes of video content get uploaded to YouTube every minute. 
Also, a large number of users will be streaming content at the same time, which means that the following resources will be required:
- Storage resources will be needed to store uploaded and processed content.
- A large number of requests can be handled by doing concurrent processing. This means web/application servers should be in place to serve these users.
- Both upload and download bandwidth will be required to serve millions of users
Storage - 180 gb storage per Minute;

A lot of data transfer will be performed for streaming and uploading videos to YouTube.
This is why we need to calculate our bandwidth estimation too.
 Assume the upload:view ratio is 1:300—that is, for each uploaded video, we have 300 video views per second. 
 We’ll also have to keep in mind that when a video is uploaded, it is not in compressed format, while viewed videos can be of different qualities. 
Bandwidth - 200 gb ps;



---------------------------------------------------------
---------------------------------------------------------
## 19 apr 23
**System design how to**

Requirements: 
During this step, we gather all the requirements of the design problem and define its scope. 
Requirements include understanding what the service is, how it works, and what its main features are. 
Our goal in this step is to gather the functional and non-functional requirements of a service we are about to design

Estimation: 
As the name suggests, this step estimates the resources required to provide the service to a defined number of users. 
By resources, we mean the hardware or infrastructural resources. Some sample estimation questions are the following
- How many servers will we require to provide smooth services to 500 million daily active users (DAU)?
- How much storage do we need if we have to store 125 million tweets per day, and 20% of tweets contain media?

Storage schema (optional): 
This step involves articulating our data model—that is, 
we define which tables we need and what type of fields are part of each table. 
However, this is an optional step, and we may not exercise this effort in every design problem

High-level design: 
This step involves identifying the main components and building blocks we’ll use to design our desired system. 
We do this by getting inspiration from our functional and non-functional requirements

API design: 
The goal in this phase is to build interfaces for our service. 
Using these interfaces, users can call various services within our system. 
These interfaces are in the form of API calls and are generally a translation of our functional requirements

Detailed design: 
The detailed design starts by recognizing the limitations of the high-level design. 
We’ll capitalize on these limitations to evolve our design. 
During this step, we’ll finalize our design by mentioning all the components and building blocks that we’ll use.

Evaluation: 
This step will measure the effectiveness of our solution. 
In other words, we justify how our design fulfills the functional and non-functional requirements.

Distinctive component/feature: 
This step is to identify a unique aspect for each design problem and discuss it. 
For example, the Uber design problem has payment service and fraud detection as its unique feature. 
In contrast, Google Docs has concurrency control, which is required when different users want to edit the same section of a document simultaneously.

---------------------------------------------------------
---------------------------------------------------------
## 18 apr 23
**System design sharded counters**

Location-based counters represent their current count when the system reaches the set threshold in a specified time and the hashtag becomes a trend for some users.
 For example, Twitter sets 10,000 as a threshold. 
 When location-based hashtag counts reach 10,000, Twitter shows these hashtags in the trends timeline of the users of the respective country where the hashtag is being used. 
 The specified hashtag may be displayed worldwide if counts increase in all countries timeline. 
 
 The Top K tweets include accounts the user is following, tweets the user has liked, and retweets of accounts the user follows. 
 Tweets get priority in Top K problems based on follower count and time. 
 Twitter also shows promoted tweets and some tweets of accounts the user doesn’t follow in the user’s home timeline, depending on the tweet’s popularity and location.

An important concern is where we should place shared counters. 
Should they reside on the same nodes as application servers, in separate nodes in the data center, 
in nodes of CDN at the edge of a network near the end users? The exact answer to this question depends on our specific use case. 
For Twitter, we can compute counts by placing sharded counters near the user, which can also help to handle heavy hitter and Top K problems efficiently.

Reads can store counter values in appropriate data stores and rely on the respective data stores for read scalability. 
The Cassandra store can be used to maintain views, likes, comments, and many more counts of the users in the specified region. 
These counts represent the last computed sum of all shards of a particular counter

When users generate a timeline, read requests are forwarded to the nearest servers, and then the persisted values in the store can be used to respond. 
This storage also helps to show the region-wise Top K trends. 
The list of local Top K trends is sent to the application server, 
and then the application server sorts all the lists to make a list of global Top K trends. 
Eventually, the application server sends all counters’ details to the cache

We also need storage for the sharded counters, which store all information about them with their metadata. 
The Redis or Memcache servers can play a vital role here. 
For example, each tweet’s unique ID can become the key, and the value of this key can be a counter ID, or a list of counters’ IDs (like counter, reply counter, and so on). 
Furthermore, each counter ID has its own key-value store where the counter (for example, a likes counter) ID is a key and the value is a list of assigned shards.

We also need storage for the sharded counters, which store all information about them with their metadata. 
The Redis or Memcache servers can play a vital role here. 
For example, each tweet’s unique ID can become the key, and the value of this key can be a counter ID, 
or a list of counters’ IDs (like counter, reply counter, and so on). 
Furthermore, each counter ID has its own key-value store where the counter (for example, a likes counter) ID is a key and the value is a list of assigned shards.

The job of identifying the relevant counter and mapping all write requests to the appropriate counter in sharded counters can be done in parallel. 
We map the all-write request to the appropriate counter, and then each counter chooses a shard randomly based on some metrics to do increments and decrements. 
In contrast, we reduce periodically to aggregate the value of all shards of the particular counter. 
Then, these counter values can be stored in the Cassandra store.

---------------------------------------------------------
---------------------------------------------------------
## 17 apr 23
**System design sharded counters**

The decision about the number of shards depends on many factors that collectively try to predict the write load on a specific counter in the short term. 
For tweets, these factors include follower count. 
The tweet of a user with millions of followers gets more shards than a user with few followers on Twitter because there is a possibility that their tweets will get many, often millions, of likes. 
Sometimes, a celebrity tweet includes one or more hashtags. 
The system also creates the sharded counter for this hashtag because it has a high chance of being marked as a trend.


We need to monitor the write load for all the shards to appropriately route requests to specific shards, possibly using load balancers. 
Such a feedback mechanism can also help us decide when to close down some of the shards for a counter and when to add additional shards. 
This process does not only provide good performance for the end user but also utilizes our resources at near-optimal levels.

As we mentioned earlier, millions of users interact with our example celebrity’s tweet, which eventually sends a burst of write requests to the system. 
The system assigns each write request to the available shards of the specified counter of the particular tweet. 
How does the system select these shards operating on different computational units (nodes) to assign the write requests? 

When the user sends the read request, the system will aggregate the value of all shards of the specified counter to return the total count of the feature (such as like or reply). 
Accumulating values from all the shards on each read request will result in low read throughput and high read latency.

The decision of when the system will sum all shards values is also very critical. 
If there is high write traffic along with reads, it might be virtually impossible to get a real current value because by the time we report a read value to the client, it will have already changed. 
So, periodically reading all the shards of a counter and caching it should serve most of the use cases. 
By reducing the accumulation period, we can increase the accuracy of read values.

we can use sharded counters to solve a real-world problem known as the Top K problem. 
We’ll continue to use the real-time application Twitter as an example, where calculating trends is one of the Top K problems for Twitter. Here,
Many users use various hashtags in their tweets. 
It is a huge challenge to manage millions of hashtags’ counts to show them in individual users’ trends timelines based on their locality

The sharded counter is the key to the above problem. 
the system creates the counters for each hashtag and decides the shard count according to the user’s followers who used the hashtag in the tweet. 
When users on Twitter use the same hashtag again in their tweet, the count maintains the same counter created initially on the first use of that hashtag

- Region-wise hashtag count indicates the number of tweets with the same hashtag used within a specific geographical region. 
For example, thousands of tweets with the same tags from New York City suggest that users in the New York area may see this hashtag in their trends timeline.
- A time window indicates the amount of time during which tweets with specific tags are posted.

The global hashtag counter represents the total of all location-based counters.

---------------------------------------------------------
---------------------------------------------------------
## 16 apr 23
**System design sharded counters**

Here, it might be easy to count the likes for this single image, 
but what will we do when thousands of such images or videos are uploaded simultaneously by many celebrities, each with millions of followers. 
This problem is known as the heavy hitters problem.

As the number of concurrent writes increases for some counter (which might be a variable residing in a node’s memory), 
the lock contention increases non-linearly. 
After some point, we might spend most of the time acquiring the lock so that we could safely update the counter.

What will happen when a single tweet on Twitter gets a million likes, and the application server receives a write request against each like to increment the relevant counter? 
These millions of requests are eventually serialized in a queue for data consistency

A single counter for each tweet posted by a celebrity is not enough to handle millions of users. 
The solution to this problem is a sharded counter, also known as a distributed counter, where each counter has a specified number of shards as needed. 
These shards run on different computational units in parallel. 
We can improve performance and reduce contention by balancing the millions of write requests across shards.

Let’s assume that a famous YouTube channel with millions of subscribers uploads a new video. 
The server receives a burst of write requests for video views from worldwide users.
 First, a new counter initiates for a newly uploaded video. 
 The server forwards the request to the corresponding counter, and our system chooses the shard randomly and updates the shard value, which is initially zero. 
 In contrast, when the server receives read requests, it adds the values of all the shards of a counter to get the current total
 

When a user posts a tweet on Twitter, the \createCounter API is called. 
The system creates multiple counters for each newly created post by the user. 
The following is the list of main counters created against each new tweet:
- Tweet like counter
- Tweet reply counter
- Tweet retweet counter
- Tweet view counter in case a tweet contains video

Now, the question is how does the system decide the number of shards in each counter? 
The number of shards is critical for good performance. 
If the shard count is small for a specific write workload, we face high write contention, which results in slow writes. 
On the other hand, if the shard count is too high for a particular write profile, we encounter a higher overhead on the read operation. 
The reason for slower reads is because of the collection of values from different shards that might reside on different nodes inside geographically distributed data centers. 
The reading cost of a counter value rises linearly with the number of shards because the values of all shards of a respective counter are added. 
The writes scale linearly as we add new shards due to increasing requests. 
Therefore, there is a trade-off between making writes quick versus read performance. 

---------------------------------------------------------
---------------------------------------------------------
## 12 apr 23
**System design distributed task scheduler**

- Distributed queue: It consists of a queue and a queue manager. 
The queue manager adds, updates, or deletes tasks in the queue. It keeps track of the types of queues we use. 
It is also responsible for keeping the task in the queue until it executes successfully. 
In case a task execution fails, that task is made visible in the queue again. 
The queue manager knows which queue to run during the peak time and which queue to run during the off-peak time.

- Queue manager: The queue manager deletes a task from the queue if it executes successfully. 
It also makes the task visible if its previous execution failed. 
It retries for the allowed number of attempts for a task in case of a failed execution

- Resource manager: The resource manager knows which of the resources are free. 
It pulls the tasks from the distributed queue and assigns them resources. 
The resource manager keeps track of the execution of each task and sends back their statuses to the queue manager. 
If a task goes beyond its promised or required resource use, that task will be terminated, and the status is sent back to the task submitter, which will notify the client about the termination of the task through an error message.

- Monitoring service: It is responsible for checking the health of the resource manager and the resources. 
If some resource fails, it alerts the administrators to repair the resource or add new resources if required. 
If resources are not being used, it alerts the administrators to remove them or power them off.

We have the following three categories for our tasks:
- Tasks that can’t be delayed.
- Tasks that can be delayed.
- Tasks that need to be executed periodically (for example, every 5 minutes, or every hour, or every day).

Some tasks take very long to execute and occupy the resource blocking other tasks. 
The execution cap is an important parameter to consider while scheduling tasks

To prioritize the tasks, the task scheduler maintains a delay tolerance parameter for each task and executes the task close to its delay tolerance

There could be a time when resources are close to the overload threshold (for example, above 80% utilization). This is called peak time

The first component in our design was a rate limiter that is appropriately replicated and ensures availability. 
Task submission is done by several nodes. 
If a node that submits a task fails, the other nodes take its place.

We store the tasks in a persistent distributed database and push the tasks into the queue near their execution time.

---------------------------------------------------------
---------------------------------------------------------
## 11 apr 23
**System design distributed task scheduler**

The big components of our system are:
- Clients: They initiate the task execution.
- Resources: The task is executed on these components.
 - Scheduler: A scheduler performs processes between clients and resources and decides which task should get resources first

it is necessary to put the incoming tasks into a queue. It is because of the following reasons:
- We might not have sufficient resources available right now.
- There is task dependency, and some tasks need to wait for others.
- We need to decouple the clients from the task execution so that they can hand off work to our system. Our system then queues it for execution.

When a task comes for scheduling, it should contain the following information with it:
- Resource requirements: The requirements include how many CPU cores it needs, how much RAM is required to execute this task, how much disk space is required,
 what should the disk access rate be (input/output rate per second, or IOPS), 
 and how many TCP ports the task needs for the execution, and so on. But, it is difficult for the clients to quantify these requirements
- Dependency: Broadly, tasks can be of two types: dependent and independent.
Dependent tasks require executing one or more additional tasks for their complete execution. These tasks must run in a sequence.
Independent tasks don’t depend on the execution of any other task. Independent tasks can run in parallel. We should know whether a task is dependent or independent.

Base Components:
- Clients: The clients of the cloud providers are individuals or organizations from small to large businesses who want to execute their tasks;
- Rate limiter: The resources available for a client depend on the cost they pay;
- Task submitter: The task submitter admits the task if it successfully passes through the rate limiter;
- Unique ID generator: It assigns unique IDs to the newly admitted tasks;
- Database: All of the tasks taken by the task submitter are stored in a distributed database;
- Relational database (RDB): A relational database stores task IDs, user IDs, required resources, execution caps, the total number of attempts made by the client, delay tolerance;
- Graph database (GDB): This is a non-relational database that uses the graph data structure to store data. 
We use it to build and store a directed acyclic graph (DAG) of dependent tasks, topologically sorted by the task submitter, so that we can schedule tasks according to that DAG. 

Batching and prioritization: 
After we store the tasks in the RDB, the tasks are grouped into batches. 
Prioritization is based on the attributes of the tasks, such as delay tolerance or the tasks with short execution cap, and so on. The top priority tasks are pushed into the distributed queue, where 
limits the number of elements we can push into the queue. 
The value depends on many factors, such as currently available resources, the client or task priority, and subscription level.

---------------------------------------------------------
---------------------------------------------------------
## 10 apr 23
**System design distributed task scheduler**

A task is a piece of computational work that requires resources (CPU time, memory, storage, network bandwidth, and so on) for some specified time.
A system that mediates between tasks and resources by intelligently allocating resources to tasks so that task-level and system-level goals are met is called a task scheduler.

Single-OS-based node: It has many processes or tasks that contend for the node’s limited computational resources. 
So, we could use a local OS task scheduler that efficiently allocates resources to the tasks. 
It uses multi-feedback queues to pick some tasks and runs them on some processor

Cloud computing services: Where there are many distributed resources and various tasks from multiple tenants, there is a strong need for a task scheduler to utilize cloud computing resources 
efficiently and meet tenants’ demands. 
A local OS task scheduler isn’t sufficient for this purpose because the tasks are in the billions.

Large distributed systems: In this system, many tasks run in the background against a single request by a user. 

The process of deciding and assigning resources to the tasks in a timely manner is called task scheduling

- Tasks will come from many different sources, tenants, and sub-systems.
- Many resources will be dispersed in a data center (or maybe across many data centers)

The functional requirements:
- Submit tasks: The system should allow the users to submit their tasks for execution;
- Allocate resources: The system should be able to allocate the required resources to each task;
- Remove tasks: The system should allow the users to cancel the submitted tasks;
- Monitor task execution: The task execution should be adequately monitored and rescheduled if the task fails to execute;
- Efficient resource utilization: The resources (CPU and memory) must be used efficiently in terms of time, cost, and fairness. Efficiency means that we do not waste resources.;
- Release resources: After successfully executing a task, the system should take back the resources assigned to the task.
- Show task status: The system should show the users the current status of the task

The non-functional requirements:
- Availability: The system should be highly available to schedule and execute tasks.
- Durability: The tasks received by the system should be durable and should not be lost.
- Scalability: The system should be able to schedule and execute an ever-increasing number of tasks per day.
- Bounded waiting time: This is how long a task needs to wait before starting execution.

---------------------------------------------------------
---------------------------------------------------------
## 4 apr 23
**System design distributed logs**

In a distributed system, clients across the globe generate events by requesting services from different serving nodes.
The nodes generate logs while handling each of the requests. 
These logs are accumulated on the respective nodes.
In addition to the building blocks, let’s list the major components of our system:
- Log accumulator: An agent that collects logs from each node and dumps them into storage.
- Storage: The logs need to be stored somewhere after accumulation. We’ll choose blob storage to save our logs.
- Log indexer: The growing number of log files affects the searching ability. The log indexer will use the distributed search to search efficiently.
- Visualizer: The visualizer is used to provide a unified view of all the logs.

Each service will push its data to the log accumulator service. It is responsible for these actions:
- Receiving the logs.
- Storing the logs locally.
- Pushing the logs to a pub-sub system

We use the pub-sub system to cater to our scalability issue. 
Now, each server has its log accumulator (or multiple accumulators) push the data to pub-sub. 
The pub-sub system is capable of managing a huge amount of logs.

To fulfill another requirement of low latency, we don’t want the logging to affect the performance of other processes, so we send the logs asynchronously via a low-priority thread. 
By doing this, our system does not interfere with the performance of others and ensures availability

The data does not reside in pub-sub forever and gets deleted after a few days before being stored in archival storage
- Filterer: It identifies the application and stores the logs in the blob storage reserved for that application since we do not want to mix logs of two different applications.
- Error aggregator: It is critical to identify an error as quickly as possible.
- Alert aggregator: Alerts are also crucial. So, it is important to be aware of them early.
- Expiration Checker: Verifying the logs that have to be deleted. Verifying the logs to store in cold storage.
---------------------------------------------------------
---------------------------------------------------------
## 3 apr 23
**System design distributed logs**
A log file records details of events occurring in a software application.

Logging allows us to understand our code, locate unforeseen errors, fix the identified errors, and visualize the application’s performance.

Log analysis helps us with the following scenarios:
- To troubleshoot applications, nodes, or network issues;
- To adhere to internal security policies, external regulations, and compliance;
- To recognize and respond to data breaches and other security problems;
- To comprehend users’ actions for input to a recommender system;


Instead of logging all the information, we can use a sampler service that only logs a smaller set of messages from a larger chunk. 
This way, we can decide on the most important messages to be logged

Applications have the liberty to choose the structure of their log data. 
For example, an application is free to write to log as binary or text data, but it is often helpful to enforce some structure on the logs. 
The first benefit of structured logs is better interoperability between log writers and readers. 
Second, the structure can make the job of a log processing system easier.

We should be careful while logging.
 The logging information should only contain the relevant information and not breach security concerns.
 For secure data, we should log encrypted data. 
 We should consider the following few points while logging

- Avoid logging personally identifiable information (PII), such as names, addresses, emails, and so on.
- Avoid logging sensitive information like credit card numbers, passwords, and so on.
- Avoid excessive information. Logging all information is unnecessary. 
- The logging mechanism should be secure and not vulnerable because logs contain the application’s flow, and an insecure logging mechanism is vulnerable to hackers.

The functional requirements:
- Writing logs: The services of the distributed system must be able to write into the logging system;
- Searchable logs: It should be effortless for a system to find logs;
- Storing logging: The logs should reside in distributed storage for easy access;
- Centralized logging visualizer: The system should provide a unified view of globally separated services;

The non-functional requirements:
- Low latency: Logging is an I/O-intensive operation that is often much slower than CPU operations;
- Scalability: We want our logging system to be scalable;
- Availability: The logging system should be highly available to log the data

---------------------------------------------------------
## 28 mar 23
**System design distributed search**

The crawler collects content from the intended resource. 
For example, if we build a search for a YouTube application, the crawler will crawl through all of the videos on YouTube and extract textual content for each video. 
The content could be the title of the video, its description, the channel name, or maybe even the video’s annotation to enable an intelligent search based 
not only on the title and description but also on the content of that video. 
The crawler formats the extracted content for each video in a JSON document and stores these JSON documents in a distributed storage

The indexer fetches the documents from a distributed storage and indexes these documents using MapReduce, which runs on a distributed cluster of commodity machines. 
The indexer uses a distributed data processing system like MapReduce for parallel and distributed index construction. The constructed index table is stored in the distributed storage

The distributed storage is used to store the documents and the index

The user enters the search string that contains multiple words in the search bar

The searcher parses the search string, searches for the mappings from the index that are stored in the distributed storage, and returns the most matched results to the user. 
The searcher intelligently maps the incorrectly spelled words in the search string to the closest vocabulary words.
 It also looks for the documents that include all the words and ranks them.

The two most common techniques used for data partitioning in distributed indexing are these below:
- Document partitioning: In document partitioning, all the documents collected by the web crawler are partitioned into subsets of documents. Each node then performs indexing on a subset of documents that are assigned to it.
- Term partitioning: The dictionary of all terms is partitioned into subsets, with each subset residing at a single node. For example, a subset of documents is processed and indexed by a node containing the term “search

In term partitioning, a search query is sent to the nodes that correspond to the query terms. 
This provides more concurrency because a stream of search queries with different query terms will be served by different nodes. 
However, term partitioning turns out to be a difficult task in practice. 
Multiword queries necessitate sending long mapping lists between groups of nodes for merging, which can be more expensive than the benefits from the increased concurrency

In document partitioning, each query is distributed across all nodes, and the results from these nodes are merged before being shown to the user. 
This method of partitioning necessitates less inter-node communication.

- We have a document set already collected by the crawler.
- The cluster manager splits the input document set into N  number of partitions. The size of each partition is decided by the cluster manager given the size of the data, the computation, memory limits, 
and the number of nodes in the cluster. All the nodes may not be available for various reasons.
 The cluster manager monitors the health of each node through periodic heartbeats.
- After making partitions, the cluster manager runs indexing algorithms for all the partitions simultaneously on the nodes in a cluster. 
Each indexing process produces a tiny inverted index, which is stored on the node’s local storage. 
In this way, we produce tiny inverted indices rather than one large inverted index

- In the search phase, when a user query comes in, we run parallel searches on each tiny inverted index stored on the nodes’ local storage generating N queries.
- The search result from each inverted tiny index is a mapping list against the queried term (we assume a single word/term user query). The merger aggregates these mapping lists.
- After aggregating the mapping lists, the merger sorts the list of documents from the aggregated mapping list based on the frequency of the term in each document.
- The sorted list of documents is returned to the user as a search result. The documents are shown in sorted (ascending) order to the user.

Generally, a replication factor of three is enough. A replication factor of three means three nodes host the same partition and produce the index. 
One of the three nodes becomes the primary node, while the other two are replicas. 
Each of these nodes produces indexes in the same order to converge on the same state.

Rather than recomputing the index on each replica, we compute the inverted index on the primary node only. 
Next, we communicate the inverted index (binary blob/file) to the replicas. 
The key benefit of this approach is that it avoids using the duplicated amount of CPU and memory for indexing on replicas.

The MapReduce framework is implemented with the help of a cluster manager and a set of worker nodes categorized as Mappers and Reducers. As indicated by its name, MapReduce is composed of two phases:
- The Map phase
- The Reduction phase
Furthermore, the input to MapReduce is a number of partitions, or set of documents, whereas its output is an aggregated inverted index.

Cluster manager - The manager initiates the process by assigning a set of partitions to Mappers. Once the Mappers are done, the cluster manager assigns the output of Mappers to Reducers.
Mappers -  This component extracts and filters terms from the partitions assigned to it by the cluster manager. These machines output inverted indexes in parallel, which serve as input to the Reducers.
Reducers - The reducer combines mappings for various terms to generate a summarized index.

The cluster manager ensures that all worker nodes are efficiently utilized in the cluster. The MapReduce is built to work under partial failures. If one node fails, it reschedules the work on another node.

---------------------------------------------------------
---------------------------------------------------------
## 27 mar 23
**System design distributed search**

A search system is a system that takes some text input, a search query, from the user and returns the relevant content in a few seconds or less. There are three main components of a search system, namely:
- A crawler, which fetches content and creates documents.
- An indexer, which builds a searchable index.
- A searcher, which responds to search queries by running the search query on the index created by the indexer

Functional requirements:
- Search: Users should get relevant content based on their search queries

Non functional requirements of a distributed search system:
- Availability: The system should be highly available to the users;
- Scalability: The system should have the ability to scale with the increasing amount of data. In other words, it should be able to index a large amount of data;
- Fast search on big data: The user should get the results quickly, no matter how much content they are searching;
- Reduced cost: The overall cost of building a search system should be less;

Indexing is the organization and manipulation of data that’s done to facilitate fast and accurate information retrieval.
The simplest way to build a searchable index is to assign a unique ID to each document and store it in a database table, as shown in the following table. 
The first column in the table is the ID of the text and the second column contains the text from each document

The response time to a search query depends on a few factors:
- The data organization strategy in the database.
- The size of the data.
- The processing speed and the RAM of the machine that’s used to build the index and process the search query.

An inverted index is a HashMap-like data structure that employs a document-term matrix. 
Instead of storing the complete document as it is, it splits the documents into individual words. 
After this, the document-term matrix identifies unique words and discards frequently occurring words like “to,” “they,” “the,” “is,” and so on. 
Frequently occurring words like those are called terms. 
The document-term matrix maintains a term-level index through this identification of unique words and deletion of unnecessary terms.

For each term, the index computes the following information:
- The list of documents in which the term appeared.
- The frequency with which the term appears in each document.
- The position of the term in each document

Inverted index is one of the most popular index mechanisms used in document retrieval. 
It enables efficient implementation of boolean, extended boolean, proximity, relevance, and many other types of search algorithms

Here are some of the factors that we should keep in mind while designing an index:
- Size of the index: How much computer memory, and RAM, is required to keep the index. We keep the index in the RAM to support the low latency of the search.
- Search speed: How quickly we can find a word from an inverted index.
- Maintenance of the index: How efficiently the index can be updated if we add or remove a document.
- Fault tolerance: How critical it is for the service to remain reliable. Coping with index corruption, supporting whether invalid data can be treated in isolation, dealing with defective hardware, partitioning, and replication are all issues to consider here.
- Resilience: How resilient the system is against someone trying to game the system and guard against search engine optimization (SEO) schemes, since we return only a handful of relevant results against a search

These are the problems that come with the architecture of a centralized search system:
- SPOF (single point of failure)
- Server overload
- Large size of the index

SPOF: A centralized system is a single point of failure. If it’s dead, no search operation can be performed.
Server overload: If numerous users perform queries and the queries are complicated, it stresses the server (node).

Large size of the index - The size of the inverted index increases with the number of documents, placing resource demands on a single server. The bigger the computer system, the higher the cost and complexity of managing

---------------------------------------------------------
---------------------------------------------------------
## 22 mar 23
**System design blob storage**
Partitioning based on the blob IDs causes certain problems. 
For example, the blobs under a specific container or account may reside in different partitions that add overhead while reading or listing the blobs linked to a particular account or a particular container.

Finding specific blobs in a sea of blobs becomes more difficult and time-consuming with an increase in the number of blobs that are uploaded to the storage. 
The blob index solves the problem of blob management and querying.

To populate the blob index, we define key-value tag attributes on the blobs while uploading the blobs. 
We use multiple tags, like container name, blob name, upload date and time, and some other categories like the image or video blob, and so on

Listing is about returning a list of blobs to the user, depending on the user’s entered prefix. 
A prefix is a character or string that returns the blobs whose name begins with that particular character or string.

For pagination, we need a continuation token as a starting point for the part of the list that’s returned next. 
A continuation token is a string token that’s included in the response of a query if the total number of queried results exceeds the maximum number of results that we can return at once. 
As a result, it serves as a pointer, allowing the re-query to pick up where we left off.

Therefore, we have a service called a garbage collector that cleans up metadata inconsistencies later. 
The deletion of a blob causes the chunks associated with that blob to be freed. 
However, there could be an appreciable time delay between the time a blob is deleted by a user and the time of the corresponding increase in free space in the blob store. 
We can bear this appreciable time delay because, in return, we have a real-time fast response benefit for the user’s delete blob request

The caching of the blob store is usually done using CDN. 
The Azure blob store service cache the publicly accessible blob in Azure Content Delivery Network until that blob’s TTL (time-to-live) elapses. 
The origin server defines the TTL, and CDN determines it from the Cache-Control header in the HTTP response from the origin server

The replication part of our design makes the system available. 
For reading the data, we keep four replicas for each blob. 
Having replicas, we can distribute the request load. 
If one node fails, the other replica node can serve the request

The replication and monitoring services ensure the durability of the data. 
The data, once uploaded, is synchronously replicated within a storage cluster. 
If data loss occurs at one node, we can recover the data from the other nodes. 
The monitoring service monitors the storage disks

The partitioning and splitting of blobs into small-sized chunks helps us scale for billions of blob requests. 
Blobs are partitioned into separate ranges and served by different partition servers. 
The partition mappings specify which partition server will serve which particular blob range requests. 
Partitioning also provides automatic load balancing between partition servers to fulfill the blobs’ traffic needs

We save chunks of a blob on different data nodes that distribute the requests for a blob to multiple machines. 
Parallel fetching of chunks from multiple data nodes helps us achieve high throughput.

We achieve reliability through our monitoring techniques. 
For example, the heartbeat protocol keeps the master node updated on the status of data nodes. 
This enables the master node to request data from reliable nodes only. 
Furthermore, it takes necessary precautions to ensure reliable service. 
For example, the failure of a node triggers the master node to request an additional replica node

---------------------------------------------------------
---------------------------------------------------------
## 21 mar 23
**System design blob storage**

The client generates the upload blob request. 
If the client’s request successfully passes through the rate limiter, the load balancer forwards the client’s request to one of the front-end servers.
The front-end server then requests the master node for the data nodes it should contact to store the blob

The master node assigns the blob a unique ID using a unique ID generator system. 
It then splits the large-size blob into smaller, fixed-size chunks and assigns each chunk a data node where that chunk is eventually stored. 
The master node determines the amount of storage space that’s available on the data nodes using a free-space management system.

After determining the mapping of chunks to data nodes, the front-end servers write the chunks to the assigned data nodes.

We replicate each chunk for redundancy purposes. 
All choices regarding chunk replication are made at the master node. 
Hence, the master node also allocates the storage and data nodes for storing replicas.
The master node stores the blob metadata in the metadata storage.

After writing the blob, a fully qualified path of the blob is returned to the client. 
The path consists of the user ID, container ID where the user has added the blob, the blob ID, and the access level of the blob

There are three layers of abstractions:
- User account: Users uniquely get identified on this layer through their account_ID. Blobs uploaded by users are maintained in their containers.
- Container: Each user has a set of containers that are all uniquely identified by a container_ID. These containers contain blobs.
- Blob: This layer contains information about blobs that are uniquely identified by their blob_ID. This layer maintains information about the metadata of blobs that’s vital for achieving the availability and reliability of the system.

When a user uploads a blob, it’s split into small-sized chunks in order to be able to support the storage of large files that can’t fit in one contiguous location, i
n one data node, or in one block of a disk associated with that data node. 
The chunks for a single blob are then stored on different data nodes that have enough storage space available to store these chunks.
There are billions of blobs that are kept in storage. 
The master node has to store all the information about the blob’s chunks and where they are stored, so that it can retrieve the chunks on reads. 
The master node assigns an ID to each chunk.

We maintain three replicas for each block. 
When writing a blob, the master node identifies the data and the replica nodes using its free space management system. 
Besides handling data node failure, the replica nodes are also used to serve read/write requests, so that the primary node is not overloaded.

---------------------------------------------------------
---------------------------------------------------------
## 20 mar 23
**System design blob storage**

Blob store is a storage solution for unstructured data.
 We can store photos, audio, videos, binary executable codes, or other multimedia items in a blob store. 
 Every type of data is stored as a blob. 
 It follows a flat data organization pattern where there are no hierarchies, that is, directories, sub-directories, and so on.
 

Functional requirements
Here are the functional requirements of the design of a blob store:
- Create a container: The users should be able to create containers in order to group blobs;
- Put data: The blob store should allow users to upload blobs to the created containers;
- Get data: The system should generate a URL for the uploaded blob, so that the user can access that blob later through this URL;
- Delete data: The users should be able to delete a blob. If the user wants to keep the data for a specified period of time (retention time), our system should support this functionality;
- List blobs: The user should be able to get a list of blobs inside a specific container;
- Delete a container: The users should be able to delete a container and all the blobs inside it;
- List containers: The system should allow the users to list all the containers under a specific account;

Non-functional requirements of a blob store system:
- Availability: Our system should be highly available.
- Durability: The data, once uploaded, shouldn’t be lost unless users explicitly delete that data.
- Scalability: The system should be capable of handling billions of blobs.
- Throughput: For transferring gigabytes of data, we should ensure a high data throughput.
- Reliability: Since failures are a norm in distributed systems, our design should detect and recover from failures promptly.
- Consistency: The system should be strongly consistent. Different users should see the same view of

Since the blob store is a read-intensive store, most of the bandwidth is required for outgoing traffic. 
Considering the aforementioned assumptions, we calculate the bandwidth required for outgoing traffic using the following formula:

Here is a list of components that we use in the blob store design:
- Client: This is a user or program that performs any of the API functions that are specified.
- Rate limiter: A rate limiter limits the number of requests based on the user’s subscription or limits the number of requests from the same IP address at the same time.
- Load balancer: A load balancer distributes incoming network traffic among a group of servers. 
- Front-end servers: Front-end servers forward the users’ requests for adding or deleting data to the appropriate storage servers.
- Data nodes: Data nodes hold the actual blob data. Blobs are split into small, fixed-size pieces called chunks. A data node can accommodate all of the chunks of a blob or at least some of them.
- Master node: A master node is the core component that manages all data nodes. It stores information about storage paths and the access privileges of blobs. 

- Metadata storage: Metadata storage is a distributed database that’s used by the master node to store all the metadata. 
Metadata consists of account metadata, container metadata, and blob metadata.
Account metadata contains the account information for each user and the containers held by each account.
Container metadata consists of the list of the blobs in each container.
Blob metadata consists of where each blob is stored. The blob metadata is discussed in detail in the next lesson.

- Monitoring service: A monitoring service monitors the data nodes and the master node. It alerts the administrator in case of disk failures that require human intervention.
- Administrator: An administrator is responsible for handling notifications from the monitoring services and conducting routine checkups of the overall service to ensure reliability

---------------------------------------------------------
---------------------------------------------------------
## 17 mar 23
**Тимлид - рефлексия**

Рефлексия – мыслительный процесс, направленный на анализ своих эмоций, мыслей, поведения. По сути является личной ретроспективой.

Представляет из себя разбор ситуаций, собственных действий и их последствий, мыслей и чувств. Цели такого анализа:
- Возможность продумать оптимальную модель поведения в подобных ситуациях. Сделать выводы из совершенных ошибок.
- Нахождение решения не только для текущего кейса, но и для всего класса подобных случаев.
- Появление лучших жизненных установок. Превращение в лучшую версию себя.
- Отпускание неприятных воспоминаний за счёт детального разбора ситуации

Примеры плохого поведения
- Негативные кейсы не разбираются.
- Выводы не делаются, поведение в аналогичных ситуациях не меняется.
- Рефлексия проводится не регулярно.

Примеры хорошего поведения
- Лид выходит из негативных ситуаций с минимальными потерями.
- Ошибки совершаются, но не становятся системными.
- Разбираются не только негативные исходы, но и положительные кейсы. Понятно, что помогло получить ожидаемый результат

Проведение рефлексии:
- Определите предмет рефлексии. Это может быть некоторый промежуток времени или конкретный кейс.
- Проводите её письменно. Это помогает сфокусироваться, выстроить причинно-следственную цепочку, через некоторое время вернуться к этим записям и пересмотреть их.
- Выделите определённый временной слот, когда вы будете заниматься рефлексией. Это поможет сделать это действие привычкой.
- Подготовьте список вопросов, которые помогут вашим рассуждениям. Например, такие:
Что получилось хорошо? Что стало причиной?
Как я мог поступить лучше? Как можно было достичь лучшего результата?
Что я думаю про конкретные кейсы, что чувствую?
Периодически возвращайтесь и перечитывайте ваши размышления. Если видите изъян в логике, смело исправляйте.
Всегда старайтесь привести рефлексию к каким-то конкретным результатам. 
Если речь идёт про решение узких прикладных задач, то результатом может быть какой-то action plan. 
Если речь про что-то более абстрактное – то результатом может быть появление нового принципа или правила, которым вы будете руководствоваться в аналогичных ситуациях.

Попробуйте писать в физическом блокноте, даже если привыкли все делать в электронном виде. Сам темп написания текста от руки заставляет хорошо обдумывать написанное.
---------------------------------------------------------
---------------------------------------------------------
## 14 mar 23
**System design - rate limiter**

The decision-maker takes decisions based on the throttling algorithms. 
The throttling can be hard, soft, or elastic. 
Based on soft or elastic throttling, requests are allowed more than the defined limit. 
These requests are either served or kept in the queue and served later, upon the availability of resources. 
Similarly, if hard throttling is used, requests are rejected, and a response error is sent back to the client

There is a possibility of a race condition in a situation of high concurrency request patterns. 
It happens when the “get-then-set” approach is followed.
Another method that could be used is the “set-then-get” approach, wherein a value is incremented in a very performant fashion, avoiding the locking approach.

To avoid numerous computations in the client’s critical path, we should divide the work into offline and online parts.
Initially, when a client’s request is received, the system will just check the respective count. 
If it is less than the maximum limit, the system will allow the client’s request. 
In the second phase, the system updates the respective count and cache offline. 
For a few requests, this won’t have any effect on the performance, but for millions of requests, this approach increases performance significantly.

Result:
- Availability: If a rate limiter fails, multiple rate limiters will be available to handle the incoming requests. So, a single point of failure is eliminated;
- Low latency: Our system retrieves and updates the data of each incoming request from the cache instead of the database. First, the incoming requests are forwarded if they do not exceed the rate limit, and then the cache and database are updated;
- Scalability: The number of rate limiters can be increased or decreased based on the number of incoming requests within the defined limit;
---------------------------------------------------------
---------------------------------------------------------
## 13 mar 23
**System design - rate limiter**

Scenarios where rate limiters can be used to make the service more reliable.
- Preventing resource starvation: Some denial of service incidents are caused by errors in software or configurations in the system, which causes resource starvation. 
Such attacks are referred to as friendly-fire denial of service. 
One of the common use cases of rate limiters is to avoid resource starvation caused by such denial of service attacks, whether intentional or unintentional.
- Managing policies and quotas: There is also a need for rate limiters to provide a fair and reasonable use of resources’ capacity when they are shared among many users. 
The policy refers to applying limits on the time duration or quantity allocated (quota).
- Controlling data flow: Rate limiters could also be used in systems where there is a need to process a large amount of data. 
Rate limiters control the flow of data to distribute the work evenly among different machines, avoiding the burden on a single machine.
- Avoiding excess costs: Rate limiting can also be used to control the cost of operations. 
For example, organizations can use rate limiting to prevent experiments from running out of control and avoid large bills. 
Some cloud service providers also use this concept by providing freemium services to certain limits, which can be increased on request by charging from users


Functional requirements
- To limit the number of requests a client can send to an API within a time window;
- To make the limit of requests per window configurable;
- To make sure that the client gets a message (error or notification) whenever the defined threshold is crossed within a single server or combination of servers;

Non-functional requirements
- Availability: Essentially, the rate limiter protects our system. Therefore, it should be highly available;
- Low latency: Because all API requests pass through the rate limiter, it should work with a minimum latency without affecting the user experience;
- Scalability: Our design should be highly scalable. It should be able to rate limit an increasing number of clients’ requests over time;

Rate limiter can perform three types of throttling.
- Hard throttling: This type of throttling puts a hard limit on the number of API requests. So, whenever a request exceeds the limit, it is discarded.
- Soft throttling: Under soft throttling, the number of requests can exceed the predefined limit by a certain percentage. 
For example, if our system has a predefined limit of 500 messages per minute with a 5% exceed in the limit, we can let the client send 525 requests per minute.
- Elastic or dynamic throttling: In this throttling, the number of requests can cross the predefined limit if the system has excess resources available. 
However, there is no specific percentage defined for the upper limit. For example, if our system allows 500 requests per minute, it can let the user send more than 500 requests when free resources are available

There are three different ways to place the rate limiter.
- On the client side: It is easy to place the rate limiter on the client side. However, this strategy is not safe because it can easily be tampered with by malicious activity. Moreover, the configuration on the client side is also difficult to apply in this approach.
- On the server side: As shown in the following figure, the rate limiter is placed on the server-side. In this approach, a server receives a request that is passed through the rate limiter that resides on the server.
- As middleware: In this strategy, the rate limiter acts as middleware, throttling requests to API servers as shown in the following figure.

One rate limiter might not be enough to handle enormous traffic to support millions of users. 
Therefore, a better option is to use multiple rate limiters as a cluster of independent nodes. 
Since there will be numerous rate limiters with their corresponding counters (or their rate limit), there are two ways to use databases to store, retrieve, and update the counters along with the user information

Components:
- Rule database: This is the database, consisting of rules defined by the service owner. 
Each rule specifies the number of requests allowed for a particular client per unit of time.
- Rules retriever: This is a background process that periodically checks for any modifications to the rules in the database. 
The rule cache is updated if there are any modifications made to the existing rules.
- Throttle rules cache: The cache consists of rules retrieved from the rule database. 
The cache serves a rate-limiter request faster than persistent storage. 
As a result, it increases the performance of the system. So, when the rate limiter receives a request against an ID (key), it checks the ID against the rules in the cache
- Decision-maker: This component is responsible for making decisions against the rules in the cache. 
This component works based on one of the rate-limiting algorithms
- Client identifier builder: This component generates a unique ID for a request received from a client. 
This could be a remote IP address, login ID, or a combination of several other attributes, due to which a sequencer can’t be used here. 
This ID is considered as a key to store the user data in the key-value database. So, this key is passed to the decision-maker for further service decisions.

When a request is received, the client identifier builder identifies the request and forwards it to the decision-maker. 
The decision-maker determines the services required by request, then checks the cache against the number of requests allowed, as well as the rules provided by the service owner. 
If the request does not exceed the count limit, it is forwarded to the request processor, which is responsible for serving the request.

---------------------------------------------------------
---------------------------------------------------------
## 9 mar 23
**Тимлид управление тех долгом**

Технический долг – это, например, плохо спроектированная архитектура или запутанный код.

Примеры плохого поведения
- Программист делает ошибки при разработке проекта, которые не отлавливаются на code review или статическим анализом;
- Когда ситуация вынуждает осознанно писать код быстро и некачественно, к нему в будущем не возвращаются для рефакторинга;
- Объем технического долга не известен руководству;
- В команде не выделяется время на периодическое исправление технического долга;
- Разработчики игнорируют мелкие дефекты качества и не пытаются их исправить на месте по правилу бойскаута;

Примеры хорошего поведения
- В каждом спринте выделяется определённый процент времени на решение технического долга;
- Весь крупный технический долг инвентаризирован;
- Неумышленный технический долг отлавливается ручными и автоматизированными проверками качества;

Стоит посмотреть на SonarQube.

---------------------------------------------------------
---------------------------------------------------------
## 8 mar 23
**System design - pub sub**

The broker server is the core component of our pub-sub system. 
It will handle write and read requests. 
A broker will have multiple topics where each topic can have multiple partitions associated with it. 
We use partitions to store messages in the local storage for persistence. 
Consequently, this improves availability. 
Partitions contain messages encapsulated in segments. 
Segments help identify the start and end of a message using an offset address. 
Using segments, consumers consume the message of their choice from a partition by reading from a specific offset address.

Topic is a persistent sequence of messages stored in the local storage of the broker. 
Once the data has been added to the topic, it cannot be modified. 
Reading and writing a message from or to a topic is an I/O task for computers, and scaling such tasks is challenging. 
This is the reason we split the topics into multiple partitions.

We’ll allocate the partitions to various brokers in the system. 
This just means that different partitions of the same topic will be in different brokers. 
We’ll follow strict ordering in partitions by adding newer content at the end of existing messages.

We have various brokers in our system. Each broker has different topics. The topic is divided into multiple partitions.

Message will be stored in a segment. 
We’ll identify each segment using an offset. 
Since these are immutable records, the readers are independent and they can read messages anywhere from this file using the necessary API functions.

We’ll have multiple brokers in our cluster. The cluster manager will perform the following tasks:
- Broker and topics registry: This stores the list of topics for each broker.
- Manage replication: The cluster manager manages replication by using the leader-follower approach. 
One of the brokers is the leader. 
If it fails, the manager decides who the next leader is. 
In case the follower fails, it adds a new broker and makes sure to turn it into an updated follower. 
It updates the metadata accordingly. 
We’ll keep three replicas of each partition on different brokers

The consumer manager will manage the consumers. It has the following responsibilities:
- Verify the consumer: The manager will fetch the data from the database and verify if the consumer is allowed to read a certain message. 
For example, if the consumer has subscribed to Topic A (but not to Topic B), then it should not be allowed to read from Topic B. 
The consumer manager verifies the consumer’s request.

- Retention time management: The manager will also verify if the consumer is allowed to read the specific message or not. 
If, according to its retention time, the message should be inaccessible to the consumer, then it will not allow the consumer to read the message.
- Message receiving options management: There are two methods for consumers to get data. 
The first is that our system pushes the data to its consumers. 
This method may result in overloading the consumers with continuous messages. 
Another approach is for consumers to request the system to read data from a specific topic. 
The drawback is that a few consumers might want to know about a message as soon as it is published, but we do not support this function. 
Therefore, we’ll support both techniques. Each consumer will inform the broker that it wants the data to be pushed automatically or it needs the data to read itself. 
We can avoid overloading the consumer and also provide liberty to the consumer. We’ll save this information in the relational database along with other consumer details.
- Allow multiple reads: The consumer manager stores the offset information of each consumer. 
We’ll use a key-value to store offset information against each consumer. 
It allows fast fetching and increases the availability of the consumers. 
If Consumer 1 has read from offset 0 and has sent the acknowledgment, we’ll store it. 
So, when the consumer wants to read again, we can provide the next offset to the reader for reading the message

---------------------------------------------------------
---------------------------------------------------------
## 7 mar 23
**System design - pub sub**

Components:
- Topic queue: Each topic will be a distributed messaging queue so we can store the messages sent to us from the producer.
 A producer will write their messages to that queue.
- Database: We’ll use a relational database that will store the subscription details. 
For example, we need to store which consumer has subscribed to which topic so we can provide the consumers with their desired messages. 
We’ll use a relational database since our consumer-related data is structured and we want to ensure our data integrity.
- Message director: This service will read the message from the topic queue, fetch the consumers from the database, and send the message to the consumer queue.
- Consumer queue: The message from the topic queue will be copied to the consumer’s queue so the consumer can read the message. For each consumer, we’ll define a separate distributed queue.
- Subscriber: When the consumer requests a subscription to a topic, this service will add an entry into the database.

Using the distributed messaging queues makes our design simple. 
However, the huge number of queues needed is a significant concern. 
If we have millions of subscribers for thousands of topics, defining and maintaining millions of queues is expensive. 
Moreover, we’ll copy the same message for a topic in all subscriber queues, which is unnecessary duplication and takes up space.

In messaging queues, the message disappears after the reader consumes it. 
So, what if we add a counter for each message? 
The counter value decrements as a subscriber consumes the message. 
It does not delete the message until the counter becomes zero. Now, we don’t need to keep a separate queue for each reader

At a high level, the pub-sub system will have the following components:
- Broker: This server will handle the messages. It will store the messages sent from the producer and allow the consumers to read them.
- Cluster manager: We’ll have numerous broker servers to cater to our scalability needs. We need a cluster manager to supervise the broker’s health. It will notify us if a broker fails.
- Storage: We’ll use a relational database to store consumer details, such as subscription information and retention period.
- Consumer manager: This is responsible for managing the consumers. For example, it will verify if the consumer is authorized to read a message from a certain topic or not.

Acknowledgment: An acknowledgment is used to notify the producer that the received message has been stored successfully. 
The system will wait for an acknowledgment from the consumer if it has successfully consumed the message.

Retention time: The consumers can specify the retention period time of their messages. 
The default will be seven days, but it is configurable. Some applications like banking applications require the data to be stored for a few weeks as a business requirement, 
while an analytical application might not need the data after consumption.

---------------------------------------------------------
---------------------------------------------------------
## 6 mar 23
**System design - pub sub**

Publish-subscribe messaging, often known as pub-sub messaging, is an asynchronous service-to-service communication method that’s popular in serverless and microservices architectures. 
Messages can be sent asynchronously to different subsystems of a system using the pub-sub system. 
All the services subscribed to the pub-sub model receive the message that’s pushed into the system.

A few use cases of pub-sub are listed below:
- Improved performance: The pub-sub system enables push-based distribution, alleviating the need for message recipients to check for new information and changes regularly. 
It encourages faster response times and lowers the delivery latency.
- Handling ingestion: The pub-sub helps in handling log ingestion. 
The user-interaction data can help us figure out useful analyses about the behavior of users.
 We can ingest a large amount of data to the pub-sub system, so much so that it can deliver the data to any analytical system to understand the behavior patterns of users. 
 Moreover, we can also log the details of the event that’s happening while completing a request from the user. 
 Large services like Meta use a pub-sub system called Scribe to know exactly who needs what data, and remove or archive processed or unwanted data. 
 Doing this is necessary to manage an enormous amount of data.
- Real-time monitoring: Raw or processed messages of an application or system can be provided to multiple applications to monitor a system in real time.
- Replicating data: The pub-sub system can be used to distribute changes. 
For example, in a leader-follower protocol, the leader sends the changes to its followers via a pub-sub system. 
It allows followers to update their data asynchronously. 
The distributed caches can also refresh themselves by receiving the modifications asynchronously. 
Along the same lines, applications like WhatsApp that allow multiple views of the same conversation—for example, on a mobile phone and a computer’s browser—can elegantly work using a pub-sub, 
where multiple views can act either as a publisher or a subscriber.


The pub-sub system and queues are similar because they deliver information that’s produced by the producer to the consumer. 
The difference is that only one consumer consumes a message in the queue, while there can be multiple consumers of the same message in a pub-sub system.

Functional requirements
- Create a topic: The producer should be able to create a topic.
- Write messages: Producers should be able to write messages to the topic.
- Subscription: Consumers should be able to subscribe to the topic to receive messages.
- Read messages: The consumer should be able to read messages from the topic.
- Specify retention time: The consumers should be able to specify the retention time after which the message should be deleted from the system.
- Delete messages: A message should be deleted from the topic or system after a certain retention period as defined by the user of the system.

Non-functional requirements
- Scalable: The system should scale with an increasing number of topics and increasing writing (by producers) and reading (by consumers) load.
- Available: The system should be highly available, so that producers can add their data and consumers can read data from it anytime.
- Durability: The system should be durable. Messages accepted from producers must not be lost and should be delivered to the intended subscribers.
- Fault tolerance: Our system should be able to operate in the event of failures.
- Concurrent: The system should handle concurrency issues where reading and writing are performed simultaneously.

---------------------------------------------------------
---------------------------------------------------------
## 4 mar 23
**Тимлид - стратегическое видение**
Стратегии в рамках компании можно условно разделить на четыре уровня:
- Стратегия компании. В эту категорию включаются глобальные решения о курсе компании, о её положении на рынке. 
Обычно такие решения принимаются владельцами и топ-менеджерами компании.

- Стратегия продукта. Если у компании имеется несколько продуктов, то каждый продукт может иметь собственную стратегию развития. 
К примеру, решение о переходе на новый технологический стек может быть продиктовано вероятными грядущими изменениями в бизнес-модели продукта.

- Стратегия команды. Например, понимание того, как может измениться модель вашего взаимодействия с клиентами под воздействием внутренних и внешних факторов, 
может помочь принять решение о необходимости смены методологии разработки.

- Индивидуальная стратегия. У каждого человека есть и своя личная стратегия развития и роста. 
Вы можете влиять на стратегии членов своей команды. Например, если вы ожидаете, что в скором времени в ваш продукт нужно будет внедрять машинное обучение, 
вы можете включить соответствующий пункт в план индивидуального развития одного из аналитиков.

Задача тимлида - формировать стратегическое видение на своём уровне ответственности таким образом, 
чтобы его курс совпадал с направлением более высокоуровневых стратегий. 
Не стоит ожидать, что формирование стратегии приведёт к быстрому, заметному результату, в краткосрочной перспективе ваши стратегические решения могут даже привести к временному снижению эффективности.

Стратегия должна отвечать на несколько ключевых вопросов:
- В чем цель и смысл существования нашей компании (продукта, команды)?
- Какие способы достижения цели мы будем использовать?
- Какие компетенции и ресурсы необходимы, чтобы использовать эти способы?
- Какие инструменты мы будем использовать для мониторинга стратегии и оценки её результатов?

Примеры плохого поведения
- Описание стратегии носит хаотичный характер, является простым набором всех входных факторов и непонятно вашей команде или другим менеджерам;
- Стратегия непрозрачна и недоступна для ознакомления и оценки другим членам команды;
- Стратегия слишком размыта или нереалистична, не учитывает ограничения по имеющимся ресурсам;
- Стратегии разных уровней (компании, продуктов, команд, людей) разрабатываются отдельно и никак не связаны друг с другом;
- Стратегия формируется в ультимативном порядке и исходит от одного человека исключительно на основе его видения будущего;
- Стратегия отсутствует в принципе, решения принимаются на основе их сиюминутной выгоды;
- Ожидается, что формирование стратегии приведёт к какому-то быстрому заметному результату;
- Стратегия формируется на слишком долгий срок и успевает устареть;

Примеры хорошего поведения
- Текущие, тактические решения сверяются с долгосрочной стратегией;
- При этом стратегия регулярно проходит ревизию исходя из текущей ситуации, итеративно изменяется и адаптируется;
- Стратегия соответствует конечным целям компании, не разрабатывается в вакууме, без понимания её назначения;
- Степень следования стратегии измеряется с помощью понятных, прозрачных метрик;
- Перед разработкой стратегии производится сбор всей имеющейся информации - о команде, продукте, конкурентах, трендах и внешних факторах. По необходимости производятся консультации экспертов;
- Стратегия не насаждается сверху директивно, её основные положения объясняются и защищаются;
- Хорошая стратегия сфокусирована, не распыляется между большим количеством действий, интересов и альтернатив

https://tlroadmap.io/self-skills/thinking/strategic-vision.html

---------------------------------------------------------
---------------------------------------------------------
## 3 mar 23
**Тимлид - принятие решений**

Принятие решений — процесс по решению проблемы или улучшению ситуации, завершающийся выбором оптимального варианта.
Процесс принятия решений может быть интуитивным и завершиться мгновенно, а также может быть рациональным и требовать много внимания, времени и других ресурсов.

Для принятия решений необходимы ресурсы, чтобы предпринимать шаги к решению, 
поэтому тимлид должен принимать решения в рамках соразмерных задач и проблем. 

Если проблемы выходят за рамки его компетенции, нужно делегировать принятие решения тем, кто обладает соразмерными проблеме ресурсами, например, руководству. 
В то же время, принятие решений в рамках соответствующих компетенций необходимо делегировать и специалистам команды.

Примеры плохого поведения
- Рассматривает слишком мало вариантов при принятии решения. Не рассматривает комбинацию вариантов: ограничивается выбором одного варианта, когда можно осуществить сразу оба.
- И наоборот, распыляет и перегружает внимание избыточными вариантами.
- Действует шаблонно, поэтому не может принять нестандартное решение в нестандартной ситуации.
- Переоценивает первое импульсивное впечатление о проблеме и не принимает в расчёт новые аргументы, которые открылись в ходе решения. 
Чрезмерно уверен в устоявшихся взглядах и не учитывает, что такие убеждения могут оказаться ошибочными.
- Продолжает по инерции действовать в рамках ошибочного решения, которое принял ранее, даже когда осознал его нецелесообразность.
- Не учитывает влияние своего эмоционального состояния, давления или стресса в момент принятия важного решения.
- Откладывает решение, даже когда негативные обстоятельств уже усугубляются с течением времени.
- И наоборот, принимает поспешные решения, когда ещё не созрел и испытывает нехватку ресурсов для решения.
- Фокусируется на второстепенном, решает не ту задачу.
- Выбирает путь наименьшего сопротивления.

Примеры хорошего поведения
- Использует лучшие практики при принятии решения — это путь, который уже кто-то прошёл.
- Ищет решения на разных уровнях, расширяет круг рассматриваемых вариантов и заимствует идеи решений, начиная от соседней команды, до решений из совершенно другой индустрии.
- Рассматривает одновременно несколько возможных решений, чем вводит здоровую конкуренцию вариантов решения и тем предотвращает риск эмоциональной привязанности к единственному варианту 
и невозможности вовремя его отбросить при несостоятельности.
- Если проблема простая и не требуется учитывать множества факторов, то принимает решение быстро и интуитивно, этим затрачивает целесообразное количество времени и ресурсов.

Практика

Вести post-mortem проблем и принятых решений, анализировать их эффективность постфактум и так улучшать результаты будущих решений.
Если есть время и ресурсы и необходимо учитывать большой объем критериев, принимает выверенное аналитическое решение по следующим этапам:
- Выявляет проблему.
- Осмысливает проблему и формулирует задачи.
- Определяет цели.
- Собирает информацию, определяет множество альтернатив.
- Оценивает эти альтернативы по критериям.
- Определяет шкалу критериев принятия решения.
- Переходит от субъективных оценок к числовым.
- Устанавливает пороги отсечения — по каким критериям будут отметаться неподходящие варианты.
- Оценивает результат и решает, есть ли необходимость пересмотра оценок, шкал и порогов отсечения.
- Выбирает процедуры обобщения некоторых оценок.
- Переходит к реализации выбранного варианта.

В сложных и составных задачах на этом последнем этапе использует итеративный метод: окончательно оценивает выбор и решение и если что-то не так — возвращается назад к первому этапу.

https://tlroadmap.io/self-skills/thinking/decision-making.html

---------------------------------------------------------
---------------------------------------------------------
## 2 mar 23
**System Design - Distributed queue**

Back-end service - This is the core part of the architecture where major activities take place.
When the frontend receives a message, it refers to the metadata service to determine the host where the message needs to be sent. 
The message is then forwarded to the host and is replicated on the relevant hosts to overcome a possible availability issue. 
The message replication in a cluster on different hosts can be performed using one of the following two models:

- Primary-secondary model. Each node is considered a primary host for a collection of queues. 
The responsibility of a primary host is to receive requests for a particular queue and be fully responsible for data replication. 
The request is received by the frontend, which in turn communicates with the metadata service to determine the primary host for the request.

For example, suppose we have two queues with the identities 101 and 102 residing on four different hosts A, B, C, and D. 
In this example, instance B is the primary host of queue 101 and the secondary hosts where the queue 101 is replicated are A and C. 
As the frontend receives message requests, it identifies the primary server from the internal cluster manager through the metadata service. 
The message is retrieved from the primary instance, which is also responsible for deleting the original message upon usage and all of its replicas.

The internal cluster manager is a component that’s responsible for mapping between the primary host, secondary hosts, and queues. 
Moreover, it also helps in the primary host selection. Therefore, it needs to be reliable, scalable, and performant

- A cluster of independent hosts. We have several clusters of multiple independent hosts that are distributed across data centers. 
As the frontend receives a message, it determines the corresponding cluster via the metadata service from the external cluster manager. 
The message is then forwarded to a random host in the cluster, which replicates the message in other hosts where the queue is stored.
The same process is applied to receive message requests from the consumer. Similar to the first approach, the randomly selected host is responsible for message delivery and cleanup upon a successful processing of the message.
Furthermore, another component called an external cluster manager is introduced, which is accountable for maintaining the mapping between queues and clusters. 
The external cluster manager is also responsible for queue management and cluster assignment to a particular queue.


Functional requirements
- Queue creation and deletion: When a request for a queue is received at the frontend, the queue is created with all the necessary details provided by the client after undergoing some essential checks. 
The corresponding cluster manager assigns servers to the newly created queue and updates the information in the metadata stores and caches through a metadata service.
Similarly, the queue is deleted when the client doesn’t need it anymore. The responsible cluster manager deallocates the space occupied by the queue and, consequently, deletes the data from all the metadata stores and caches.

- Send and receive messages: Producers can deliver messages to specific queues once they are created. 
At the backend, receiving messages are sorted based on time stamps to preserve their order and are placed in the queue. Similarly, a consumer can retrieve messages from a specified queue.

- Message deletion: Primarily, two options are used to delete a message from a queue.
The first option is to not delete a message after it’s consumed. 
However, in this case, the consumer is responsible for keeping track of what’s consumed. 
For this, we need to maintain the order of messages in the queue and keep track of a message within a queue. 
A job can then delete the message when the expiration conditions are met. Apache Kafka mostly uses this idea where multiple processes can consume a message.
The second approach also doesn’t delete a message after it’s consumed. However, it’s made invisible for some time via an attribute—for example, visibility_timeout. 
This way, the other consumers are unable to get messages that have already been consumed. 
The message is then deleted by the consumer via an API call.

In both cases, the message being retrieved by the consumer is only deleted by the consumer. 
The reason behind this is to provide high durability if a consumer can’t process a message due to some failure. 
In such a case, in the absence of a delete call, the consumer can retrieve the message again when it comes back.
Moreover, this approach also provides at-least-once delivery semantic. For example, when a worker fails to process the message, another worker can retrieve the message after it becomes visible in the queue.

Non-functional requirements
- Durability: To achieve durability, the queues’ metadata is replicated on different nodes. 
Similarly, when a message is received, it’s replicated in the queues that reside on different nodes. 
Therefore, if a node fails, other nodes can be used to deliver or retrieve messages.

- Scalability: Our design components, such as front-end servers, metadata servers, caches, back-end clusters, and more are horizontally scalable.
 We can add to or remove their capacity to match our needs. The scalability can be divided into two dimensions:
Increase in the number of messages: When the number of messages touches a specific limit—say, 80%—the specified queue is expanded. 
Similarly, the queue is shrunk when the number of messages drops below a certain threshold.
Increase in the number of queues: With an increasing number of queues, the demand for more servers also increases, in which case the cluster manager is responsible for adding extra servers. 
We commission nodes so that there is performance isolation between different queues. An increased load on one queue shouldn’t impact other queues.

- Availability: Our data components, metadata and actual messages, are properly replicated inside or outside the data center, and the load balancer routes traffic around failed nodes. 
Together, these mechanisms make sure that our system remains available for service under faults.

- Performance: For better performance we use caches, data replication, and partitioning, which reduces the data reads and writes time. 
Moreover, the best effort ordering strategy for ordering messages is there to use to increase the throughput and lower the latency when it’s necessary. 
In the case of strict ordering, we also suggest time-window based sorting to potentially reduce the latency.


---------------------------------------------------------
---------------------------------------------------------
## 1 mar 23
**System Design - Distributed queue**

High level design
- Queue data is replicated using either a primary-secondary or quorum-like system inside a cluster. 
Our service can use data partitioning if the queue gets too long to fit on a server. 
We can use a consistent hashing-like scheme for this purpose, or we may use a key-value store where the key might be the sequence numbers of the messages. 
In that case, each shard is appropriately replicated (refer to the Partition lesson for more details on this).
- We also assume that our system can auto-expand and auto-shrink the resources as per the need to optimally utilize resources.

Load balancer
- The load balancer layer receives requests from producers and consumers, which are forwarded to one of the front-end servers. 
This layer consists of numerous load balancers. Therefore, requests are accepted with minimal latency and offer high availability.

Front-end service - (it is better to call it Application Service)
The front-end service comprises stateless machines distributed across data centers. The frontend provides the following services:
- Request validation: This ensures the validity of a request and checks if it contains all the necessary information.
- Authentication and authorization: This service checks if the requester is a valid user and if these services are authorized for use to a requester.
- Caching: In the front-end cache, metadata information is stored related to the frequently used queues. 
Along with this, user-related data is also cached here to reduce request time to authentication and authorization services.
- Request dispatching: The frondend is also responsible for calling two other services, the backend and the metadata store. 
Differentiating calls to both of these services is one of the responsibilities of the frontend.
- Request deduplication: The frontend also tracks information related to all the requests, therefore, it also prevents identical requests from being put in a queue. 
Deciding what to do about duplicates might be as easy as searching a hash key in a store. If something is found in the store, this implies a duplicate and the message can be rejected.
- Usage data collection: This refers to the collection of real-time data that can be used for audit purposes.

Metadata service
- This component is responsible for storing, retrieving, and updating the metadata of queues in the metadata store and cache.
Whenever a queue is created or deleted, the metadata store and cache are updated accordingly. 
The metadata service acts as a middleware between the front-end servers and the data layer. 
Since the metadata of the queues is kept in the cache, the cache is checked first by the front-end servers for any relevant information related to the receipt of the request. 
If a cache miss occurs, the information is retrieved from the metadata store and the cache is updated accordingly.

If the metadata that needs to be stored is small and can reside on a single machine, then it’s replicated on each cluster server. 
Subsequently, the request can be served from any random server. 
In this approach, a load balancer can also be introduced between the front-end servers and metadata services

If the metadata that needs to be stored is too large, then one of the following modes can be followed:
- The first strategy is to use the sharding approach to divide data into different shards. 
Sharding can be performed based on some partition key or hashing techniques, as was discussed in the lesson on database partitioning. 
Each shard is stored on a different host in the cluster. 
Moreover, each shard is also replicated on different hosts to enhance availability. 
In this cluster-organization approach, the front-end server has a mapping table between shards and the hosts. 
Therefore, the front-end server is responsible for redirecting requests to the host where the data is stored

- The second approach is similar to the first one. However, the mapping table in this approach is stored on each host instead of just on the front-end servers. 
Because of this, any random host can receive a request and forward it to the host where the data resides. This technique is suitable for read-intensive applications.

---------------------------------------------------------
---------------------------------------------------------
## 28 feb 23
**System Design - Distributed queue**
A messaging queue is used to receive messages from producers. 

These messages are consumed by the consumers at their own pace. Some operations are critical in that they require strict ordering of the execution of the tasks, driven by the messages in the queue. 
For example, while chatting over a messenger application with a friend, the messages should be delivered in order; otherwise, such communication can be confusing, to say the least. 
Similarly, emails received by a user from different users may not require strict ordering. 

Therefore, in some cases, the strict order of incoming messages in the queue is essential, while many use cases can tolerate some reordering.

Let’s discuss the following two categories of messages ordering in a queue:
- Best-effort ordering
- Strict ordering

With the best-effort ordering approach, the system puts the messages in a specified queue in the same order that they’re received.
For example, the producer sends four messages, A, B, C, and D, in the same order as illustrated. 
Due to network congestion or some other issue, message B is received after message D. 
Hence, the order of messages is A, C, D, and B at the receiving end. 
Therefore, in this approach, the messages will be put in the queue in the same order they were received instead of the order in which they were produced on the client side.

The strict ordering technique preserves the ordering of messages more rigorously. 
Through this approach, messages are placed in a queue in the order that they’re produced.
Before putting messages in a queue in the correct sequence, it’s crucial to have a mechanism to identify the order in which the messages were produced on the client side. 
Often, a unique identifier or time-stamp is used to mark a message when it’s produced.

One of the following three approaches can be used for ordering incoming messages:
- Monotonically increasing numbers: One way to order incoming messages is to assign monotonically increasing numbers to messages on the server side.
 When the first message arrives, the system assigns it a number, such as 1. It then assigns the number 2 to the second message, and so on.

- Causality-based sorting at the server side: Keeping in view the drawbacks of using monotonically increasing numbers, another approach that can be used for time-stamping and ordering of incoming messages 
is causality-based sorting. In this approach, messages are sorted based on the time stamp that was produced at the client side and are put in a queue accordingly.

- Using time stamps based on synchronized clocks: To tackle the potential issues that arise with both of the approaches described above, we can use another appropriate method to assign time stamps to messages 
that’s based on synchronized clocks. 
In this approach, the time stamp (ID) provided to each message through a synchronized clock is unique and in the correct sequence of production of messages. 
We can tag a unique process identifier with the time stamp to make the overall message identifier unique and tackle the situation when two concurrent sessions ask for a time stamp at the exact same time. 
Moreover, with this approach, the server can easily identify delayed messages based on the time stamp and wait for the delayed messages.

Once messages are received at the server side, we need to sort them based on their time stamps. 
Therefore, we use an appropriate online sorting algorithm for this purpose.

Primarily, a queue is designed for first-in, first-out (FIFO) operations; First-in, first-out operations suggest that the first message that enters a queue is always handed out first. 
However, it isn’t easy to maintain this strict order in distributed systems. 
Since message A was produced before message B, it’s still uncertain that message A will be consumed before message B. 

Using monotonically increasing message identifiers or causality-bearing identifiers provide high throughput while putting messages in a queue. 
Though the need for the online sorting to provide a strict order takes some time before messages are ready for extraction. 
To minimize latency caused by the online sorting, we use a time-window approach.

Similarly, for strict ordering at the receiving end, we need to serialize all the requests to give out messages one by one. 
If that’s not required, we have better throughput and lower latency at the receiving end.

Due to the reasons mentioned above, many distributed messaging queue solutions either don’t guarantee a strict order or have limitations around throughput. 

*concurrency*
Concurrent queue access needs proper management. Concurrency can take place at the following stages:
- When multiple messages arrive at the same time.
- When multiple consumers request concurrently for a message.

The first solution is to use the locking mechanism. When a process or thread requests a message, it should acquire a lock for placing or consuming messages from the queue. 
However, as was discussed earlier, this approach has several drawbacks. It’s neither scalable nor performant.

Another solution is to serialize the requests using the system’s buffer at both ends of the queue 
so that incoming messages are placed in an order and consumer processes also receive messages in their arrival sequence. 
This is a more viable solution because it can help us avoid the occurrence of race conditions.

Applications might use multiple queues with dedicated producers and consumers to keep the ordering cost per queue under check, although this comes at the cost of more complicated application logic.


---------------------------------------------------------
---------------------------------------------------------
## 27 feb 23
**System Design - Distributed queue**

A messaging queue is an intermediate component between the interacting entities known as producers and consumers. 

The producer produces messages and places them in the queue, while the consumer retrieves the messages from the queue and processes them. 
There might be multiple producers and consumers interacting with the queue at the same time.
A messaging queue has several advantages and use cases.

- Improved performance: A messaging queue enables asynchronous communication between the two interacting entities, producers and consumers, and eliminates their relative speed difference. 
A producer puts messages in the queue without waiting for the consumers. 
Similarly, a consumer processes the messages when they become available.
 Moreover, queues are often used to separate out slower operations from the critical path and, therefore, help reduce client-perceived latency. 
 For example, instead of waiting for a specific task that’s taking a long time to complete, the producer process sends a message, which is kept in a queue if there are multiple requests, 
 for the required task and continues its operations. The consumer can notify us about the fate of the processing, whether a success or failure, by using another queue.
 
- Better reliability: The separation of interacting entities via a messaging queue makes the system more fault tolerant. 
For example, a producer or consumer can fail independently without affecting the others and restart later. 
Moreover, replicating the messaging queue on multiple servers ensures the system’s availability if one or more servers are down.

- Granular scalability: Asynchronous communication makes the system more scalable. 
For example, many processes can communicate via a messaging queue. 
In addition, when the number of requests increases, we distribute the workload across several consumers. 
So, an application is in full control to tweak the number of producer or consumer processes according to its current need.

- Easy decoupling: A messaging queue decouples dependencies among different entities in a system. 
The interacting entities communicate via messages and are kept unaware of each other’s internal working mechanisms.

- Rate limiting: Messaging queues also help absorb any load spikes and prevent services from becoming overloaded, acting as a rudimentary form of rate limiting when there is a need to avoid dropping any incoming request.

- Priority queue: Multiple queues can be used to implement different priorities—for example, one queue for each priority—and give more service time to a higher priority queue.

Examples
- Sending many emails: Emails are used for numerous purposes, such as sharing information, account verification, resetting passwords, marketing campaigns, and more. 
All of these emails written for different purposes don’t need immediate processing and, therefore, they don’t disturb the system’s core functionality. 
A messaging queue can help coordinate a large number of emails between different senders and receivers in such cases.
- Data post-processing: Many multimedia applications need to process content for different viewer needs, such as for consumption on a mobile phone and a smart television. 
Oftentimes, applications upload the content into a store and use a messaging queue for post-processing of content offline. 
Doing this substantially reduces client-perceived latency and enables the service to schedule the offline work at some appropriate time—probably late at night when the compute capacity is less busy.
- Recommender systems: Some platforms use recommender systems to provide preferred content or information to a user. 
The recommender system takes the user’s historical data, processes it, and predicts relevant content or information. 
Since this is a time-consuming task, a messaging queue can be incorporated between the recommender system and requesting processes to increase and quicken performance.

Functional requirements
- Queue creation: The client should be able to create a queue and set some parameters—for example, queue name, queue size, and maximum message size.
- Send message: Producer entities should be able to send messages to a queue that’s intended for them.
- Receive message: Consumer entities should be able to receive messages from their respective queues.
- Delete message: The consumer processes should be able to delete a message from the queue after a successful processing of the message.
- Queue deletion: Clients should be able to delete a specific queue.

Non-functional requirements
- Durability: The data received by the system should be durable and shouldn’t be lost. 
Producers and consumers can fail independently, and a queue with data durability is critical to make the whole system work, because other entities are relying on the queue.
- Scalability: The system needs to be scalable and capable of handling the increased load, queues, producers, consumers, and the number of messages. 
Similarly, when the load reduces, the system should be able to shrink the resources accordingly.
- Availability: The system should be highly available for receiving and sending messages. 
It should continue operating uninterrupted, even after the failure of one or more of its components.
- Performance: The system should provide high throughput and low latency.

A single-server messaging queue has the following drawbacks:
- High latency: As in the case of a single-server messaging queue, a producer or consumer acquires a lock to access the queue. Therefore, this mechanism becomes a bottleneck when many processes try to access the queue. This increases the latency of the service.
- Low availability: Due to the lack of replication of the messaging queue, the producer and consumer process might be unable to access the queue in events of failure. This reduces the system’s availability and reliability.
- Lack of durability: Due to the absence of replication, the data in the queue might be lost in the event of a system failure.
- Scalability: A single-server messaging queue can handle a limited number of messages, producers, and consumers. Therefore, it is not scalable.

To extend the design of a single-server messaging queue to a distributed messaging queue, we need to make extensive efforts to eliminate the drawbacks outlined above.

---------------------------------------------------------
---------------------------------------------------------
## 23 feb 23
**DMN engine passing dynamic variales**

Tried to build dmn with dynamic variables, to select next step, still thinking about best solution.
Previous was:
- Stupid decision diagrams like bpmns;
- Decision which routes to concrete decision;


https://github.com/zolotarevandrew/camunda/tree/main/CamundaTests/CamundaDmnApiTests

---------------------------------------------------------
---------------------------------------------------------
## 22 feb 23
**DMN engine passing dynamic variales**

Tried to pass multiple dynamic variables, one worked normally, but with four it stopped working.

https://github.com/zolotarevandrew/camunda/tree/main/CamundaTests/CamundaDmnApiTests

---------------------------------------------------------
---------------------------------------------------------
## 21 feb 23
**DMN engine api test**

Tried to use dmn engine inside api in release mode, because in tests it was like 500 ms to execute.

So what i found was
- Load dmn context into static variable on singleton scope reduces about 300ms;
- Release mode reduces about 50-100ms;
- Engine has inside caching mechanism, so it is better to hot execute some base decisions to be cached;

https://github.com/zolotarevandrew/camunda/tree/main/CamundaTests/CamundaDmnApiTests

---------------------------------------------------------
## 21 feb 23
**System Design - Distributed cache**

functional requirements:
- Insert data: The user of a distributed cache system must be able to insert an entry to the cache.
- Retrieve data: The user should be able to retrieve data corresponding to a specific key

non-functional requirements:
- High performance: The primary reason for the cache is to enable fast retrieval of data. Therefore, both the insert and retrieve operations must be fast.
- Scalability: The cache system should scale horizontally with no bottlenecks on an increasing number of requests.
- High availability: The unavailability of the cache will put an extra burden on the database servers, which can also go down at peak load intervals. We also require our system to survive occasional failures of components and network, as well as power outages.-
- Consistency: Data stored on the cache servers should be consistent. For example, different cache clients retrieving the same data from different cache servers (primary or secondary) should be up to date.
- Affordability: Ideally, the caching system should be designed from commodity hardware instead of an expensive supporting component within the design of a system


Then our data is large, we may require sharding and therefore use shard servers for cache partitions. 
Should these shard servers be specialized or commodity hardware? 
Specialized hardware will have good performance and storage capacity, but it will cost more.
 We can build a large cache from commodity servers. 
 In general, the number of shard servers will depend on the cache’s size and access frequency.

We can consider storing our data on the secondary storage of these servers for persistence while we still serve data from RAM. 
Secondary storage may be used in cases where a reboot happens, and cache rebuilding takes a long time. 
Persistence, however, may not be a requirement in a cache system if there’s a dedicated persistence layer, such as a database

The writing strategy over the cache and database has consistency implications. 
In general, there’s no optimal choice, but depending on our application, the preference of writing policy is significantly important

By design, the cache provides low-latency reads and writes. 
To achieve this, data is often served from RAM memory. 
Usually, we can’t put all the data in the cache due to the limited size of the cache as compared to the full dataset. 
So, we need to carefully decide what stays in the cache and how to make room for new entries

Cache client: This library resides in the service application servers. 
It holds all the information regarding cache servers. The cache client will choose one of the cache servers using a hash and search algorithm for each incoming insert and retrieve request. 
All the cache clients should have a consistent view of all the cache servers. 
Also, the resolution technique to move data to and from the cache servers should be the same. 
Otherwise, different clients will request different servers for the same data.

Cache servers: These servers maintain the cache of the data. 
Each cache server is accessible by all the cache clients. 
Each server is connected to the database to store or retrieve data. C
ache clients use TCP or UDP protocol to perform data transfer to or from the cache servers. 
However, if any cache server is down, requests to those servers are resolved as a missed cache by the cache clients

- Solution 1: It’s possible to have a configuration file in each of the service hosts where the cache clients reside. 
The configuration file will contain the updated health and metadata required for the cache clients to utilize the cache servers efficiently. 
Each copy of the configuration service can be updated through a push service by any DevOps tool. 
The main problem with this strategy is that the configuration file will have to be manually updated and deployed through some DevOps tools.

- Solution 2: We can store the configuration file in a centralized location that the cache clients can use to get updated information about cache servers. 
This solves the deployment issue, but we still need to manually update the configuration file and monitor the health of each server.

- Solution 3: An automatic way of handling the issue is to use a configuration service that continuously monitors the health of the cache servers. 
In addition to that, the cache clients will get notified when a new cache server is added to the cluster. 
When we use this strategy, no human intervention or monitoring will be required in case of failures or the addition of new nodes. 
Finally, the cache clients obtain the list of cache servers from the configuration service.


The second problem relates to cache unavailability if the cache servers fail. 
A simple solution is the addition of replica nodes. 
We can start by adding one primary and two backup nodes in a cache shard. 
With replicas, there’s always a possibility of inconsistency. 
If our replicas are in close proximity, writing over replicas is performed synchronously to avoid inconsistencies between shard replicas. 
It’s crucial to divide cache data among shards so that neither the problem of unavailability arises nor any hardware is wasted


Each cache client should use three mechanisms to store and evict entries from the cache servers:
- Hash map: The cache server uses a hash map to store or locate different entries inside the RAM of cache servers. 
The illustration below shows that the map contains pointers to each cache value.
- Doubly linked list: If we have to evict data from the cache, we require a linked list so that we can order entries according to their frequency of access. 
The illustration below depicts how entries are connected using a doubly linked list.
- Eviction policy: The eviction policy depends on the application requirements. 
Here, we assume the least recently used (LRU) eviction policy.


Let’s summarize the proposed detailed design in a few points:
- The client’s requests reach the service hosts through the load balancers where the cache clients reside.
- Each cache client uses consistent hashing to identify the cache server. Next, the cache client forwards the request to the cache server maintaining a specific shard.
- Each cache server has primary and replica servers. Internally, every server uses the same mechanisms to store and evict cache entries.
- Configuration service ensures that all the clients see an updated and consistent view of the cache servers.
- Monitoring services can be additionally used to log and report different metrics of the caching service.

Design choices we made that will contribute to overall good performance:

We used consistent hashing. Finding a key under this algorithm requires a time complexity of O(log(N)), where N represents the number of cache shards.
Inside a cache server, keys are located using hash tables that require constant time on average.
The LRU eviction approach uses a constant time to access and update cache entries in a doubly linked list.
The communication between cache clients and servers is done through TCP and UDP protocols, which is also very fast.
Since we added more replicas, these can reduce the performance penalties that we have to face if there’s a high request load on a single machine.
An important feature of the design is adding, retrieving, and serving data from the RAM. Therefore, the latency to perform these operations is quite low.

We can create shards based on requirements and changing server loads. 
While we add new cache servers to the cluster, we also have to do a limited number of rehash computations, thanks to consistent hashing.

We have improved the availability through redundant cache servers. 
Redundancy adds a layer of reliability and fault tolerance to our design. 
We also used the leader-follower algorithm to conveniently manage a cluster shard. 
However, we haven’t achieved high availability because we have two shard replicas, and at the moment, we assume that the replicas are within a data center

It’s possible to write data to cache servers in a synchronous or asynchronous mode. 
In the case of caching, the asynchronous mode is favored for improved performance. 
Consequently, our caching system suffers from inconsistencies. Alternatively, strong consistency comes from synchronous writing, but this increases the overall latency, and the performance takes a hit

Generally, for concurrent access to shared data, some locking mechanisms like Semaphore, Monitors, Mutex locks, and others are good choices. 
But as the number of users (readers in case of cache hit or writers in case of a cache miss) grows, locking the entire data structure isn’t a suitable solution. 
In that case, we can consider the following options:
- Limited locking: In this strategy, only specific sections of the entire data structure will be locked. While some threads or processes can read from the data structure simultaneously, some threads may temporarily block access to specific sections of the data structure.
- Offline eviction: Offline eviction may be a possibility where instead of making actual changes to the data structure, only the required changes will be recorded while performing different operations 
until it’s necessary to commit the changes. 
This solution is desirable and easy if the cache hit rate is high because a change to the data structure will likely be required when a cache miss occurs.
- Lock-free implementation: Multiple solutions have been proposed which suggest that simultaneous reading and writing over doubly linked list is feasible to support large number of concurrent reading and writing.

---------------------------------------------------------
---------------------------------------------------------
## 20 feb 23
**System Design - Distributed cache**

A cache is a nonpersistent storage area used to keep repeatedly read and written data, which provides the end user with lower latency. 
Therefore, a cache must serve data from a storage component that is fast, has enough storage, and is affordable in terms of dollar cost as we scale the caching service. 


A distributed cache is a caching system where multiple cache servers coordinate to store frequently accessed data. 
Distributed caches are needed in environments where a single cache server isn’t enough to store all the data. 
At the same time, it’s scalable and guarantees a higher degree of availability.

Caches are generally small, frequently accessed, short-term storage with fast read time. Caches use the locality of reference principle

When the size of data required in the cache increases, storing the entire data in one system is impractical. This is because of the following three reasons:
- It can be a potential single point of failure (SPOF).
- A system is designed in layers, and each layer should have its caching mechanism to ensure the decoupling of sensitive data from different layers.
- Caching at different locations helps reduce the serving latency at that layer.


Often, cache stores a copy (or part) of data, which is persistently stored in a data store. When we store data to the data store, some important questions arise:
- Where do we store the data first? Database or cache?
- What will be the implication of each strategy for consistency models


*Write-through cache*: The write-through mechanism writes on the cache as well as on the database. 
Writing on both storages can happen concurrently or one after the other. 
This increases the write latency but ensures strong consistency between the database and the cache

*Write-back cache*: In the write-back cache mechanism, the data is first written to the cache and asynchronously written to the database. 
Although the cache has updated data, inconsistency is inevitable in scenarios where a client reads stale data from the database.
However, systems using this strategy will have small writing latency.


*Write-around cache*: This strategy involves writing data to the database only. 
Later, when a read is triggered for the data, it’s written to cache after a cache miss. 
The database will have updated data, but such a strategy isn’t favorable for reading recently updated data


One of the main reasons caches perform fast is that they’re small. Small caches mean limited storage capacity. 
Therefore, we need an eviction mechanism to remove less frequently accessed data from the cache

Several well-known strategies are used to evict data from the cache. The most well-known strategies include the following:
- Least recently used (LRU)
- Most recently used (MRU)
- Least frequently used (LFU)
- Most frequently used (MFU)
Other strategies like first in, first out (FIFO) also exist. The choice of each of these algorithms depends on the system the cache is being developed for


The situation demands a question: How do we identify stale entries?
Resolution of the problem requires storing metadata corresponding to each cache entry. 
Specifically, maintaining a time-to-live (TTL) value to deal with outdated cache items


We can use two different approaches to deal with outdated items using TTL:
- Active expiration: This method actively checks the TTL of cache entries through a daemon process or thread.
- Passive expiration: This method checks the TTL of a cache entry at the time of access.
Each expired item is removed from the cache upon discovery

- Which data should we store in which cache servers?
- What data structure should we use to store the data

It’s possible to use hashing in two different scenarios:
- Identify the cache server in a distributed cache to store and retrieve data.
- Locate cache entries inside each cache server.

For the first scenario, we can use different hashing algorithms. 
However, consistent hashing or its flavors usually perform well in distributed systems because simple hashing won’t be ideal in case of crashes or scaling


We’ll use a doubly linked list. The main reason is its widespread usage and simplicity. 
Furthermore, adding and removing data from the doubly linked list in our case will be a constant time operation. 
This is because we either evict a specific entry from the tail of the linked list or relocate an entry to the head of the doubly linked list. 
Therefore, no iterations are required.


Note: Bloom filters are an interesting choice for quickly finding if a cache entry doesn’t exist in the cache servers. 
We can use bloom filters to determine that a cache entry is definitely not present in the cache server, but the possibility of its presence is probabilistic. 
Bloom filters are quite useful in large caching or database systems.


To avoid SPOF and high load on a single cache instance, we introduce sharding. 
Sharding involves splitting up cache data among multiple cache servers. It can be performed in the following two ways

Dedicated cache servers
In the dedicated cache servers method, we separate the application and web servers from the cache servers.
The advantages of using dedicated cache servers are the following:
- There’s flexibility in terms of hardware choices for each functionality.
- It’s possible to scale web/application servers and cache servers separately.

The co-located cache embeds cache and service functionality within the same host.
The main advantage of this strategy is the reduction in CAPEX and OPEX of extra hardware. 
Furthermore, with the scaling of one service, automatic scaling of the other service is obtained. 
However, the failure of one machine will result in the loss of both services simultaneously

---------------------------------------------------------
---------------------------------------------------------
## 19 feb 23
**Управление приоритетами**

Управление приоритетами – это навык, который помогает тимлиду выжить в условиях полной загруженности задачами. Он помогает:

В первую очередь делать то, что принесёт больше пользы и ценности.
Удерживать work/life balance.
Трезво оценивать свои личные ресурсы.

Человеческий мозг очень легко путает понятия срочности и важности. Система управления приоритетами позволяет их оценивать с большей степенью объективности.
Очень часто начинающие тимлиды сталкиваются с тем, что на них в один момент сваливается огромное количество новых незнакомых задач и проектов. 
Делать всё сразу – зачастую невозможно, а поступиться чем-то из списка страшно. 
Без системы управления приоритетами тимлид не сможет быть уверенным в том, что делает именно то, что нужно.

Опытные тимлиды сталкиваются с другой стороной проблемы. Им периодически нужно решать, готовы ли они вписаться в какую-то новую активность. 
Навык управления приоритетами позволит им понять место нового проекта среди остальных и количество энергии, которое они готовы будут на него выделить, и в случае его недостатка говорить "нет".
Для команды и руководителя наличие этого навыка у тимлида важно, так как оно делает его действия и результаты более предсказуемыми.

Примеры хорошего поведения
- Тимлид умеет говорить "нет" новым проектам.
- Тимлид балансирует между решением задач, которые горят, и задач, которые улучшат жизнь ему и команде когда-то в будущем.
- Тимлид умеет отказываться от срочных, но не важных задач, в пользу важныхю

Для начала возьмите любую возможную систему сравнения проектов и задач между собой и начните её использовать. 
Через какое-то время вы скорее всего столкнётесь с её ограничениями или недостатками. 
Тогда переходите к её постепенному изменению под вас – удаляйте, изменяйте или добавляйте новые компоненты и отслеживайте их влияние на ваши процессы и результаты.

Матрица Эйзенхауэра.
Анализ Кано.

Взвешенная оценка
Самый интуитивный вариант приоритизации. 
Вы выбираете систему критериев, по которым оцениваете все задачи, и выводите формулу, по которой сводите их в единый показатель. 
Например, можно воспользоваться RICE подходом:
- Reach. Какое количество людей или других компонентов системы затрагивает решение этой задачи.
- Impact. Насколько значимым будет влияние этой задачи.
- Confidence. Уровень вашей уверенности в утверждениях выше.
- Effort. Количество усилий на выполнение задачи.
Итоговый RICE score подсчитывается по формуле Reach * Impact * Confidence / Effort. 
В работу берутся первыми те задачи, которые оказались на верхушке получившегося бэклога.

**Плюсы**
- Попробую использовать RICE подход.

---------------------------------------------------------
---------------------------------------------------------
## 17 feb 23
**Camunda DMN**

After talking to chat gpt, i realized that DMN can be executed in code. What's awesome.
But i found only one c# library, which can execute DMN (built insise camunda modeler).
I will try to use Java camunda DMN engine, to compare speed (maybe i have to port it later..)

**Pluses**
- Decision can be visualized in a better way and language, instead of using a lot conditions in code.
**Minuses**
- Execution of one simple decision can took over 200ms, that's too much.

https://github.com/zolotarevandrew/camunda/blob/main/CamundaTests/CamundaDmnTests/SimpleDecisionTest.cs

---------------------------------------------------------
---------------------------------------------------------
## 15 feb 23
**System Design - Monitoring**

In a distributed system, clients often access the service via an HTTP request. 
We can monitor our web and application servers’ logs if a request fails to process. 
If multiple requests fail, we can observe a spike in internal errors (error 500).

There are many factors that can cause failures that can result in clients being unable to reach the server. These include the following:
- Failure in DNS name resolution.
- Any failure in routing along the path from the client to the service provider.
- Any failures with third-party infrastructure, such as middleboxes and content delivery networks (CDNs)

To ensure that the client’s requests reach the server, we’ll act as clients and perform reachability and health checks. 
We’ll need various vantage points across the globe. W
e can run a service, let’s call it prober, that periodically sends requests to the service to check availability.
This way, we can monitor reachability to our service from many different places
Problems
- Incomplete coverage: We might not have good coverage across all autonomous systems. 
There are 100,000 unique autonomous systems on the Internet as of March 2021. 
It’s not cost-effective or even possible to put those many probes across the globe. C
ountry or ISP-specific regulations and the need for periodic maintenance are additional hurdles to implementing such a scheme.
- Lack of user imitation: Such probes might not represent a typical user behavior to explain how a typical user will use the service.

Instead of using a prober on vantage points, we can embed the probers into the actual application instead. 
We’ll have the following two components
- Agent: This is a prober embedded in the client application that sends the appropriate service reports about any failures.
- Collector: This is a report collector independent of the primary service. 
It’s made independent to avoid the situations where client agents want to report an error to the failed service. 
We summarize errors reports from collectors and look for spikes in the errors graph to see client-side issues
- Data processing systems. We can place them near the client network, and over time, we can accumulate these statistics from all such localized sites. W
e’ll use online stream processing systems to make such a system near real-time.
 If we’re mainly looking for summary statistics, our system can tolerate the loss of some error reports. 
 Some reports will be relative to the overall user population. 
 We might say 1% of service users are “some.” 
 If we don’t want to lose any reports, we’ll need to design a system with more care, which will be more expensive

Solution can be to use a client-side application that the service controls, and then we can easily include such headers over HTTP.
The client can fill in the request header if the client has already consented to that. The service can then reply with appropriate values for the policy and collection endpoints

The collectors need to be in a different failure domain from the web service endpoint that we’re trying to monitor. 
The client side can try various collectors in different failure domains until one works. 
We can see a similar pattern in the following examples. 
At times, we refer to such a phenomenon as being outside the blast radius of a fault.

The human user who uses the client-side software should be in full control to precisely know what data is collected and sent with each request. The user should also be able to reactivate the feature any time they wish. If we use our client-side application (and not a browser application), we have a lot of flexibility in what diagnostic could be included in the report. For a browser-based client, we can avoid the following information:

- In a distributed system, it’s difficult to detect and respond to errors on the client side. 
So, it’s necessary to monitor such events to provide a good user experience.
- We can handle errors using an independent agent that sends service reports about any failures to a collector. 
Such collectors should be independent of the primary service in terms of infrastructure and deploy


---------------------------------------------------------
---------------------------------------------------------
## 14 feb 23
**System Design - Monitoring**

Monitoring system should do for us:
- Monitor critical local processes on a server for crashes.
- Monitor any anomalies in the use of CPU/memory/disk/network bandwidth by a process on a server.
- Monitor overall server health, such as CPU, memory, disk, network bandwidth, average load, and so on.
- Monitor hardware component faults on a server, such as memory failures, failing or slowing disk, and so on.
- Monitor the server’s ability to reach out-of-server critical services, such as network file systems and so on.
- Monitor all network switches, load balancers, and any other specialized hardware inside a data center.
- Monitor power consumption at the server, rack, and data center levels.
- Monitor any power events on the servers, racks, and data center.
- Monitor routing information and DNS for external clients.
- Monitor network links and paths’ latency inside and across the data centers.
- Monitor network status at the peering points.
- Monitor overall service health that might span multiple data centers—for example, a CDN and its performance

The high-level components of our monitoring service are the following:
- Storage: A time-series database stores metrics data, such as the current CPU use or the number of exceptions in an application.
- Data collector service: This fetches the relevant data from each service and saves it in the storage.
- Querying service: This is an API that can query on the time-series database and return the relevant information

Monitoring systems are critical in distributed systems because they help in analyzing the system and alerting the stakeholders if a problem occurs.
We can make a monitoring system scalable using a hybrid of the push and pull methods.
Heat maps are a powerful tool for visualization and help us learn about the health of thousands of servers in a compact space


---------------------------------------------------------
---------------------------------------------------------
## 13 feb 23
**System Design - Monitoring**

To avoid cascading failures, monitoring can play a vital role with early warnings or steering us to the root cause of faults.

Let’s consider a scenario where a user uploads a video, intro-to-system-design, to YouTube. 
The UI service in server A takes the video information and gives the data to service 2 in server B. Service 2 makes an entry in the database and stores the video in blob storage. 
Another service, 3, in server C manages the replication and synchronization of databases X and Y.
In this scenario, service 3 fails due to some error, and service 2 makes an entry in the database X. 
The database X crashes, and the request to fetch a video is routed to database Y. 
The user wants to play the video intro-to-system-design, but it will give an error of “Video not found

Having a monitoring system reduces operational costs and encourages an automated way to detect failures.
Let’s consider an example to understand the types of errors we want to monitor. 
At Educative, whenever a learner connects to an executable environment, a container is assigned. 
Consider service 1 in server A, which is responsible for allocating a container whenever a learner connects. 
Another service, 2 on server B takes this information and informs the service responsible for UI. The UI service running in server C updates the UI for the learner. 
Let’s assume that service 2 fails because of some error, and the learner sees the error of “Cannot connect

Type of errors:
- Service-side errors: These are errors that are usually visible to monitoring services as they occur on servers. Such errors are reported as error 5xx in HTTP response codes.
- Client-side errors: These are errors whose root cause is on the client-side. Such errors are reported as error 4xx in HTTP response codes. 
Some client-side errors are invisible to the service when client requests fail to reach the service.

A good monitoring system needs to clearly define what to measure and in what units (metrics). 
The monitoring system also needs to define threshold values of all metrics and the ability to inform appropriate stakeholders (alerts) when values are out of acceptable ranges. 
Knowing the state of our infrastructure and systems ensures service stability. 
The support team can respond to issues more quickly and confidently if they have access to information on the health and performance of the deployments. 
Monitoring systems that collect measurements, show data, and send warnings when something appears wrong are helpful for the support

- In the reactive approach, corrective action is taken after the failure occurs. In this approach, even if DevOps takes quick action to find the cause of the error and promptly handle the failures, it causes downtime. As a result, there will be system downtime in the reactive approach, which is generally undesirable for continuously running applications.
- In a proactive approach, proactive actions are taken before failure occurs. Therefore, it prevents downtimes and associated losses. 
The proactive approach works on predicting system failures to take corrective action to avoid the failure. This approach offers better reliability by preventing downtime.

In modern services, completely avoiding problems is not possible. 
Something is always failing inside huge data centers and network deployments. 
he goal is to find the impending problems early on and design systems in such a way that service faults are invisible to the end users

Metrics objectively define what we should measure and what units will be appropriate. 
Metric values provide an insight into the system at any point in time For example, 
a web server’s ability to handle a certain amount of traffic per second or its ability to join a pool of web servers are examples of high-level data correlated with a component’s specific purpose or activity. 
Another example can be measuring network performance in terms of throughput (megabits per second) and latency (round-trip time). 
We need to collect values of metrics with minimal performance penalty. We may use user-perceived latency or the amount of computational resources to measure this penalty.

The metrics should be logically centralized for global monitoring and alerting purposes. Fetching metrics is crucial to the monitoring system. Metrics can either be pushed or pulled into a monitoring system, depending on the preference of the user
Time-series databases help maintain durability, which is an important factor. 
Without a historical view of events in a monitoring system, it isn’t very useful. 
Samples having a value of time stamp are stored in chronological sequence. 
So, a whole metric’s timeline can be shown in the form of a time series.

Alerting is the part of a monitoring system that responds to changes in metric values and takes action. 
There are two components to an alert definition: a metrics-based condition or threshold, and an action to take when the values fall outside of the permitted range

**Pluses**
- Monitoring should enabled for Server and client sides to improve system observability.
- Metrics should identified for each unique system, including standard metrics.
- Alerting should be the one of main parts of system monitoring.
- Metrics should be persisted in time series databases.


---------------------------------------------------------
---------------------------------------------------------
## 12 feb 23
**Распространение знаний**

Определить текущие проблемы и боли, связанные с распространением знаний. В этом могут помочь такие инструменты:
- Интервью с членами вашей и других команд с вопросами: "Когда вы последний раз что-то пытались выяснить о нашем проекте и не могли этого найти?", "В каких областях нашего проекта разбирается слишком мало людей?", "Чему вы хотели бы научиться у кого-то из команды?", "Расскажите, как устроена архитектура нашего продукта".
- Изучение вопросов, которые задают в канале вашей команды или продукта в мессенджере. Если какие-то из них повторяются, это сигнал о том, что соответствующей информации нет, либо её тяжело найти.- 
- Пообщайтесь с новичками и узнайте, с какими сложностями они столкнулись в период онбординга.

Постепенно пробовать разные способы распространения знаний, которые могут помочь вам в решении ваших проблем.
- Ведение базы знаний в Confluence, Quip, Notion или другой системе;
- Организация командного Stack Overflow;
- Отдельный чат в мессенджере для Q&A;
- Регулярные внутренние технические митапы;
- Архитектурные и code ревью;
- Технический радар;
- Рассылки про изменения в технологиях и продукте;
- Краткие дайджесты в командном чате;
- Общие демо или стендапы;

**Плюсы**
- Провести интервью, получить ответы на вопросы, составить более качественную документацию.
- Формализовать видео из митапов, в документацию.
- Пообщаться с новичками, пересмотреть подход к составлению документации.


---------------------------------------------------------
---------------------------------------------------------
## 11 feb 23
**Оптимизация тестирования**

Оптимизация тестирования — это выработка такой стратегии тестирования, которая позволит минимальными средствами снижать критичные риски по продукту.

Менеджер:
- Влияет на скорость доставки фичи до боя;
- Понимает, почему тестирование проводится так и занимает столько времени;

Разработчик:
- Понимает, что делают тестировщики и что от них ждать;
- Влияет на тестирование — от unit-тестов до testability;

Тестировщик:
- Понимает, что тестировать и насколько подробно;
- Чувствует меньше давления, что тестирование замедляет процесс работы команды;
- Участвует в тестировании вместе с командой;

Прежде чем оптимизировать то тестирование, которое у вас уже есть, или строить новый процесс, поймите зачем вам нужно тестирование и нужно ли на самом деле?
Начните с того, что подумайте о рисках для вашего продукта. Опирайтесь на функциональные и нефункциональные требования. Например:

- Что будет, если вы выпустите продукт с багом?
- Насколько много ваших пользователей это затронет?
- Насколько сложно будет пользоваться продуктом с этим багом? Будет ли это вообще возможно?
- Что будет, если ваш продукт будет сложно поддерживать?
- Что будет, если ваш продукт будет уязвим для взлома?
- Что будет, если в вашем продукте будет сложно разобраться пользователям?
- Как быстро вы поймёте, что у вас баг на бою и оцените, насколько он критичный?
- Как быстро вы исправите баг, который нашли на бою?

Чтобы снизить эти риски, используется три процесса — тестирование, проверку и мониторинг.
- Тестирование — это процесс исследования и изучения продукта. Когда вы тестируете, вы узнаете как программа работает на самом деле, ну или не работает. 
Тестировщики не просто проверяют ТЗ, а используют всю свою креативность, опыт и знание продукта. 
Тестирование нельзя автоматизировать, как нельзя автоматизировать человеческое мышление.

- Проверка — подтверждение существующих ожиданий. Когда вы что-то проверяете — точно знаете, что должны получить, и убеждаетесь, что 2+2 все так же равно 4. 
Многие проверки можно эффективно автоматизировать.

- Мониторинг — процесс выявления проблем, которые возникают на бою у пользователей. Включите в него как технические показатели, так и показатели работы бизнеса. 
Важно, чтобы вы могли быстро среагировать на проблему — выкатить фикс или откатить проблемный код.

Если вы знаете, что определённые проблемы будут критичны для вашего бизнеса, то нужно тщательно тестировать и проверять места, в которых они могут быть. 
Например, важно, чтобы пользователи могли оплатить ваш продукт. Или баги могут подорвать доверие к вашему продукту, например если вы разрабатываете платёжную систему. 
Важно помнить, что нельзя протестировать абсолютно все и на 100% знать, что все работает. Но можно снизить риски.
Когда вы разрабатываете MVP — вам надо убедиться, что ваш продукт жизнеспособен. А значит можно ограничиться минимальными проверками.
На этой базе составить стратегию тестирования. *Каждый человек в команде должен понимать, что мы тестируем, где и зачем.*

**Плюсы**
- Надо подумать как сделать процесс тестирования прозрачным и понятным со всех сторон (менеджеры, разработчики, тестировщики).
- Надо ответить на вопросы про функциональные/не функциональные требования и выявить основные точки оптимизации;
- Надо подумать как улучшить мониторинг выпущенных фич на продакшене.


---------------------------------------------------------
---------------------------------------------------------
## 10 feb 23
**Обратная связь**

Нужна для корректировки поведения и результатов работы сотрудника. 
Всю работу с обратной связью можно разделить на два процесса: сбор и дача сотруднику. 
Обратная связь характеризуется двумя главными свойствами – своевременностью и полнотой. 
Нахождение правильного баланса - главная задача.

Тимлид должен обеспечить поступление обратной связи к сотруднику. 
В самом базовом варианте всю обратную связь он пропускает через себя, в более продвинутом – обучает команду ценности обратной связи, способам её дачи и способствует развитию соответствующей культуры.

Типы:
- Периодическая. Привязана к какому-то циклу – регулярных встреч, целеполагания, календарному.
- Постоянная. Даётся сотруднику по ходу работы, без проведения дополнительных ритуалов по её сбору и обработке. 
- Ситуативная. Даётся в какой-то определённый момент времени, чаще с привязкой к какому-то событию.

Примеры плохого поведения
- Дача обратной связи без запроса;
- Даётся только корректирующая обратная связь;
- Даётся только позитивная обратная связь;
- Отсылается сотруднику в виде отчёта письмом через несколько недель;

Примеры хорошего поведения
- Обратная связь даётся только после разрешения сотрудника, которому она предназначена;
- Обратная связь сбалансирована, т.е. содержит и зоны роста и позитивные моменты;
- Обратная связь адресуется сотруднику в личной беседе;
- Обратная связь своевременна, т.е. даётся сразу после наблюдаемого поведения;
- Обратная связь конструктивна, даны объяснения, какое поведение правильное/не правильное;
- Запрашивать обратную связь у окружающих по своему поведению;

**Плюсы**
- Сотрудник понимает, над чем ему стоит поработать, прорабатывается на тет-а-тетах.
- Повторяющееся поведение, нужно уметь обрабатывать не сильно давя на сотруднику, людей бывает трудно поменять.

**Минусы**
- Можно переборщить с описанием минусов, нужно быть аккуратнее и начинать с позитивных моментов.

---------------------------------------------------------

---------------------------------------------------------
## 9 feb 23
**System design - CDN**

Periodic polling#
Using the pull model, proxy servers request the origin server periodically for updated data and change the content in the cache accordingly. 
When content changes infrequently, the polling approach consumes unnecessary bandwidth. 
Periodic polling uses time-to-refresh (TTR) to adjust the time period for requesting updated data from the origin servers

Because of the TTR, the proxy servers may uselessly request the origin servers for updated data. 
A better approach that could be employed to reduce the frequency of refresh messages is the time-to-live (TTL) approach. 
In this approach, each object has a TTL attribute assigned to it by the origin server. 
The TTL defines the expiration time of the content. The proxy servers serve the same data version to the users until that content expires.
 Upon expiration, the proxy server checks for an update with the origin server. 
If the data is changed, it gets the updated data from the origin server and then responds to the user’s requests with the updated data. 
Otherwise, it keeps the same data with an updated expiration time from the origin servers.

The CDN proxy servers must be placed at network locations with good connectivity
- On-premises represents a smaller data center that could be placed near major IXPs.
- Off-premises represents placing CDN proxy servers in ISP’s networks.

Minimizing latency. Some of the key design decisions that minimize latency are as follows:
- Proxy servers usually serve content from the RAM.
- CDN proxy servers are placed near the users to provide faster access to content.
- A CDN can also be the provider of proxy servers located in the ISP or Internet exchange points (IXPs) to handle high traffic.
- The request routing system ensures that users are directed to the nearest proxy servers.
- The proxy servers have long-tail content stored in nonvolatile storage systems like SSD or HDD.

Long-tail content
= proxy servers can be implemented in layers where if one layer doesn’t have the content, the request can be entertained by the next layer of proxy servers. 
For example, the edge proxy servers can request the parent proxy servers. 
Placing proxy servers at specific ISPs could be the best option when most traffic comes from those ISP regions.

CDN ensures availability through its cached content that serves as a backup whenever the origin servers fail. 
 Moreover, if one or more proxy servers in the CDN stop working, other operational proxy servers step in and continue to drive the web traffic. 
 In addition, edge proxy servers can be made available through redundancy by replicating data to as many proxy servers as needed to avoid a single point of failure and to meet the request load. 
 Finally, we can use a load balancer to distribute the users’ requests to nearby active proxy servers.

CDN ensures no single failure point by carefully implementing maintenance cycles and integrating additional hardware and software when required. 
Apart from failures, the CDN handles massive traffic loads by equally distributing the load to the edge proxy servers. 
We can use scrubber servers to prevent DDoS attacks and securely host content.
Moreover, we can use the heartbeat protocol to monitor the health of servers and omit faulty servers. 
Real-time applications also build their own specified CDNs to prevent content leakage problems and securely serve content to their end users.

**Pluses**
- Minimizing latency for dymanic/static content delivering;
- Leveraging single point of failure, content is distributed in CDN.
- Faster content delivering because of caching;

**Minuses**
- Sometimes it is better to use own infrastructure for CDN to ensure availability, security and total control, it also can be combined with cloud CDN.

---------------------------------------------------------
---------------------------------------------------------
## 8 feb 23
**System design - CDN**

Since dynamic content often changes, it’s a good idea to cache it optimally.
It’s optimal to run the scripts at proxy servers instead of the origin servers.
To reduce the communication between the origin server and proxy servers and storage requirements at proxy servers, it’s useful to employ compression techniques as well. 
For example, Cloudflare uses Railgun to compress dynamic content.

The content provider sends the content to a large number of clients through a CDN. 
The task of distributing data to all the CDN proxy servers simultaneously is challenging and burdens the origin server significantly.

The tree structure for data distribution allows us to scale our system for increasing users by adding more server nodes to the tree. 
It also reduces the burden on the origin server for data distribution. 
A CDN typically has one or two tiers of proxy servers (caches).

There are two important factors that are relevant to finding the nearest proxy server to the user:
Network distance between the user and the proxy server is crucial. This is a function of the following two things:
- The first is the length of the network path.
- The second is the capacity (bandwidth) limits along the network path.

The shortest network path with the highest capacity (bandwidth) is the nearest proxy server to the user in question. 
This path helps the user download content more quickly.Requests load refers to the load a proxy server handles at any point in time. 
If a set of proxy servers are overloaded, the request routing system should forward the request to a location with a lesser load. 
This action balances out the proxy server load and, consequently, reduces the response latency.
Let’s look at the techniques that can be used to route users to the nearest proxy server.

In a typical DNS resolution, we use a DNS system to get an IP against a human-readable name. 
However, the DNS can also return another URI (instead of an IP) to the client. Such a mechanism is called DNS redirect

There are two steps in the DNS redirection approach:
- In the first step, it maps the clients to the appropriate network location.
- In the second step, it distributes the load over the proxy servers in that location to balance the load among the proxy servers

---------------------------------------------------------
---------------------------------------------------------
## 7 feb 23
**System design - CDN**

Designing Api methods
- Retrieve (proxy server to origin server). If the proxy servers request content, the GET method retrieves the content through
- Deliver (origin server to proxy servers). The origin servers use this API to deliver the specified content, theupdated version, to the proxy servers through the distribution system.
- Request (clients to proxy servers). The users use this API to request the content from the proxy servers.
- Search (proxy server to peer proxy servers). Although the content is first searched locally at the proxy server, the proxy servers can also probe requested content in the peer proxy servers in the same PoP through the /searchContent API. This could flood the query to all proxy servers in a PoP.
- Update (proxy server to peer proxy servers). The proxy servers use the /updateContent API to update the specified content in the peer proxy servers in the PoP.

Push/Pull models.
Push CDN - Content gets sent automatically to the CDN proxy servers from the origin server in the push CDN model. 
The content delivery to the CDN proxy servers is the content provider’s responsibility. 
Push CDN is appropriate for static content delivery, where the origin server decides which content to deliver to users using the CDN. 
The content is pushed to proxy servers in various locations according to the content’s popularity. 
If the content is rapidly changing, the push model might struggle to keep up and will do redundant content pushes

Pull CDN - A CDN pulls the unavailable data from origin servers when requested by a user. 
The proxy servers keep the files for a specified amount of time and then remove them from the cache if they’re no longer requested to balance capacity and cost.
When users request web content in the pull CDN model, the CDN itself is responsible for pulling the requested content from the origin server and serving it to the users. 
Therefore, this type of CDN is more suited for serving dynamic content.

The push CDN is mostly used for serving static content. 
Since static content is served to a wide range of users for longer than dynamic content, the push CDN scheme maintains more replicas than the pull CDN, thus improving availability. O
n the other hand, the pull CDN is favored for frequently changing content and a high traffic load.
Low storage consumption is one of the main benefits of the pull CDN.

---------------------------------------------------------
---------------------------------------------------------
## 6 feb 23
**System design - CDN**

A CDN is a group of geographically distributed proxy servers. 
A proxy server is an intermediate server between a client and the origin server. 
The proxy servers are placed on the network edge. As the network edge is close to the end users, the placement of proxy servers helps quickly deliver the content to the end users by reducing latency and saving bandwidth. 
A CDN has added intelligence on top of being a simple proxy server.

Functional requirements#
- Retrieve: CDN should be able to retrieve content from the origin servers.
- Request: Content delivery from the proxy server is made upon the user’s request.
- Deliver: In the case of the push model, the origin servers should be able to send the content to the CDN proxy servers.
- Search: The CDN should be able to execute a search against a user query for cached or otherwise stored content within the CDN infrastructure.
- Update: In most cases, content comes from the origin server, but if we run script in CDN, the CDN should be able to update the content within peer CDN proxy servers in a PoP.
- Delete: Depending upon the type of content (static or dynamic), it should be possible to delete cached entries from the CDN servers after a certain period.

Non-functional requirements#
- Performance: Minimizing latency.
- Availability: CDNs are expected to be available at all times because of their effectiveness. Availability includes protection against attacks like DDoS.
- Scalability: An increasing number of users will request content from CDNs.
- Reliability and security: Our CDN design should ensure no single point of failure. Apart from failures, the designed CDN must reliably handle massive traffic loads. 

Components:
- Clients: End users use various clients, like browsers, smartphones, and other devices.
- Routing system: The routing system directs clients to the nearest CDN facility.
- Scrubber servers: Scrubber servers are used to separate the good traffic from malicious traffic and protect against well-known attacks, like DDoS.
- Proxy servers: The proxy or edge proxy servers serve the content from RAM to the users. Proxy servers store hot data in RAM, though they can store cold data in SSD or hard drive as well.
- Distribution system: The distribution system is responsible for distributing content to all the edge proxy servers to different CDN facilities.
- Origin servers: The CDN infrastructure facilitates users with data received from the origin servers.
- Management system: The management systems are important in CDNs from a business and managerial aspect where resource usage and statistics are constantly observed.

---------------------------------------------------------

---------------------------------------------------------
## 3 feb 23
**System design - Databases partiotining**

At some point, a single node-based database isn’t enough to tackle the load. 
We might need to distribute the data over many nodes but still export all the nice properties of relational databases.

Data partitioning (or sharding) enables us to use multiple nodes where each node manages some part of the whole data. 
To handle increasing query rates and data amounts, we strive for balanced partitions and balanced read/write load

We can put different tables in various database instances, which might be running on a different physical server. 
We might break a table into multiple tables so that some columns are in one table while the rest are in the other.
We should be careful if there are joins between multiple tables. 
We may like to keep such tables together on one shard.

Often, vertical sharding is used to increase the speed of data retrieval from a table consisting of columns with very wide text or a binary large object (blob). 
In this case, the column with large text or a blob is split into a different table.

At times, some tables in the databases become too big and affect read/write latency. 
Horizontal sharding or partitioning is used to divide a table into multiple tables by splitting data row-wise. 
Each partition of the original table distributed over database servers is called a shard. 


In the key-range based sharding, each partition is assigned a continuous range of keys.

Advantages
- Using this method, the range-query-based scheme is easy to implement.
- Range queries can be performed using the partitioning keys, and those can be kept in partitions in sorted order.
Disadvantages
- Range queries can’t be performed using keys other than the partitioning key.
- If keys aren’t selected properly, some nodes may have to store more data due to an uneven distribution of the traffic.


Hash-based sharding uses a hash-like function on an attribute, and it produces different values based on which attribute the partitioning is performed. 
The main concept is to use a hash function on the key to get a hash value and then mod by the number of partitions. 
Once we’ve found an appropriate hash function for keys, we may give each partition a range of hashes (rather than a range of keys). 
Any key whose hash occurs inside that range will be kept in that partition

Advantages
- Keys are uniformly distributed across the nodes.
Disadvantages#
- We can’t perform range queries with this technique. Keys will be spread over all partition

Consistent hashing assigns each server or item in a distributed hash table a place on an abstract circle, called a ring, irrespective of the number of servers in the table. 
This permits servers and objects to scale without compromising the system’s overall performance.
Advantages of consistent hashing
- It’s easy to scale horizontally.
- It increases the throughput and improves the latency of the application.
Disadvantages of consistent hashing#
- Randomly assigning nodes in the ring may cause non-uniform distribution.

A fixed number of partitions is used in Elasticsearch, Riak, and many more.

Dynamic partitioning
In this approach, when the size of a partition reaches the threshold, it’s split equally into two partitions. O
ne of the two split partitions is assigned to one node and the other one to another node. 
In this way, the load is divided equally. 
The number of partitions adapts to the overall data amount, which is an advantage of dynamic partitioning.
However, there’s a downside to this approach. It’s difficult to apply dynamic rebalancing while serving the reads and writes. This approach is used in HBase and MongoDB.

Following are a few approaches to work with partitioned db:
- Allow the clients to request any node in the network. If that node doesn’t contain the requested data, it forwards that request to the node that does contain the related data.
- The second approach contains a routing tier. All the requests are first forwarded to the routing tier, and it determines which node to connect to fulfill the request.
- The clients already have the information related to partitioning and which partition is connected to which node. So, they can directly contact the node that contains the data they need

---------------------------------------------------------
---------------------------------------------------------
## 1 feb 23
**System design - Databases replication**

Additional complexities that could arise due to replication are as follows:
- How do we keep multiple copies of data consistent with each other?
- How do we deal with failed replica nodes?
- Should we replicate synchronously or asynchronously?
- How do we deal with replication lag in case of asynchronous replication?
- How do we handle concurrent writes?
- What consistency model needs to be exposed to the end programmers


In *primary-secondary* replication, data is replicated across multiple nodes. 
One node is designated as the primary. It’s responsible for processing any writes to data stored on the cluster. 
It also sends all the writes to the secondary nodes and keeps them in sync.
Primary-secondary replication is appropriate when our workload is read-heavy applications.
To better scale with increasing readers, we can add more followers and distribute the read load across the available followers.
Additionally, primary-secondary replication is inappropriate if our workload is write-heavy

Primary-secondary replication methods
- Statement-based replication
- Write-ahead log (WAL) shipping
- Logical (row-based) log replication

Statement-based replication approach, the primary node saves all statements that it executes, like insert, delete, update, and so on, and sends them to the secondary nodes to perform.
In the write-ahead log (WAL) shipping approach, the primary node saves the query before executing it in a log file known as a write-ahead log file. 
It then uses these logs to copy the data onto the secondary nodes. This is used in PostgreSQL and Oracle. 
The problem with WAL is that it only defines data at a very low level. It’s tightly coupled with the inner structure of the database engine


*Multi-leader replication* is an alternative to single leader replication. 
There are multiple primary nodes that process the writes and send them to all other primary and secondary nodes to replicate. 
This type of replication is used in databases along with external tools like the Tungsten Replicator for MySQL.


Conflict avoidance
A simple strategy to deal with conflicts is to prevent them from happening in the first place. 
Conflicts can be avoided if the application can verify that all writes for a given record go via the same leader
However, the conflict may still occur if a user moves to a different location and is now near a different data center. 
If that happens, we need to reroute the traffic. 
In such scenarios, the conflict avoidance approach fails and results in concurrent writes


Last-write-wins
Using their local clock, all nodes assign a timestamp to each update. 
When a conflict occurs, the update with the latest timestamp is selected.
This approach can also create difficulty because the clock synchronization across nodes is challenging in distributed systems. 
There’s clock skew that can result in data

There are many topologies through which multi-leader replication is implemented, such as 
- circular topology
- star topology
- all-to-all topology. 
The most common is the all-to-all topology. 
In star and circular topology, there’s again a similar drawback that if one of the nodes fails, it can affect the whole system. 
That’s why all-to-all is the most used topology


*Peer-to-peer/leaderless replication*
In primary-secondary replication, the primary node is a bottleneck and a single point of failure. 
Moreover, it helps to achieve read scalability but fails in providing write scalability. 
The peer-to-peer replication model resolves these problems by not having a single primary node. 
All the nodes have equal weightage and can accept reads and writes requests. 

A helpful approach used for solving write-write inconsistency is called quorums.

---------------------------------------------------------
---------------------------------------------------------
## 1 feb 23
**System design - Databases**

- Key-value databases use key-value methods like hash tables to store data in key-value pairs. 
Here, the key serves as a unique or primary key, and the values can be anything ranging from simple scalar values to complex objects. 
These databases allow easy partitioning and horizontal scaling of the data.

- A document database is designed to store and retrieve documents in formats like XML, JSON, BSON, and so on. 
These documents are composed of a hierarchical tree data structure that can include maps, collections, and scalar values. 
Documents in this type of database may have varying structures and data.

- Graph databases use the graph data structure to store data, where nodes represent entities, and edges show relationships between entities. 
The organization of nodes based on relationships leads to interesting patterns between the nodes. 
This database allows us to store the data once and then interpret it differently based on relationships. 
Graph data is kept in store files for persistent storage. 
Each of the files contains data for a specific part of the graph, such as nodes, links, properties, and so on

- Columnar databases store data in columns instead of rows. 
They enable access to all entries in the database column quickly and efficiently.

---------------------------------------------------------
---------------------------------------------------------
## 30 jan 23
**System design - Databases**

*Relational*
Relational databases adhere to particular schemas before storing the data. 
The data stored in relational databases has prior structure. 
Mostly, this model organizes data into one or more relations. 

- Atomicity: A transaction is considered an atomic unit.
- Consistency: At any given time, the database should be in a consistent state, and it should remain in a consistent state after every transaction.
- Isolation: In the case of multiple transactions running concurrently, they shouldn’t be affected by each other.
- Durability: The system should guarantee that completed transactions will survive permanently in the database even in system failure events

Pros

Flexibility
In the context of SQL, data definition language (DDL) provides us the flexibility to modify the database, including tables, columns, renaming the tables, and other changes.
DDL even allows us to modify schema while other queries are happening and the database server is running. Reduced redundancy.

Concurrency
Concurrency is an important factor while designing an enterprise database. 
In such a case, the data is read and written by many users at the same time.

Integration
The process of aggregating data from multiple sources is a common practice in enterprise applications. 
A common way to perform this aggregation is to integrate a shared database where multiple applications store their data.
This way, all the applications can easily access each other’s data while the concurrency control measures handle the access of multiple applications.

Backup and disaster recovery
Relational databases guarantee the state of data is consistent at any tim

Cons
Impedance mismatch is the difference between the relational model and the in-memory data structures


*NoSQL*

A NoSQL database is designed for a variety of data models to access and manage data. 
There are various types of NoSQL databases. 
These databases are used in applications that require a large volume of semi-structured and unstructured data, low latency, and flexible data models. 
This can be achieved by relaxing some of the data consistency restrictions of other databases.
- Simple design: Unlike relational databases, NoSQL doesn’t require dealing with the impedance mismatch.
- Horizontal scaling: Primarily, NoSQL is preferred due to its ability to run databases on a large cluster. This solves the problem when the number of concurrent users increases. 
- Availability: To enhance the availability of data, node replacement can be performed without application downtime. Most of the non-relational databases’ variants support data replication to ensure high availability and disaster recovery.
- Support for unstructured and semi-structured data: Many NoSQL databases work with data that doesn’t have schema at the time of database configuration or data writes.
- Cost: Licenses for many RDBMSs are pretty expensive, while many NoSQL databases are open source and freely available.


---------------------------------------------------------
---------------------------------------------------------
## 24 jan 23
**System design - Load balancer**
Millions of requests could arrive per second in a typical data center. 
To serve these requests, thousands (or a hundred thousand) servers work together to share the load of incoming requests.

The load balancing layer is the first point of contact within a data center after the firewall. 
A load balancer may not be required if a service entertains a few hundred or even a few thousand requests per second. 
However, for increasing client requests, load balancers provide the following capabilities

- Scalability: By adding servers, the capacity of the application/service can be increased seamlessly. Load balancers make such upscaling or downscaling transparent to the end users.
- Availability: Even if some servers go down or suffer a fault, the system still remains available. One of the jobs of the load balancers is to hide faults and failures of servers.
- Performance: Load balancers can forward requests to servers with a lesser load so the user can get a quicker response time. This not only improves performance but also improves resource utilization.
- Health checking: LBs use the heartbeat protocol to monitor the health and, therefore, reliability of end-servers. Another advantage of health checking is the improved user experience.
- TLS termination: LBs reduce the burden on end-servers by handling TLS termination with the client.
- Predictive analytics: LBs can predict traffic patterns through analytics performed over traffic passing through them or using statistics of traffic obtained over time.
- Reduced human intervention: Because of LB automation, reduced system administration efforts are required in handling failures.
- Service discovery: An advantage of LBs is that the clients’ requests are forwarded to appropriate hosting servers by inquiring about the service registry.
- Security: LBs may also improve security by mitigating attacks like denial-of-service (DoS) at different layers of the OSI model (layers 3, 4, and 7).

Global server load balancing (GSLB): GSLB involves the distribution of traffic load across multiple geographical regions.
Local load balancing: This refers to load balancing achieved within a data center. 
This type of load balancing focuses on improving efficiency and better resource utilization of the hosting servers in a data center.

Load balancers distribute client requests according to an algorithm. Some well-known algorithms :
- Round-robin scheduling: In this algorithm, each request is forwarded to a server in the pool in a repeating sequential manner.
- Weighted round-robin: If some servers have a higher capability of serving clients’ requests, then it’s preferred to use a weighted round-robin algorithm. In a weighted round-robin algorithm, each node is assigned a weight. LBs forward clients’ requests according to the weight of the node. The higher the weight, the higher the number of assignments.
- Least connections: In certain cases, even if all the servers have the same capacity to serve clients, uneven load on certain servers is still a possibility. For example, some clients may have a request that requires longer to serve. Or some clients may have subsequent requests on the same connection. In that case, we can use algorithms like least connections where newer arriving requests are assigned to servers with fewer existing connections. LBs keep a state of the number and mapping of existing connections in such a scenario. We’ll discuss more about state maintenance later in the lesson.
- Least response time: In performance-sensitive services, algorithms such as least response time are required. This algorithm ensures that the server with the least response time is requested to serve the clients.
- IP hash: Some applications provide a different level of service to users based on their IP addresses. In that case, hashing the IP address is performed to assign users’ requests to servers.
- URL hash: It may be possible that some services within the application are provided by specific servers only. In that case, a client requesting service from a URL is assigned to a certain cluster or set of servers. The URL hashing algorithm is used in those scenarios.

Layer 4 load balancers: Layer 4 refers to the load balancing performed on the basis of transport protocols like TCP and UDP. 
These types of LBs maintain connection/session with the clients and ensure that the same (TCP/UDP) communication ends up being forwarded to the same back-end server. 
Even though TLS termination is performed at layer 7 LBs, some layer 4 LBs also support it.


Layer 7 load balancers: Layer 7 load balancers are based on the data of application layer protocols. 
It’s possible to make application-aware forwarding decisions based on HTTP headers, URLs, cookies, and other application-specific data—for example, user ID. 
Apart from performing TLS termination, these LBs can take responsibilities like rate limiting users, HTTP routing, and header rewriting.

---------------------------------------------------------
---------------------------------------------------------
## 22 jan 23
**System design - Bulding blocks**
Bulding blocks
- Dns;
- Load balancers;
- Databases;
- Key value store;
- Cdn;
- Sequencer;
- Service monitoring;
- Distributed caching;
- Distributed message queue;
- Pub sub system;
- Rate limiter;
- Blob store;
- Distributed search;
- Distributed logging;
- Distributed task scheduler;
- Sharded counters;

Functional requirements: These represent the features a user of the designed system will be able to use. 
For example, the system will allow a user to search for content using the search bar.

Non-functional requirements (NFRs): The non-functional requirements are criteria based on which the user of a system will consider the system usable. 
NFR may include requirements like high availability, low latency, scalability, and so on.


 ---------------------------------------------------------
---------------------------------------------------------
## 21 jan 23
**System design - back of envelope**
A distributed system has compute nodes connected via a network. 
There’s a wide variety of available compute nodes and they can be connected in many different ways. 
Back-of-the-envelope calculations help us ignore the nitty-gritty details of the system (at least at the design level) and focus on more important aspects.

Some examples of a back-of-the-envelope calculation could be:
 • The number of concurrent TCP connections a server can support.
 • The number of requests per second (RPS) a web, database, or cache server can handle.
 • The storage requirements of a service.

Data centers don’t have a single type of server. 
Enterprise solutions use commodity hardware to save cost and develop scalable solutions. 
Below, we discuss the types of servers that are commonly used within a data center to handle different workloads

For scalability, the web servers are decoupled from the application servers. 
Web servers are the first point of contact after load balancers. 
Data centers have racks full of web servers that usually handle API calls from the clients. 
Depending on the service that’s offered, the memory and storage resources in web servers can be small to medium. 
However, such servers require good computational resources. For example, Facebook has used a web server with 32 GB of RAM and 500 GB of storage space. 
But for its high-end computational needs, it partnered with Intel to build a custom 16-core processor.

Application servers run the core application software and business logic. 
The difference between web servers and application servers is somewhat fuzzy. 
Application servers primarily provide dynamic content, whereas web servers mostly serve static content to the client, which is mostly a web browser. 
They can require extensive computational and storage resources. Storage resources can be volatile and non-volatile. 
Facebook has used application servers with a RAM of up to 256 GB and two types of storage—traditional rotating disks and flash—with a capacity of up to 6.5 TB

With the explosive growth of Internet users, the amount of data stored by giant services has multiplied. 
Additionally, various types of data are now being stored in different storage units. 
For instance, YouTube uses the following datastores:
- Blob storage for its encoded videos.
- A temporary processing queue storage that can hold a few hundred hours of video content uploaded daily to YouTube for processing.
- Specialized storage called Bigtable for storing a large number of thumbnails of videos.
- Relational database management system (RDBMS) for users and videos metadata (comments, likes, user channels, and so on.

Let’s understand two types of requests.
 • CPU-bound requests: These are the type of requests where the limiting factor is the CPU.
 • Memory-bound requests: These are the types of requests that are limited by the amount of memory a machine has.

---------------------------------------------------------
---------------------------------------------------------
## 19 jan 23
**Team leading тет-а-теты**

Важно подготовить личную карточку для каждого сотрудника, в которой будут храниться:
 ◦ Мотивационные факторы;
 ◦ История договорённостей;
 ◦ Важные факты;
 ◦ Перечень достижений и провалов;
 ◦ История встреч;
 
Понимаем какую пользу ожидаем от встреч, какие конкретные вещи и действия ожидаем на выходе.

После встречи прислать сотруднику follow-up с кратким содержанием встречи и вашим договорённостям.
- Обновить карточку сотрудника с учётом новой информации.
- Запросить обратную связь по тому, как прошла встреча.

**Плюсы**
- Подготовил каверзные вопросы заранее, чтобы раскачать человека на диалог, дало мощный эффект, все полчаса
говорил только сотрудник.
- После встречи подбил договоренности, чтобы дать человеку понять, что оно не забудется и будет решено.
Тем самым улучшил мотивацию.
- Создал и заполнил карточки по каждому сотруднику в гугл таблице.

---------------------------------------------------------
---------------------------------------------------------
## 18 jan 23
**System design Main concepts**

Failure models

*Fail-stop*
In this type of failure, a node in the distributed system halts permanently. 
However, the other nodes can still detect that node by communicating with it.
From the perspective of someone who builds distributed systems, fail-stop failures are the simplest and the most convenient.

*Crash*
In this type of failure, a node in the distributed system halts silently, and the other nodes can’t detect that the node has stopped working.

In *omission failures*, the node fails to send or receive messages. 
There are two types of omission failures. 
If the node fails to respond to the incoming request, it’s said to be a send omission failure. 
If the node fails to receive the request and thus can’t acknowledge it, it’s said to be a receive omission failure.

In *temporal failures*, the node generates correct results, but is too late to be useful. 
This failure could be due to bad algorithms, a bad design strategy, or a loss of synchronization between the processor clock.

In *Byzantine failures*, the node exhibits random behavior like transmitting arbitrary messages at arbitrary times, producing wrong results, or stopping midway. 
This mostly happens due to an attack by a malicious entity or a software bug. 
A byzantine failure is the most challenging type of failure to deal with.


Non functional system characteristics
*Availability* is the percentage of time that some service or infrastructure is accessible to clients and is operated upon under normal conditions. 
For example, if a service has 100% availability, it means that the said service functions and responds as intended (operates normally) all the time.
We measure availability as a number of nines.

*Reliability*, R, is the probability that the service will perform its functions for a specified time. R measures how the service performs under varying operating conditions.
We often use mean time between failures (MTBF) and mean time to repair (MTTR) as metrics to measure realibility.


Reliability and availability are two important metrics to measure compliance of service to agreed-upon service level objectives (SLO).
The measurement of availability is driven by time loss, whereas the frequency and impact of failures drive the measure of reliability.

*Scalability* is the ability of a system to handle an increasing amount of workload without compromising performance. 
A search engine, for example, must accommodate increasing numbers of users, as well as the amount of data it indexe

The workload can be of different types, including the following:
 • Request workload: This is the number of requests served by the system.
 • Data/storage workload: This is the amount of data stored by the system.

*Maintainability*
Besides building a system, one of the main tasks afterward is keeping the system up and running by finding and fixing bugs, adding new functionalities, keeping the system’s platform updated, and ensuring smooth system operations. One of the salient features to define such requirements of an exemplary system design is maintainability. We can further divide the concept of maintainability into three underlying aspects
- Operability: This is the ease with which we can ensure the system’s smooth operational running under normal circumstances and achieve normal conditions under a fault;
- Lucidity: This refers to the simplicity of the code. The simpler the code base, the easier it is to understand and maintain it, and vice versa;
- Modifiability: This is the capability of the system to integrate modified, new, and unforeseen features without any hassle;

*Fault tolerance* refers to a system’s ability to execute persistently even if one or more of its components fail. 
Here, components can be software or hardware. 
Conceiving a system that is hundred percent fault-tolerant is practically very difficult

One of the most widely-used techniques is replication-based fault tolerance. 
With this technique, we can replicate both the services and data. 
We can swap out failed nodes with healthy ones and a failed data store with its replica. 
A large service can transparently make the switch without impacting the end customers

Checkpointing is a technique that saves the system’s state in stable storage when the system state is consistent. 
Checkpointing is performed in many stages at different time intervals. 
The primary purpose is to save the computational state at a given point. 
When a failure occurs in the system, we can get the last computed data from the previous checkpoint and start working from there.

---------------------------------------------------------
---------------------------------------------------------
## 17 jan 23
**System design Consistency models**

In distributed systems, consistency may mean many things. 
One is that each replica node has the same view of data at a given point in time. 
The other is that each read request gets the value of the recent write. 
These are not the only definitions of consistency, since there are many forms of consistency.
Normally, consistency models provide us with abstractions to reason about the correctness of a distributed system doing concurrent data reads, writes, and mutations.

If we have to design or build an application in which we need a third-party storage system like S3 or Cassandra, 
we can look into the consistency guarantees provided by S3 to decide whether to use it or not. Let’s explore different types of consistency.
The two ends of the consistency spectrum are:
 • Strongest consistency
 • Weakest consistency

Database rules are at the heart of ACID consistency. 
If a schema specifies that a value must be unique, a consistent system will ensure that the value is unique throughout all actions. 
If a foreign key indicates that deleting one row will also delete associated rows, a consistent system ensures that the state can’t contain related rows once the base row has been destroyed.

CAP consistency guarantees that, in a distributed system, every replica of the same logical value has the same precise value at all times. 
It’s worth noting that this is a logical rather than a physical guarantee. 
Due to the speed of light, replicating numbers throughout a cluster may take some time. 
By preventing clients from accessing different values at separate nodes, the cluster can nevertheless give a logical picture.


*Eventual consistency* is the weakest consistency model. 
The applications that don’t have strict ordering requirements and don’t require reads to return the latest write choose this model. 
Eventual consistency ensures that all the replicas will eventually return the same value to the read request, but the returned value isn’t meant to be the latest value. 
However, the value will finally reach its latest state.
Eventual consistency ensures high availability.

Example
The domain name system is a highly available system that enables name lookups to a hundred million devices across the Internet. 
It uses an eventual consistency model and doesn’t necessarily reflect the latest values.

*Causal consistency*
Causal consistency works by categorizing operations into dependent and independent operations. 
Dependent operations are also called causally-related operations. 
Causal consistency preserves the order of the causally-related operations

The causal consistency model is used in a commenting system. 
For example, for the replies to a comment on a Facebook post, we want to display comments after the comment it replies to. 
This is because there is a cause-and-effect relationship between a comment and its replies.

*Sequential consistency*
Sequential consistency is stronger than the causal consistency model. 
It preserves the ordering specified by each client’s program. 
However, sequential consistency doesn’t ensure that the writes are visible instantaneously or in the same order as they occurred according to some global clock.

Example
In social networking applications, we usually don’t care about the order in which some of our friends’ posts appear. 
However, we still anticipate a single friend’s posts to appear in the correct order in which they were created). 
Similarly, we expect our friends’ comments in a post to display in the order that they were submitted. The sequential consistency model captures all of these qualities.


A *strict consistency or linearizability* is the strongest consistency model. 
This model ensures that a read request from any replicas will get the latest write value. 
Once the client receives the acknowledgment that the write operation has been performed, other clients can read that value.

Synchronous replication ensures linearizability, in which an acknowledgment is not sent to the client until the new value is written to all replicas.
Linearizability affects the system’s availability, which is why it’s not always used. 
Applications with strong consistency requirements use techniques like quorum-based replication to increase the system’s availability.


---------------------------------------------------------
---------------------------------------------------------
## 16 jan 23
**System design abstractions**
System design is the process of defining components and their integration, APIs, and data models to build large-scale systems that meet a specified set of functional and non-functional requirements.
The discipline of system design helps us tame this complexity and get the work done.

System design aims to build systems that are reliable, effective, and maintainable, among other characteristics.
 • Reliable systems handle faults, failures, and errors.
 • Effective systems meet all user needs and business requirements.
 • Maintainable systems are flexible and easy to scale up or down. The ability to add new features also comes under the umbrella of maintainability.

Abstraction is the art of obfuscating details that we don’t need. It allows us to concentrate on the big picture.

Remote procedure calls (RPCs) provide an abstraction of a local procedure call to the developers by hiding the complexities of packing and 
sending function arguments to the remote server, receiving the return values, and managing any network retries.

---------------------------------------------------------
---------------------------------------------------------
## 7 jan 23
**Курс тимлид - неделя 5**
тех долг и качество кода

Техдолг — это всё, что отнимает внимание людей;
Неаккуратный и сложный код убивает внимание: если код в проекте отстой, то каждый джун в вашей компании каждое утро 
будет за два часа сжигать об него весь свой дневной запас внимания — и на фичи и профессиональный рост ресурсов уже не останется.

Вещи, которые гарантированно сжирают внимание
- Долгий feedback loop: релизные поезда, ручной деплой, тормозные пайплайны CI\CD;
- Высокий порог входа в проект — сложные зависимости, старые либы;
- Нечитаемый код в часто обновляемых местах;
- Кросс связи в команде: к примеру, iOS-разработчику приходится ставить Ruby, чтобы развернуть бекенд.
- Структуры данных, которые не соответствуют реальному миру: «божественные» таблицы, данные без схемы и т. д;
- Собственные инструменты вместо готовых;

По большому счёту продукт тимлида — это среда, в которой разработчики счастливы и не тратят время на глупую или скучную фигню: 
разворачивание проекта, ковыряние в говнокоде, общение с людьми, которые не умеют выражать мысли, 
анализ невнятных требований, пустые встречи и т. д. 
Чем меньше сил разработчик тратит на попытки продраться через неудобную среду, тем больше сил у него остаётся на полезную для бизнеса работу.

Не получится обмануть только метрики, на которые может посмотреть бизнес:
- time2market. Время от появления задачи в трекере до продакшена;
- Сложность приёмки. Как часто мы возвращаем задачи на доработку, грубо говоря, часто ли программисты делают «не то» и «не так»;
- Количество дефектов в эксплуатации. Часто ли мы ломаем прод?

Понятие техдолга тяжело воспринимать, пока не понимаешь, чем плохой код отличается от хорошего. 
Можно использовать метафору обычного денежного долга

Продуктовый налог: встраиваем рефакторинг в спринты
Если заказчик понимает технический долг так же, как денежный, половина дела сделана. 
Остаётся только договориться о том, что часть ваших ресурсов будет расходоваться на оплату этого долга

Не забывать про потребности бизнеса
Перегибать со сложностью, качеством и читаемостью тоже не стоит. 
Скажем, Kafka и CQRS не нужны, если вы делаете сайт регионального турагентства: как ни вкладывайся в архитектуру, 50 посетителей этого не оценят

Как объяснить, что такое достаточное качество
Иногда рациональные разговоры не действуют — в любых рациональных доводах легко услышать, что твой тимлид говорит тебе «слишком хорошо» и «слишком надёжно». 
Если с вашими ребятами такое случается — поговорите с ними о бизнесе. 
Напомните, что это не бизнес существует, чтобы спонсировать их инженерные амбиции, а инженеры существуют, чтобы приносить бизнесу деньги. 
 
**Плюсы**
- Нужно закладывать время на рефакторинг, сразу в большие задачи, чтобы не обсуждать это с продактами.
- Не перегибать с качеством;

**Минусы**
- Может быть достаточно сложно рефакторить унаследованный код, поскольку он настолько запутан, что его проще переписать.
 
---------------------------------------------------------
---------------------------------------------------------
## 31 december 22
**Курс тимлид - неделя 4**
продакт менеджмент примерить шляпу бизнеса

1) Начинаем пилить и получаем первую шишку
Наняли программистов потратили денег,  сделали сырой продукт, он никому не нужен, есть работающий конкурент.

В терминологии стартапов есть понятие Product-Market Fit, один из главных показателей успешности стартапа.
Причины смерти стартапов
- нет потребности на рынке
- закончился кэш

Jobs to be done - фреймворк, который позволяет сфокусировать нашу гипотезу. 
люди покупают продукт не для того, чтобы купить его, а чтобы решить свою задачу.

2) Проверяем гипотезу

Нужно проверить как можно быстрее и дешевле сформулированную выше гипотезу. 
Лучший способ проверить гипотезу — пойти и продать свой продукт.

Чтобы продать продукт, его вовсе не обязательно разрабатывать и запускать, люди готовы платить деньги даже за мокапы.
Если вы смогли заключить хотя бы пару контрактов, в которых люди обещают вам начать платить деньги, как только вы запустите свою систему, можете считать свою гипотезу подтвержденной и переходить к MVP.

Кому это нужно: исследуем аудиторию
Исследование аудитории - на рынке есть проблема, которую вы собираетесь решать. 
Сильнее всего в исследовании аудитории продвинулся метод дизайн-мышления.

Глубинное интервью — это встреча с пользователем (обычно не более часа), на которой вы, задавая открытые вопросы, узнаете о его опыте, чтобы сделать выводы и получить инсайты.
Когда вы поняли свою аудиторию, имеет смысл еще раз посмотреть на вашу гипотезу и ее уточнить.
А еще не помешает посмотреть, есть ли вообще запросы пользователей в поисковике по нашей теме, и если да, то какие.

Проверяем гипотезу без разработки, чтобы повысить вероятность, что она сработает.
выполним задачу клиента «на руках» — просто наймем на полставки живого человека
Если не докажем гипотезу — сэкономим кучу денег на MVP.
Если докажем — получим кучу ценных данных: возражения, которые дают клиенты, стоимость привлечения одного клиента.

Придет понимание сколько денег уйдет на продажников
откуда они возьмут базу потенциальных клиентов.


3) Считаем деньги

Воспользуемся для этого ключевым вопросом из юнит-экономики: зарабатываем ли мы с пользователя больше денег, чем нам обошлось его привлечение

Как формировать цену на продукт?
Подумать о своей психологически комфортной цене, а потом пририсуйте к цене нолик и подумайте, какой ценности не хватает в продукте, чтобы продавать его в 10 раз дороже.

4) Добавляем функциональность b2c

В b2c привлечение покупателей происходит немного по-другому — обычно это работа по готовой аудитории (вроде телеграм-канала или паблика вконтосе) или платная контекстная реклама.
Наша задача — понять, можем ли мы привлечь пользователя дешевле, чем зарабатываем с него денег.

5) Строим системный рост — циклы PDCA

Самое время выстроить системный рост. 
Задача роста — привлекать больше людей, не роняя доход.
 Для этого нужно либо уменьшать CAC, либо увеличивать LTV. 
 Одновременно на две эти цифры можно повлиять разработкой: если вы будете запускать нужные клиентам фичи, которых нет у ваших конкурентов, вы, скорее всего, будете дешевле привлекать людей и больше получать с них денег
разработка — штука дорогая, и сжечь через нее деньги еще проще, чем через контекстную рекламу

Как принимать продуктовые решения. 
Возьмем 4 шага:
 • Спланировать (Plan). Выделить гипотезу, определить самый дешевый механизм ее проверки: не пилить фичу, а сделать фокус-группу.
 • Сделать (Do). Собственно, проверить гипотезу. Поговорить с людьми, запустить фичу.
 • Проверить результаты (Check). Сесть и посмотреть на результаты. Скажем, «сделали кнопку, на нее нажало 500 человек за день» или «сходили по 20 клиентам, из них никто не выразил заинтересованности».
 • Действовать (Act). Если гипотеза выстрелила, инвестируем в продукт на ее основе — полноценно пилим фичу.
 
 **Плюсы**
 - Нужно поиметь опыт проработки фич.
 
 
---------------------------------------------------------
## 26 december 22
**Курс тимлид - неделя 3**

Процессы и ритуалы в команде
- Чем более самостоятельные люди у вас в команде, тем меньше процессов нужно для их контроля;
- Есть люди-change и люди-run. И сравнили их с бегом: есть спринт и марафон. Оба вида — это бег, но марафонцу помогает одно, а спринтеру — другое;
- Процессы бывают основными и вспомогательными. Основные — связанные непосредственно с созданием продукта, вспомогательные — с атмосферой и условиями труда;

Поставить задачу, назначить ответственного или даже выставить дедлайн — это совершенно несложно. 
Сложно сделать так, чтобы эта задача выполнилась. 
И для этого нужно задавать ритм.

Задача лидера — задать и поддерживать такой ритм, который не выматывает команду, но при этом закрывает потребности бизнеса

Типсы:
- Просто раз в неделю пишем письмо о том, что за эту неделю случилось. (по сути надо будет расширить, результаты текущей лидовской планерки на письмо)
В адресаты у письма ставим всех причастных — и стейкхолдеров, и команду, письмо не отменяем ни в коем случае: оно должно уходить 52 раза в год. 

- Кому-то для задания ритма достаточно простой устной договорённости, а кому-то нужен микроконтроль.
- Чем взрослее человек, тем меньше нужно управленческих воздействий, чтобы он совершал задачу.

Есть такой опасный типаж исполнителей — ненастоящие взрослые. Это ребята, которые ведут себя уверенно, общаются взвешенно, обещания дают чётко и внятно. 
Когда наступает дедлайн, они даже что-то приносят: только это «что-то» оказывается на поверку дерьмом.


Как воспитать у других ответственность
- Собственным примером. Если лично вас нужно бить палкой (или штрафовать), то и вы как-то неявно это будете транслировать команде, и рано или поздно все начнут работать как вы. 
Если вы сами будете выполнять обещания — команда будет стремиться к тому же. 

Основной процесс похож на цикл Деминга (PDCA)
Этапы:
 1 Планирование (plan)
 2 50% cпринта (do)
 3 Подведение итогов (check) 
 4 Работа над улучшениями (act)

Планирование
Во время планирования по каждой задаче нужно получить чёткие обещания:
 • Нам понятно, что нужно сделать по этой задаче;
 • Нам понятно, для чего мы делаем эту задачу, что изменится в бизнесе;
 • Мы берём эту задачу в работу;
 • Мы собираемся её закончить к концу спринта / мы закончим только часть / мы только начнём делать;

Definition of Done: что нужно сделать по задаче
- Это список пунктов, которые вы проверите, когда задача будет сделана, чёткое определение того, что именно можно считать завершённой задачей;
- DoD клёво делать списком, чтобы можно было поставить рядом с каждым пунктом галочку и точно не пропустить;
- Для чего мы делаем эту задачу? Что изменится в бизнесе;
- задачи нужно привязывать к бизнесовым метрикам. В идеале — к денежным;
- Задачи для счастья — это хорошо и правильно. Потому что они кроме счастья ещё и дают разгон для выполнения других задач и помогают не сгореть в аду задач;
- А что будет, если я её не сделаю?»;


Декомпозируем задачи: что успеем к концу спринта
- Надо оценивать задачи или нет — вопрос очень спорный. С одной стороны, это даёт предсказуемость, планирование выглядит четко, как в бухгалтерии: накидал на неделю задач на 40 часов (вернее, на 30, нужен же запас), 
и дальше получай результат. Это добавляет прозрачности в общении с заказчиком: он понимает, что происходит. Ну или думает, что понимает.
С другой стороны, оценка задач, несмотря на все игрища с голосованием и planning poker, очень часто оказывается неадекватной: в процессе вылезает техдолг, 
меняются условия задачи или даже изобретается новый способ решить задачу в 10 раз быстрее. Задача не попадает в оценку, начинаются проблемы с доверием;

Не даем оценку маленьким задачам:
- если в задаче указано, сколько её нужно делать, это подрывает саму основу контракта с исполнителем: ведь я же, отдавая задачу, ожидаю получить фичу на проде, а не 5 часов работы по написанию кода;
- работа всегда занимает всё отведённое на неё время, даже когда этого времени отведено чрезмерно много. Вот приходит тебе задача, оцененная в 5 часов, и у тебя уже нет стимула придумать, как сделать её за час;

Важно, чтобы люди давали обещания и выполняли их, а не ставили эстимейты, в которые потом всё равно никто не попадает.

Оценка задачи.
 1 Важно понимать, в каком состоянии вы оцениваете задачи. Это торги, на которых заказчик пытается продавить оценку пониже, а программист сдаётся, или спокойная атмосфера, где есть возможность спокойно обдумать?
 2 Это оценка «пальцем в небо»? Или, может, вы вспомнили похожие задачи и оценили на их основе? У первой точность невысока, на вторую можно опираться. 
 3 Насколько хорошо программист понимает задачу? Попросите его голосом рассказать, что он будет делать, а ещё лучше — написать.
 4 Учитывается ли при планировании уменьшение ресурса? Отпуска, отгулы, больничные, праздничные дни и переключение на починку внезапно срочных багов.
 5 Сильно ли раздуваются задачи после того, как их взяли в спринт? Например, договорились об одном MVP, а дизайнер решил усложнить задачу и на полноценную реализацию уже не хватает времени.

50% спринта
- Ровно посередине спринта собираем команду и спрашиваем, как у каждого дела по их задачам. Сделана ли половина? Есть ли проблемы? Как менеджер может помочь?


Классика детского сада: дейли-митинги
- Не нужно собирать всю команду, чтобы узнать, кто что будет делать сегодня
- Не нужно ждать дейли-митинга, чтобы сообщить о проблеме

Подведение итогов
Демо по задачам
- Если программист утверждает, что задача сделана, неплохо было бы это доказать. 
В больших компаниях для этого используют QA, который проверяет, что бизнес-требования выполнены и ничего не сломалось. Это медленно и неуважительно.
Странно потому, что проверка и сдача — это такая же часть работы, как написание кода. Когда программист передаёт непроверенную задачу QA, он как бы говорит: «Я тут что-то сделяль, посмотри, то это или нет». 
Вместо того, чтобы вдумчиво разобраться в бизнес-требованиях и покрыть сценарии автотестами, он отдаёт свою работу дополнительному человеку
Попросите программистов самостоятельно сдавать свою работу.

- Требовать результата
- От обещаний, которые не выполняются, нет никакого толку. Если вы не будете спрашивать с людей за обещания, которые вам дали и не выполнили, то у вас останется единственный способ сделать задачу гарантированно — сделать её самому.
Ребята в команде быстро перенимают эту привычку и начинают спрашивать за обещания уже друг с друга. 
В какой-то момент функция внешнего контроля в такой команде становится ненужной — ребята просто сами всё делают вовремя.
- проверять итоги спринтов, чтобы не осталось ничего незакрытого или тихонько перемещённого в следующий спринт. 
При работе по канбану ставить себе напоминалки даже для взрослых людей.

Что делать, если команда сорвала спринт, хотя были обещания закончить?
 1 Не устраивать публичную порку. Это сейчас ни к чему. Скорее всего, ребята сами себя винят. Даже если это не впервые;
 2 Разобраться, почему так случилось. Посмотреть, как часто это случается;
 3 Разработайте план, как выполнить обещание. Если это критичная задача — придумайте, от каких требований можно отказаться, или выбрасывайте все остальные;

Нужно ли стремиться закрывать спринты «в ноль», пытаться идеально планировать, если главные цели — квартальные, а как ты разбираешься со спринтами — не важно?
- Да.
- Во-первых, потому что это на уровне майндсета: если вы позволяете себе не выполнить план недели, то ничего не мешает сорвать квартальные. 
- Во-вторых, маленькие шаги важны. Они повышают вероятность, что вы точно добежите до цели. 
- В-третьих, чувство завершённости всегда приятнее, чем его отсутствие. Вспомните, как вы себя чувствовали, когда зачёркивали все запланированные дела за день;

Ретроспектива
Цель ретроспективы — остановиться и посмотреть, что было, как мы себя чувствовали и куда хотим прийти. 
Проводите хотя бы по окончании большого этапа, но не реже чем раз в месяц. Идеально — в конце каждого спринта.
Ретро — это самый недооценённый инструмент для поиска точек роста. И на то есть ряд причин: 
 • Во-первых, потому что его часто проводят для галочки. Потому что так сказал шеф или написано в скраме. А значит, для участников ретро это пустая трата времени, т. к. цель — это провести, а не найти улучшения.
 • Во-вторых, потому что не рождаются улучшения. Или не берутся потом в работу.

Самый простой способ состоит из нескольких шагов:
 1 Вы создаёте табличку с полями: «Что было хорошо», «Что хочется улучшить», «Что перестать делать». «Благодарность». Вот шаблон (https://docs.google.com/spreadsheets/d/1LW_vsr4Bn3Y_p3DdX-iKZ9dRP_3uvtEcUitFagfIslU/edit#gid=0). Даете заполнить каждому участнику.
 2 Встреча. Обычно 1,5–2 часа. Зависит от количества участников и сложности ретро. На ней каждый проговаривает, что имел в виду. Это важно, чтобы все имели одинаковый контекст. Желательно, чтобы встречу вёл человек извне команды, чтобы был беспристрастен;
 3 Со встречи выйти с решением. После того, как все рассказали свои мысли, важно выбрать 2-3 улучшения, которые берёт команда, чтобы улучшить свою жизнь;
 4 Следующее ретро начинается с подведения результатов предыдущего. И оно не назначается, пока не решили предыдущие вопросы. Иначе ретро начинает быть пустой тратой времени. Зачем проводить встречу, если никто не делает улучшения. Значит, всем норм работать в этом *овне;


Если мы хотим внедрить новую инициативу, у этой инициативы должен быть оунер — тот, кому больше всего надо.
Мы не организовываем новые ретро, пока не начали решать задачи с предыдущего. И исключаем позицию жертвы, когда все жалуются, но никто не берёт задачи.


Коммуникация и уважение ко времени
Никакие процессы работать не будут, если люди в команде не уважают время друг друга: требуют немедленного ответа (или даже звонят по телефону!).

Даже если вы работаете в офисе — старайтесь быть асинхронными:
 1 Не ходите на встречи, если не знаете, зачем вы там.
 2 Предпочитайте почту чату.
 3 Если уж пользуетесь чатом — пишите как в почте, по принципу «в письме всё есть (https://bureau.ru/soviet/20190307/)».
 4 Если говорите голосом — пишите минутки (см. ниже), чтобы не вспоминать, о чём договорилис

Как настроить процессы, если в работе возникает много вопросов друг к другу, чтобы не отвлекать и не залипать в ожидании ответа?
- Ограничить количество коммуникации, чтобы каждое сообщение стало более ценным;
- Асинхронная коммуникация: когда ты представляешь, что человек сидит на «необитаемом острове» и у тебя есть возможность написать ему одно письмо в день. 
Тогда скорее всего это письмо будет более содержательным и в нём будет всё, что ты хочешь спросить;

- «минутки», «митинг ноутс» и «фоллоу-апы». Это такие простые письма, в которых вы явно фиксируете договорённости, которые проговорили устно. 
Имея такое письмо, всегда легко «подтянуть» человека за невыполненные обязательства.

Пара правил личных встреч
Самый большой потребитель на работе — это встречи. Пришли 10 человек на часовую встречу — компания потратила 10 человеко-часов. Ещё хуже, когда люди пытаются работать на таких ненужных встречах: проверяют почту или даже пытаются делать что-то новое. Человеческий мозг устроен так, что не может фокусироваться на двух вещах одновременно, так что таких людей нет ни на встрече, ни в почте. 
Чтобы не расходовать время на встречи, соблюдайте несколько простых правил:
 • Все встречи опциональны. Если вас зовут на встречу — спросите, зачем вы там. Если не могут объяснить или отмазываются вроде «ты можешь пригодиться» — не ходите.
 • Пишите сами и требуйте у других повестки встреч. Встречи без повестки длятся дольше, а пользы приносят меньше.
 • Ходите на встречи без ноутбука и не стесняйтесь требовать того же самого от коллег.

Составляйте асинхронную базу знаний
- Простое правило: каждый программист может попросить меня поговорить о любой теме, но мы сделаем это под запись. 
Так, примерно за полгода, проводя по 2–4 встречи в месяц, можно передать почти все знания о коде, которые были в голове.

- Празднуйте победы.

**Плюсы**
- Появилось понимание как улучшить текущие процессы (снизить нагрузку на себя, ввести дежурства, составить асинхронную базу знаний, нормально проводить ретро - не для галочки);

**Минусы**
- Минусов в этом уроке не вижу.
---------------------------------------------------------
## 21 december 22
**Курс тимлид - неделя 2**
Как нанимать команду и налаживать коммуникации

1) понять кто нужен восточный и западный подход.
- Западный - жесткий, люди маленькие винтики;
- Восточный - мягкий система состоит из людей;

По мере роста организации подход смещается от восточного к западному.

2) понять кому нужны вы - конкуретные преимущества

Рынок перенасыщен - даже джуны быстро находят работу.
Нужно заранее поработать на преимуществами.

Преимущества
- зп
- печеньки
- бытовые плюсы

Не материальные
- проф рост
- социальное значение, многим важно чтобы компания делала что то хорошее
- технологии
- личный бренд
- формат работы, удаленка, гибкий график


Написать классную вакансию
- технологии;
- легаси, скажите честно, что нагавнокодили монолит на 300к строк;
- качество кода, количество тестов, уровень цикломатической сложности;
- устройство команды;

Покажите как у вас хорошо - второй действенный способ, чтобы люди сами захотели у вас работать.
Здесь поможет публичность
- телеграм канал
- конференции
- opensource

Сделайте интересное тестовое задание. Идеальное тестовое - реальная задача на проекте

3) расширьте воронку входящих кандидатов

- Попросите знакомых или коллег, тех кто уже работает, рассказать о компании;
- бонус лучше делать маленьким и выдавать после испытательного срока;

Ресурсы
- телеграм знакомых;
- чатики фрейморков;
- специализированные сайты;

4) отсейте ненужных и выберите нужных

Не стесняйтесь отказывать в собеседовании

Проверяйте софт скилы
- насколько человек ответственный;
- как относится к ошибкам;
- насколько важно свое мнение;
- честность;
- интерес;

Важный вопрос - почему уходишь с другого места работы;

Конечное решение принимает команда - а не ТИМЛИД!

Плюсы
- ответственность разделена на команде и лиде;
- смотрим на кандидата с разных сторон;
- чаще находим нашего человека;

Минусы
- удлиняется процесс найма;
- Кандидат может резко уйти на другой оффер;

5) сделайте чтобы люди остались

Барьеры которые мешают доверию
- часть коллектива работает удаленно

Встречи 1:1 инструмент роста
- обсудить то что нельзя при всех;
- расширить картинку происходящего;
- точки роста;

Fire fast - увольняем быстро.
- регулярно делает ошибки и не стремится их исправить;
- команде сложно с ним общаться;
- не на своем месте;

В начале разговора дайте понять что речь будет идти об увольнении
- решение не подлежит обсуждению;
- четко обьясните причины;
- если не хотят слушать, дайте выбор уйти или послушать отзыв о работе;

**Плюсы**
- Fire fast идеальная стратегия, нельзя держать человека больше месяца-край двух, если не приносит явной пользы (западный подход наверно, но и пофиг).
- 1:1 про просто как дела, не сработает, нужно стараться узнавать боли, проблемы, и куда хочет двигаться человек, какие долгосрочные цели.
- хорошая воронка позволит найти лучших кандидатов.

**Минусы**
- слишком сложная воронка, снизит вероятность найма хорошего кандидата.
---------------------------------------------------------
## 12 december 22
**Курс тимлид - неделя 1**

Встречи
1) Подготовиться заранее. 
Если предстоят сложные переговоры, значит проблема накопилась и нужно было предпринимать действия заранее
Следить за процессами. 
Не нуждаться в сделке (переговоры о работе и тп) - чувствовать себя уверенно. Человеку нужно оставлять право на нет, а не загонять в угол;

План встречи:
- Что будем обсуждать;
- Что вы будете чувствовать и ваш партнер;
- Чего хотим добиться;

Важно думать насколько полезна идея для других.
Некоторые встречи ваще можно не проводить.

2) Поймите свои эмоции и чувстсва

Прорабатывать эмоциональный интеллект.

3) уберите барьеры
Чем более эмоциональный разговор тем ближе вы должны быть. Чат, звонок, видеовстреча, личная встреча.

Барьеры:
- Недостаточно передающий формат встречи;
- Вы или партнер не в ресурсе;
- Лишние люди;
- Давление времени;

4) говорите о чувствах
Можно рассказать аудитории о том что вы чувствуете, Выдохнуть и начать разговор.

5) поймите эмоции собеседника
И снимите напряжение если оно есть.

Не копите обиды и эмоции, разбирайте почему вы так думаете.
Abc анализ
- A Событие - было что то, Поругались на ретре;
- B автоматическая мысль - теперь меня буду считать таким то;
- C эмоция реакция - я злюсь;

Можно назначить встречу и разобрать


6) узнайте что важно вашему собеседнику

Открытые вопросы и прием примерить шляпу собеседника
Первая задача в таких ситуациях — снять напряжение. 
Все эмоциональные переговоры, ничем хорошим не заканчиваются. 
Рассказать об искренних намерениях — это хороший способ обнулить эмоции и начать всё сначала.
Что вам на самом деле важно

7) повторите много раз

**Плюсы**
- Буду стараться постепенно внедрять (снятие барьеров, разговоры о чувствах) на встречах.

**Минусы**
- прорабатывать эмоциональный интеллект, не так то просто и найти бы еще нормальных людей под это;
- не всегда возможно снизить градус напряжения, в таком случае приходится применять "силу" авторитета или своей уверенности;

**Саморефлексия**
Последние важные неудачные переговоры были на собеседовании в одну аутсорс компанию:)
Чувствовал себя более менее спокойно перед ними;
Что сделал не правильно:
- не было чувства, что я не нуждаюсь в сделке.
- интервьювер скорее всего не был в ресурсе;
- не говорил о свох чувствах, только в конце;
- не узнал почему важно именно ответить на вопросы по алгоритмам, сбился на них, ни разу не сталкивался с такими;
До сих пор волнует - что не ответил на вопросы про алгоритмы:)



[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 20-21 sep 22
**Camunda events deep dive**

Error + compensation events.

Compensation events will work only after process completion.
Error events can be handled in subprocess then parent process is working, can be boundary events.

![image](https://user-images.githubusercontent.com/49956820/191318013-bc2e93ab-5117-41cb-9940-a099def0fc8d.png)

https://github.com/zolotarevandrew/camunda/tree/main/CamundaTests/CamundaTests/Models/BookingFlowV2

**Pluses**
- I can now use compensation events for process cancellation correctly;
- I can now use subprocesses with events correctly to split up some parallel processing (microservices);
- I can now use error events to change workflow explicitly;


**Minuses**
- We have a problem with events on our work, because of their async manner, they can stuck in camunda queue...;

[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 15 sep 22
**Camunda events deep dive**

Boundary events;

Interrupting - then received interrupt current process and token moving by new condition by event (but external task was not cancelled!!);
![image](https://user-images.githubusercontent.com/49956820/190448423-1ba90fff-aa92-494e-bb7f-ae5e47dba861.png)

Noninterrupting - then received new token created and process is splitting;
![image](https://user-images.githubusercontent.com/49956820/190450666-05348372-dca8-468e-8ac2-e41ca23e32a5.png)

Timer events - can be start events (run every week or day some task).
Interrupting - interupt current process;
Non interuppting - don't interrupt current process;

![image](https://user-images.githubusercontent.com/49956820/190453655-d0af4a8e-d8e9-4e53-a9cb-59cda68a6c38.png)

https://github.com/zolotarevandrew/camunda/tree/main/CamundaTests/CamundaTests/Models/BoundaryEvents
https://github.com/zolotarevandrew/camunda/tree/main/CamundaTests/CamundaTests/Models/BoundaryEvents

[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 14 sep 22
**Camunda event process booking**

Parent process not receive subprocess throwing event.

![image](https://user-images.githubusercontent.com/49956820/190214012-7a1615ae-f51c-4295-adbe-d2a01b3aa388.png)

https://github.com/zolotarevandrew/camunda/tree/main/CamundaTests/CamundaTests/Models/BookingFlow

[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 12-13 sep 22
**Camunda event process trying**

Events with subprocess now working. They should be inside the process to work..

![image](https://user-images.githubusercontent.com/49956820/190204238-91061897-515f-42a8-81c3-888d66dadd8a.png)

https://github.com/zolotarevandrew/camunda/tree/main/CamundaTests/CamundaTests/Models/EventMessages

[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 9 sep 22
**Camunda event process**

Tried to use events with subprocess. It's not working now...

![image](https://user-images.githubusercontent.com/49956820/189517078-12701672-36f8-428c-97a4-092080690eb0.png)

https://github.com/zolotarevandrew/camunda/tree/main/CamundaTests/CamundaTests/Models/EventMessages

[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 8 sep 22
**Camunda extended events**

Start events - events which creates new process;
Intermediate events - events which can be in execution of process;
End events - events can kill token or process; 

Interrupting events - interrupt items;
Non interrupting events - non interrupt items;

Subprocess event - can only be started events;

Event types - behaviour:
- None;
- Message;
- Timer;
- Error;
- Escalation;
- Cancel;
- Compensation;
- Conditional;
- Link;
- Signal;
- Terminate;
- Multiple;
- Multiple parallel;


[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 7 sep 22
**Camunda pizza workflow 1.2**

Timer throw events works incorrect? Parallel Notify pizza didn't worked..

![image](https://user-images.githubusercontent.com/49956820/188914146-24ad7de2-c67d-44a4-acca-aacbd7c5bc83.png)


https://github.com/zolotarevandrew/camunda/tree/main/CamundaTests/CamundaTests/Models/ComplexPizzaOrder

**Pluses**
- I can now build not so complex workflows using base elements;

**Minuses**
- I didn't solve problems with boundary events, so i am going to deep dive into it later;

[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 6 sep 22
**Camunda pizza workflow 1.1**

Used workflow without participants, but with subprocesses, now its working.
![image](https://user-images.githubusercontent.com/49956820/188666189-be0ad9ec-12d6-4844-9932-9e5351872b21.png)

https://github.com/zolotarevandrew/camunda/tree/main/CamundaTests/CamundaTests/Models/ComplexPizzaOrder

[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 5 sep 22
**Camunda pizza workflow 1.0**

Tried to build simple pizza workflow by participants. That's not worked:)

![image](https://user-images.githubusercontent.com/49956820/188658470-c749ba08-7157-4c51-a7e5-e561ceec88b0.png)

https://github.com/zolotarevandrew/camunda/tree/main/CamundaTests/CamundaTests/Models/ComplexPizzaOrder

[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 30 aug 22
**Camunda workers**

- Minimize what data you read for your job. In your job client, you can define which process variables you will need in your worker;
- Minimize what data you write on job completion. You should explicitly not transmit the input variables of a job upon completion;

Workers can control the number of jobs retrieved at once. 
- In a busy system it makes sense to not only request one job, but probably 20 or even up to 50 jobs in one remote request to the workflow engine, and then start working on them locally;
- In a lesser utilized system, long polling is used to avoid delays when a job comes in; 

Long polling means the client’s request to fetch jobs is blocked until a job is received.

Whenever a process instance arrives at a service task, a new job is created and pushed to an internal persistent queue within Camunda. 
A client application can subscribe to these jobs with the workflow engine by the task type name.

[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 29 aug 22
**Camunda architecture**

Base components
- Process Engine Public API - API allowing Java applications to interact with the process engine;
- BPMN 2.0 Core Engine - lightweight execution engine for graph structures and BPMN 2.0 parser which transforms BPMN 2.0 XML files into Java Objects and a set of BPMN Behavior implementations;
- Job Executor - processing asynchronous background work such as Timers or asynchronous continuations in a process;
- Persistence Layer - persisting process instance state to a relational database (they using ORM).

Clustering model:
- In order to provide scale-up or fail-over capabilities, the process engine can be distributed to different nodes in a cluster. 
Each process engine instance must then connect to a shared database.
The individual process engine instances do not maintain session state across transactions. 
Whenever the process engine runs a transaction, the complete state is flushed out to the shared database. 
This makes it possible to route subsequent requests which do work in the same process instance to different cluster nodes;
- There is no load-balancing capabilities or session replication capabilities. It should be provided by a third-party system;
- The process engine job executor is also clustered and runs on each node;

Multi-tenancy:
- Table-level data separation by using different database schemas or databases;
- Row-level data separation by using a tenant marker;
Users should choose the model which fits their data separation needs.

Process engine services:
- RepositoryService - offers operations for managing and manipulating deployments and process definitions. A deployment is the unit of packaging within the engine.  
Allows to Query on deployments and process definitions known to the engine;
- RuntimeService - starting new process instances of process definitions. A process instance is one execution of such a process definition. 
For each process definition there are typically many instances running at the same time. Also used to retrieve and store process variables. 
This is data specific to the given process instance and can be used by various constructs in the process 
(e.g., an exclusive gateway often uses process variables to determine which path is chosen to continue the process).
Allows to query on process instances and executions. Basically an execution is a pointer pointing to where the process instance currently is. 
Also used whenever a process instance is waiting for an external trigger and the process needs to be continued;
- TaskService - Querying tasks assigned to users or groups. Creating new standalone tasks (not related to process instances). Manipulating to which user a task is assigned.
Claiming - someone decided to be the assignee for the task, meaning that this user will complete the task. 
Completing means ‘doing the work of the tasks’;
- Identity service - allows the management (creation, update, deletion, querying, …) of groups and users. core engine actually doesn’t do any checking on users at runtime;
- HistoryService - historical data gathered by the engine;
- ManagementService - query capabilities and management operations for jobs. Jobs are used in the engine for various things such as timers, asynchronous continuations, delayed suspension/activation;
- FilterService allows to create and manage filters;
- ExternalTaskService - access to internal task instances;
- CaseService - It deals with starting new case instances of case definitions and managing the lifecycle of case executions;
- DecisionService - evaluate decisions that are deployed to the engine;

**Minuses**
- As a c# developer i can't use full features of camunda, because it java based;

[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 25 aug 22
**Camunda call activities**
A call activity (or reusable subprocess) allows you to call and invoke another process as part of this process.
When a call activity is entered, a new process instance of the referenced process is created. The new process instance is activated at the none start event.
Interrupting and non-interrupting boundary events can be attached to a call activity.

![image](https://user-images.githubusercontent.com/49956820/186733497-32143bc2-3a67-4cc3-b531-d5bee507636a.png)

https://github.com/zolotarevandrew/camunda/tree/main/CamundaTests/CamundaTests/Models/SimpleCallActivity

**Pluses**
- I can now use call activities to separate bpmn schemes;

**Minuses**
- It is hard to support different bpmns and output/input mapping should be defined;

[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 24 aug 22
**Camunda gateways, events**
Exclusive gateway - XOR-gateway allows you to make a decision based on data (i.e. on process instance variables).
Parallel gateway - AND-gateway allows you to split the flow into concurrent paths.
Event based gateway - must have at least two outgoing sequence flows. Each sequence flow must to be connected to an intermediate catch event of type timer or message.

Message events - events which reference a message; they are used to wait until a proper message is received.
Intermediate message catch event - then entered, a corresponding message subscription is created.

![event_gateway](https://user-images.githubusercontent.com/49956820/186483800-93bcf468-570a-49dd-8cd8-0b42b28e5d59.png)

https://github.com/zolotarevandrew/camunda/tree/main/CamundaTests/CamundaTests/Models/SimpleEventGateway

**Pluses**
- I can now use all of the gateway types correctly;

**Minuses**
- Time event can not be good idea for some cases, it can be called in different times;

[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 23 aug 22
**Camunda multi instance**
Finally found how to run multiinstance task with camunda.
The problem was with ValueInfo for java object.

![parallel_multiple](https://user-images.githubusercontent.com/49956820/186442259-c127d4c2-7612-48f3-8566-2d6a47b92fbc.png)

https://github.com/zolotarevandrew/camunda/tree/main/CamundaTests/CamundaTests/Models/SimpleParallelMultiple

[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 10 aug 22
**Camunda multi instance**
A multi-instance activity is executed multiple times - once for each element of a given collection (like a foreach loop in a programming language).

On the execution level, a multi-instance activity has two parts: a multi-instance body and an inner activity. The multi-instance body is the container for all instances of the inner activity.

A multi-instance activity is executed  
- sequentially, the instances are executed one-by-one;
- parallel (default), independently from each other;

A multi-instance activity must have an inputCollection expression.
The output of a multi-instance activity can be collected from the instances by defining the outputCollection.

Completion Condition:
- A completionCondition defines whether the multi-instance body can be completed immediately when the condition is satisfied.It is a boolean expression that will be evaluated each time the instance of the multi-instance body completes. 
Any instances that are still active are terminated and the multi-instance body is completed when the expression evaluates to true.

Element variable:
- To access the current element of the inputCollection value within the instance, the multi-instance activity can define the inputElement variable (e.g. item). The element is stored as a local variable of the instance under the given name.

![image](https://user-images.githubusercontent.com/49956820/187065024-ecc83ff7-57a7-429c-9c98-f70b7b8d660b.png)

**Pluses**
- I can use multi instance tasks for array multiple processing in one external task;

[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 02 aug 22
**Camunda base api and modeling BMPN**
First bmpn should be created.
- Process can start with a key and variables;
- Process can start with a id and variables;

Business Process Model and Notation (BPMN).

A simple flow for BMPN:
- start event;
- Task, the heart of the process, something has to happen with desired outcome (part of activities category, includes sub-processes);
- Intermediate event, represent the status that is reached in process, throwing event;
- End event, throwing event;

User Task
- Generated form
- Generic form
- Embeddeed html forms
- External forms 

Service task
- Expression trigger by specific language
- Connector - http and others
- Called sync
- External called async

Script task
- Task with javascript/groovy

External task
- Independent from camunda
- async
- scalable

Can get and lock task by rest api.
Lock has a timeout. On error lock is released

![image](https://user-images.githubusercontent.com/49956820/187064979-ed608c42-3100-4623-a825-f1dcee28097a.png)

![image](https://user-images.githubusercontent.com/49956820/187064993-da1d1aa0-2aa9-4da4-8e46-223959ebf549.png)


https://github.com/zolotarevandrew/camunda/tree/main/CamundaTests

**Pluses**
- I can use external task as scalable solution for orchestrating business flows;
- I can use user task for managing tasks from any UI;
- I can use camunda events for some useful notifications;

**Minuses**
- Have a lot of using problems with not java clients;

[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 01 aug 22
**Camunda**
Camunda is an open source platform for modeling business processes, built on java.
Development flow:
- flow process in camunda modeler;
- writing code for camunda steps;
- delegate, read something from context, write something to context;

Advantages:
Resume process after error from point of failure - readable trace, number of attempts before failure, custom error handler;
GUI for processes;

Alternative for GUI - rest api;


[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 24 jul 22
**Redis distributed locks**
The simplest way to use Redis to lock a resource is to create a key in an instance.
The key is usually created with a limited time to live, using the Redis expires feature, so that eventually it will get released.

To acquire the lock, the way to go - SET resource_name my_random_value NX PX 30000.
The command will set the key only if it does not already exist (NX option), with an expire of 30000 milliseconds (PX option).
The "lock validity time" is the time we use as the key's time to live.
Basically the random value is used in order to release the lock in a safe way, with a script that tells Redis: remove the key only if it exists and the value stored at the key is exactly the one I expect to be.

How it works - Multiple processes execute the following redis command.
SETNX lock.foo <current Unix time + lock timeout + 1>
- If setnx returns 1, the process obtains the lock, and setnx sets to the timeout time of the lock.
- If setnx returns 0, it means that other processes have acquired the lock and cannot enter the critical area. 
(Processes can continuously try setnx operations in a loop to obtain locks);

Process is disconnected from redis after obtaining a lock - (if there is no effective mechanism to release the lock, other processes will be in a state of waiting, that is, “deadlock”);
After the lock timeout is detected, the process cannot directly and simply perform the Del delete key operation to obtain the lock.

To solve the problem that multiple processes may acquire locks at the same time:
- P1 has acquired lock.foo first, and then process P1 hangs up;
- P4 executes setnx lock.foo to try to acquire the lock;
- P1 has acquired the lock, P4 returns 0 after executing setnx lock.foo, that is, failed to acquire the lock;
- P4 executes get lock.foo to check whether the lock has expired. If not, wait for a period of time and check again;
- If P4 detects that the lock has expired, (the current time is greater than the value of key lock.foo), P4 will perform the GETSET operation;
- Because of the GETSET semantic, P4 can check if the old value stored at key is still an expired timestamp. If it is, the lock was acquired;

In order to make this locking algorithm more robust, a client holding a lock should always check the timeout didn't expire before unlocking the key with DEL.
Because client failures can be complex, not just crashing but also blocking a lot of time against some operations and trying to issue DEL after a lot of time (when the LOCK is already held by another client).


[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 13 jul 22
**Redis pipelining**
Redis pipelining is a technique for improving performance by issuing multiple commands at once without waiting for the response to each individual command.

Redis is a TCP server using the client-server model.
- The client sends a query to the server, and reads from the socket, usually in a blocking way;
- The server processes the command and sends the response back to the client;

While the client sends commands using pipelining, the server will be forced to queue the replies, using memory. It is better to send a lot of commands as batches. The speed will be nearly the same, but the additional memory used will be at most the amount needed to queue the replies for these 10k commands.

Pipelining improves the number of operations you can perform per second in a given Redis server.

StackExchange.Redis - when used concurrently by different callers, it automatically pipelines the separate requests, so regardless of whether the requests use blocking or asynchronous access, the work is all pipelined.

**Pluses**
- I can easily use StackExchange.Redis for pipelining redis requests;


[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 11 jul 22
**Redis pub/sub**
SUBSCRIBE, UNSUBSCRIBE and PUBLISH implement the Publish/Subscribe messaging paradigm. 
Rather, published messages are characterized into channels, without knowledge of what (if any) subscribers there may be. 

Subscribers express interest in one or more channels, and only receive messages that are of interest, without knowledge of what (if any) publishers there are.

A message is an array-reply with three elements:
- subscribe: means that we successfully subscribed to the channel given as the second element in the reply;
- unsubscribe: means that we successfully unsubscribed from the channel given as second element in the reply;
- message: it is a message received as result of a PUBLISH command issued by another client;
- The second element is the name of the originating channel;
- The third argument is the actual message payload;

Pub/Sub has no relation to the key space.
The Redis Pub/Sub implementation supports pattern matching.

A client may receive a single message multiple times if it's subscribed to multiple patterns matching a published message, 
or if it is subscribed to both patterns and channels matching the message.

Only connected subscribers receive messages. Every connected subscriber receives each message.
Once the message is delivered to all current subscribers, it is deleted from the channel.

If a subscriber unsubscribes (disconnects) and later subscribes to a channel again:
- It will not receive any of the intervening messages that it missed while disconnected, and
- It doesn’t know if it’s missed any messages.
- If there are no current subscribers to the channel,  the message will simply be discarded and not delivered to any subscribers.

The delivery semantics are, "at-most-once" per subscriber.
Because the message must be delivered to all current subscribers before being deleted:
- This will take longer with more subscribers.
- This is unlike a radio broadcast, which delivers content currently at the speed of light to every receiver in range.

**Pluses**
- Redis pub/sub can be used in some simple scenarios, where messages can be lost (simple websockets, games, or other little apps);

**Minuses**
- It is better to use RabbitMQ, because a lot of broker features;
- At most once delivery guarantees;
- Internally uses push notifications, can be bad for perfomance;


https://github.com/zolotarevandrew/databases/tree/main/redis/NetRedis/RedisPubSub

[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 06 jul 22
**Redis transactions**
Redis Transactions allow the execution of a group of commands in a single step.
Guarantees:
- All the commands in a transaction are serialized and executed sequentially;
- The EXEC command triggers the execution of all the commands in the transaction, so if a client loses the connection to the server in the context of a transaction before calling the EXEC command none of the operations are performed;

Transaction is entered using the MULTI command. Instead of executing next commands, Redis will queue them.
All the commands are executed once EXEC is called.

Calling DISCARD instead will flush the transaction queue and will exit the transaction.

Errors:
- Starting with Redis 2.6.5, the server will detect an error during the accumulation of commands. It will then refuse to execute the transaction returning an error during EXEC, discarding the transaction;

Rollbacks:
- Redis does not support rollbacks of transactions;

Optimistic locking:
- WATCH is used to provide a check-and-set (CAS) behavior to Redis transactions;
- Watched keys are monitored in order to detect changes against them;
- If at least one watched key is modified before the EXEC command, the whole transaction aborts;

**Pluses**
- I can use watch command to implement distributed optimistic locking;
- I can use redis transactions for some atomic batch operaitons;

**Minuses**
- StackExchange.Redis because of multiplexing uses WATCH instead of raw MULTI/EXEC commands;
- Redis does not support rollbacks of transactions;

https://github.com/zolotarevandrew/databases/tree/main/redis/NetRedis/RedisTransactions

[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 05 jul 22
**Redis persistence**
Persistence options:
- RDB, point-in-time snapshots at specified intervals;
- AOF - append only file, logs every write operation received by the server, that will be played again at server startup, reconstructing the original dataset;
- No persistence;
- RDB + AOF combination;

RDB advantages:
- compact single-file point-in-time representation, perfect for backups;
- disaster recovery, single compact file that can be transferred to far data centers;
- maximizes Redis performances since the only work the Redis parent process needs to do in order to persist is forking a child that will do all the rest;
- faster restarts with big datasets;

RDB disadvantages:
- NOT good if you need to minimize the chance of data loss in case Redis stops working;
- fork() can be time consuming if the dataset is big;

AOF advantages:
- is much more durable: different fsync policies: no fsync at all, fsync every second, fsync at every query;
fsync is performed using a background thread
-  AOF log is an append-only log, so there are no seeks, nor corruption problems if there is a power outage;
- Redis is able to automatically rewrite the AOF in background when it gets too big;
- contains a log of all the operations one after the other in an easy to understand and parse format;

AOF disadvantages:
- AOF files are usually bigger than the equivalent RDB files;
- AOF can be slower than RDB depending on the exact fsync policy;

Snapshotting:
By default Redis saves snapshots of the dataset on disk, in a binary file.
- Redis forks (child process);
- child starts to write the dataset to a temporary RDB file;

Snapshotting is not very durable. 

Log rewriting (log compaction) - while Redis continues appending to the old file, a completely new one is produced with the minimal set of operations needed to create the current data set, and once this second file is ready Redis switches the two and starts appending to the new one.

- redis forks;
- child starts writing the new base AOF in a temporary file;
- parent opens a new increments AOF file to continue writing updates;
- When the child is done rewriting the base file, the parent gets a signal, and uses the newly opened increment file;
- atomic exchange of the manifest files;

Three options, configure how many times Redis will fsync data on disk:
- always, very slow;
- everysec;
- no;

**Pluses**
- I can now choose correct persistence settings for my redis databases;


[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 1 july 22
**Redis streams**
Redis Streams are primarily an append-only data structure.
Being an abstract data type represented in memory, Redis Streams implement powerful operations to overcome the limitations of a log file.

It implements additional, non-mandatory features: a set of blocking operations allowing consumers to wait for new data added to a stream by producers, 
and in addition to that a concept called Consumer Groups.

Every new added entry ID will be monotonically increasing, so in more simple terms, every new entry added will have a higher ID compared to all the past entries.
Because the ID is related to the time the entry is generated, this gives the ability to query for time ranges basically for free.

Reading modes:
- get messages by ranges of time;
- stream of messages that can be partitioned to multiple consumers that are processing such messages;

XREAD - subscribe to new items arriving to the stream.
- A stream can have multiple clients (consumers) waiting for data;
- Can access multiple streams at once;


https://github.com/zolotarevandrew/databases/tree/main/redis/NetRedis/RedisStreams

**Pluses**
- I can now choose correct redis data structure for my purposes;
- I can use redis sets to implement relations model;

**Minuses**
- StackExchange.Redis doesn't support xread blocking command because of multiplexing...;

[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 30 june 22
**Redis sorted sets**
Elements inside sorted sets are not ordered, every element in a sorted set is associated with a floating point value, called the score.

Elements are ordered according to the following rule:
- different scores, A > B if A.score is > B.score;
- same score,  A > B if the A string is lexicographically greater than the B string.

Sorted sets are implemented via a dual-ported data structure containing both a skip list and a hash table, so every time we add an element Redis performs an O(log(N)) operation.

Redis 2.8, a new feature was introduced that allows getting ranges lexicographically.

Sorted sets scores can be updated at any time. 
Just calling ZADD against an element already included in the sorted set will update its score (and position) with O(log(N)) time complexity. 
As such, sorted sets are suitable when there are tons of updates.


Because of this characteristic a common use case is leader boards.

https://github.com/zolotarevandrew/databases/tree/main/redis/NetRedis/RedisSortedSet

**Pluses**
- I can use redis sets and sorted set to implement relations model;
- I can use redis sorted set to some interesting queries for sorted data structures;


[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 29 june 22
**Redis sets**
Redis' responsibility to delete keys when data types are left empty, or to create an empty data type if the key does not exist and we are trying to add elements to it.

Sets are good for expressing relations between objects. For instance we can easily use sets in order to implement tags.
A simple way to model this problem is to have a set for every object we want to tag. 
The set contains the IDs of the tags associated with the object.

https://github.com/zolotarevandrew/databases/tree/main/redis/NetRedis/RedisSet


[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 28 june 22
**Redis lists**
From a very general point of view a List is just a sequence of ordered elements.
Lists are implemented via Linked Lists.
Lists can be taken at constant length in constant time.
When fast access to the middle of a large collection of elements is important, sorted sets can be used.

Common use cases:
- Remember the latest updates posted by users into a social network;
- Communication between processes, using a consumer-producer pattern where the producer pushes items into a list, and a consumer (usually a worker) consumes those items and executed actions;

Capped lists - In many use cases we just want to use lists to store the latest items, whatever they are: social network updates, logs, or anything else.
Redis allows us to use lists as a capped collection, only remembering the latest N items and discarding all the oldest items using the LTRIM command. (Technically O(n)).

Lists have a special feature that make them suitable to implement queues.
Redis implements commands called BRPOP and BLPOP which are versions of RPOP and LPOP able to block if the list is empty.

StackExchange Redis uses multiplexing, which means it only maintains a single connection with the redis server. 
When there are concurrent requests, it will automatically use the pipeline to send each request, and each request needs to wait until the previous request is completed.
For this reason, StackExchange Redis does not provide the corresponding api for BRPOP/BLPOP, because these two operations are likely to block the entire Mulitplexer.

https://github.com/zolotarevandrew/databases/tree/main/redis/NetRedis/RedisList

**Pluses**
- I can implement simple pub/sub queues by using redis lists;

**Minuses**
- StackExchange.Redis does not support BRPOP operations.


[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 27 june 22
**Redis data types**
Strings - binary safe, can contain any kind of data (max value 512mg).

Sets - unordered collection of Strings (desirable property of not allowing repeated members).

Hashes - are maps between string fields and string values.

Sorted Sets - difference with Set is that every member of a Sorted Set is associated with a score, that is used keep the Sorted Set in order (elements in order, fast existence test, fast access to elements in the middle)
- Retreiving data by ranges.

Bitmaps - handle String values like an array of bits; (realtime analytics,Storing space efficient);

HyperLogLogs - probabilistic data structure used in order to count unique things.

Streams - append only collections.

https://github.com/zolotarevandrew/databases/tree/main/redis/NetRedis/RedisDataTypes

[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 22 june 22
**Caching strategies**
Two common approaches

Cache-aside or lazy loading (a reactive approach), cache is updated after the data is requested.
The cache contains only data that the application actually requests, which helps keep the cache size cost-effective;
Problem - Overhead is added to the initial response time because additional roundtrips to the cache and database are needed.

Write-through (a proactive approach), updated immediately when the primary database is updated. 

With both approaches, the application is essentially managing what data is being cached and for how long.
Data will be found in the cache, on next access. Performance of database is optimal because fewer reads are performed.
Problem - infrequently-requested data is also written to the cache;

Concurrency:
Optimistic - application checks if data in the cache has changed since it was retrieved (infrequent updates)
Pessmistic - when retrieveing data, the application locks it in the cache to prevent another instance from changing it;

Key Expiration:
Too short Key expiration will make objects to expires sooner. 
Too long will make you used stale old data producing issues.

**Pluses**
- I can use redis as a distributed cache in my future projects;

**Minuses**
- I can't use client side caching good new mechanics for improving my cache strategies in .NET projects now;

https://github.com/zolotarevandrew/databases/tree/main/redis/NetRedis/RedisCachingAdvanced

[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 21 june 22
**Redis Caching**
two key advantages of client-side caching:
- Data is available with a very small latency;
- database system receives less queries;

Redis client-side caching support is called Tracking, two modes:
- default mode, the server remembers what keys a given client accessed, and sends invalidation messages when the same keys are modified;
- broadcasting mode, clients subscribe to key prefixes such as object: or user:, and receive a notification message every time;

The server remembers the list of clients that may have cached a given key in a single global Invalidation Table.
Inside the invalidation table just store client IDs (each Redis client has a unique numerical ID).

Using the new version of the Redis protocol, RESP3, supported by Redis 6, it is possible to run the data queries and receive the invalidation messages in the same connection.

Broadcasting mode:
- Clients enable client-side caching using the BCAST option;
- Instead of invalidation table, it uses a different Prefixes Table, where each prefix is associated to a list of clients;
- No two prefixes can track overlapping parts of the keyspace (foo, foob);
- Every time a key matching any of the prefixes is modified, all the clients subscribed to that prefix, will receive the invalidation message;

NOLOOP - using this option, clients are able to tell the server they don't want to receive invalidation messages for keys that they modified.

Race conditions:
When implementing client-side caching redirecting the invalidation messages to a different connection, you should be aware that there is a possible race condition.

**Pluses**
- StackExchange.Redis has async methods support, i will use it my new projects.

**Minuses**
- StackExchange.Redis doesnt support client-side caching modifications..

https://github.com/zolotarevandrew/databases/tree/main/redis/NetRedis/RedisCachingAdvanced

[Log Index]
----------------------------------------------------------
---------------------------------------------------------
## 20 june 22
**Redis Introduction**
Redis - in-memory data structure store used as a database, cache, message broker, and streaming engine.
has built-in replication, transactions, and different levels of on-disk persistence.

works with an in-memory dataset.

Can persist your data either by periodically dumping the dataset to disk or by appending each command to a disk-based log.

Redis accepts clients connections on the configured TCP port.
- Socket is put in the non-blocking state;
- TCP_NODELAY is set to ensure that there are no delays;

When Redis can't accept a new client connection because the maximum number clients (max_clients = 10000 by default) it will send an error.

Processing clients - Once new data is read from a client, all the queries contained in the current buffers are processed sequentially.


https://github.com/zolotarevandrew/databases/tree/main/redis/NetRedis/RedisCaching

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 27 may 22
**REST best practices**
REST - Representational State Transfer. software architectural style.

Any API (Application Programming Interface) that follows the REST design principle is said to be RESTful.

Best practices:
- Use JSON as the Format for Sending and Receiving Data;
- Use Nouns Instead of Verbs in Endpoints, Instead, it should be something like this: /posts;
- Name Collections with Plural Nouns. You can think of the data of your API as a collection of different resources from your consumers: /posts/123;
- Use Status Codes in Error Handling;
- Use Nesting on Endpoints to Show Relationships, posts/postId/comments, avoid nesting that is more than 3 levels deep as this can make the API less elegant and readable;
- Use SSL for Security;
- Be Clear with Versioning, https://mysite.com/v1;
- Provide Accurate API Documentation, use swagger;

**Pluses:**
- I can now use correct naming and http error codes in my projects to improve restful api quality;

**Minuses**
- There a lot misconceptions about how to implement REST;

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 26 may 22
**GraphQL Queries**
GraphQL is about asking for specific fields on objects.

**Pluses**
- I can correctly use get/join/mutate options in apps using graphql;

**Minuses**
- If using EntityFramework, some queries can affect perfomance, so every call should be checked and SQL logged;


https://github.com/zolotarevandrew/protocols/tree/main/graphQL/SimpleApi

[Log Index]
----------------------------------------------------------
## 26 may 22
**Kafka consumers**

A consumer group is a set of consumers which cooperate to consume data from some topics. 
The partitions of all the topics are divided among the consumers in the group. 
As new group members arrive and old members leave, the partitions are re-assigned so that each member receives a proportional share of the partitions - rebalancing the group.

The coordinator of each group is chosen from the leaders of the internal offsets topic __consumer_offsets (which is used to store committed offsets).

When the consumer starts up, it finds the coordinator for its group and sends a request to join the group.

Each consumer must send heartbeats to the coordinator. If no hearbeat is received before expiration of the configured session timeout, then the coordinator will kick the member out of the group.

Offsets:

When the group is first created, the position is set according to a configurable offset reset policy (auto.offset.reset).
Consumption starts either at the earliest offset or the latest offset.

The offset commit policy providing the message delivery guarantees. By default, the consumer is configured to use an automatic commit policy, which triggers a commit on a periodic interval.

Processing guarantees:
- At-least-once, records are never lost but may be redelivered. (At-least-once semantics is enabled by default (processing.guarantee="at_least_once);
- Exactly-once, records are processed once. Even if a producer sends a duplicate record, it is written to the broker exactly once (processing.guarantee="exactly_once_v2").
write is not considered successful until it is acknowledged, and a commit is made to “finalize” the write;

By default, the consumer is configured to auto-commit offsets.

When auto-commit disabled and commit uses sync versions, this may reduce overall throughput since the consumer might otherwise be able to process records while that commit is pending.
Solution - the consumer has a configuration setting fetch.min.bytes which controls how much data is returned in each fetch.

The problem with asynchronous commits is dealing with commit ordering. By the time the consumer finds out that a commit has failed, you may already have processed the next batch of messages and even sent the next commit.


if the last commit fails before a rebalance occurs or before the consumer is shut down, then offsets will be reset to the last commit and you will likely see duplicates.

Rebalance:
Each rebalance has two phases: partition revocation and partition assignment.


Configuration:
- session.timeout.ms, default 10s;
- heartbeat.interval.ms, this controls how often the consumer will send heartbeats to the coordinator;
- max.poll.interval.ms, the maximum time allowed time between calls to the consumers poll method, default 300s;
- enable.auto.commit, default true;
- auto.commit.interval.ms, default 5s;
- auto.offset.reset, default latest;


**Pluses**
- i can use kafka consumers to reread my prev messages in my apps (create new app version, or something another);
- I can use kafka consumers as simple message consumers with autocommit enabled in my apps, with at-least-once or exactly-once delivery guarantees;
- I can use kafka consumers to provide at-most-once delivery guarantee with autocommit disabled, and committing before consuming in my apps;

**Minuses**
- I should very careful with, async and sync commits, kafka has a lot login upon it;

https://github.com/zolotarevandrew/kafka/tree/main/Consumers

[Log Index]
----------------------------------------------------------
## 25 may 22
**GraphQL**
A GraphQL service is created by defining types and fields on those types, then providing functions for each field on each type.
Every field and nested object can get its own set of arguments, making GraphQL a complete replacement for making multiple API fetches.

Each GraphQL query passes through three phases: parse, validate and execute.

Schema - endpoint provides a schema used to inform API consumers about the functionality available for clients to consume.

Has three primary operations: 
- Query for reading data;
- Mutation for writing data, used to add, modify, or delete data;
- Subscription for receiving real-time updates, allow a server to send data to its clients, notifying them when events occur.

**Pluses**
- I can use graphql in my apps, which has complex queries, such as mobile apps or other.
- I can use graphql to reduce bandwidth for my future apps;

**Minuses**
- It is difficult to implement caching or rate-limiting;
- A little bit complex;

https://github.com/zolotarevandrew/protocols/tree/main/graphQL/SimpleApi

[Log Index]
----------------------------------------------------------
## 24 may 22
**GRPC streams**
.Net 6 has load balancing mechanism for GRPC and also HTTP3 support was added, but http3 is draft in RFC.

**Pluses**
- I can use streams with async enumerable in .net projects for batch operations;

**Minuses**
- Streams have problems with errors, if there are 2 messages sent and 3 failed, server receive error and stream process will fail;

https://github.com/zolotarevandrew/protocols/tree/main/gRPC/Streams

[Log Index]
----------------------------------------------------------
## 24 may 22
**Kafka topics**
Topics are partitioned, meaning a topic is spread over a number of "buckets" located on different Kafka brokers. This distributed placement of your data is very important for scalability because it allows client applications to both read and write the data from/to many brokers at the same time. When a new event is published to a topic, it is actually appended to one of the topic's partitions. Events with the same event key (e.g., a customer or vehicle ID) are written to the same partition, and Kafka guarantees that any consumer of a given topic-partition will always read that partition's events in exactly the same order as they were written.

Properties:
cleanup.policy - delete or compact, based on log retention;
compression.type - standard compression codecs on uncompressed;
delete.retention.ms - the amount of time to retain delete tombstone markers for log compacted topics. (default 1 day);
file.delete.delay.ms -  time to wait before deleting a file from the filesystem; (default 1min);
flush.messages - calling fsync to prevent os caching, recommended not to use;
index.interval.bytes - how frequently Kafka adds an index entry to its offset index, recommended not to use;
max.message.bytes - the largest record batch size allowed by Kafka;
message.timestamp.difference.max.ms - maximum difference allowed between the timestamp when a broker receives a message and the timestamp specified in the message. CreateTime, a message will be rejected if the difference in timestamp exceeds this threshold. ignored if message.timestamp.type=LogAppendTime;
message.timestamp.type - CreateTime or LogAppendTime;
retention.bytes - the maximum size a partition can grow to before discarding old log segments to free up space;
retention.ms - maximum time retaining a log before discarding old log segments to free up space;
segment.bytes - the segment file size for the log (default 1gb);
segment.index.bytes - size of the index that maps offsets to file positions (default 10mb);
message.downconversion.enable - down-conversion of message formats is enabled to satisfy consume request (broker will not perform down-conversion for consumers expecting an older message format. The broker responds with UNSUPPORTED_VERSION error for consume requests from such older clients);


**Pluses**
- I can use correct properties to create topic by my apps requirements;
- I can use topics to scale my apps correctly using partitions;
- I can use topics to increase my apps reliability;

**Minuses**
- I think i will not use, most of all topic properties in my apps;

https://github.com/zolotarevandrew/kafka/tree/main/Topics

[Log Index]
----------------------------------------------------------
## 23 may 22
**GRPC Introduction**
gRPC is a language agnostic, high-performance Remote Procedure Call (RPC) framework.
main benefits:
- Contract-first API development, using Protocol Buffers by default, allowing for language agnostic implementations.
- Supports client, server, and bi-directional streaming calls.
- Reduced network usage with Protobuf binary serialization.

By default, uses Protocol Buffers, open source mechanism for serializing structured data.

Service definitions:
- Unary RPCs where the client sends a single request to the server and gets a single response back;
- Server streaming RPCs where the client sends a request to the server and gets a stream to read a sequence of messages back (gRPC guarantees message ordering within an individual RPC call);
- Client streaming RPCs where the client writes a sequence of messages and sends them to the server, again using a provided stream;
- Bidirectional streaming RPCs where both sides send a sequence of messages using a read-write stream;

Unary RPC:
- client calling stub;
- server is notified that the RPC has been invoked with the client’s metadata;
- server can send back its initial metadata, or wait for the client’s request message;
- Once the server has the client’s request message, it does whatever work is necessary to create and populate a response;

Timeouts - DEADLINE_EXCEEDED property;

In gRPC, both the client and server make independent and local determinations of the success of the call, and their conclusions may not match.

**Minuses**
- I should always copy or create library to share protobuf contracts for correct sending and receiving;

https://github.com/zolotarevandrew/protocols/tree/main/gRPC/Simple

[Log Index]
----------------------------------------------------------
## 18 may 22
**QUIC**
A UDP-Based Multiplexed and Secure Transport.

Integrates the TLS handshake [TLS13], although using a customized framing for protecting packets, structured to permit the exchange of application data as soon as possible.

Authenticates the entirety of each packet and encrypts as much of each packet as is practical.
QUIC packets are carried in UDP datagrams.

Application protocols exchange information over a QUIC connection via streams, which are ordered sequences of bytes.

Two types of streams can be created: 
- bidirectional streams, which allow both endpoints to send data;
- unidirectional streams, which allow a single endpoint to send data.  

Flow control:
To enable a receiver to limit memory commitments for a connection, streams are flow controlled both individually and across a connection as a whole.

Data flow control:
- Stream flow control, limiting data in each stream;
- Connection flow control, limiting data in all streams;

An endpoint limits the cumulative number of incoming streams a peer can open.

A QUIC connection is shared state between a client and a server.
Each connection starts with a handshake phase, during which the two endpoints establish a shared secret using the cryptographic handshake protocol.

QUIC relies on a combined cryptographic and transport handshake to minimize connection establishment latency.
Uses the CRYPTO frame to transmit the cryptographic handshake.



https://datatracker.ietf.org/doc/rfc9000/

[Log Index]
----------------------------------------------------------
## 17 may 22
**IP**
The internet protocol provides for transmitting blocks of data called datagrams from sources to
destinations, where sources and destinations are hosts identified by fixed length addresses.

Implements two basic functions: addressing and fragmentation.

Uses four key mechanisms in providing its service:  
Type of Service - quality of the service desired. To select the actual transmission parameters for a particular network.
Time to Live - set by the sender of the datagram and reduced at the points along the route where it is processed.
Options - provisions for timestamps, security, and special routing.
Header Checksum - verification that the information used in processing internet datagram has been transmitted correctly.

Errors detected may be reported via the Internet Control Message Protocol (ICMP).

Transmitting:
- Prepare data;
- Prepare datagram header and attach data to it;
- Local network interface creates a local network header, and send datagram to local network;
- Local network strips this header and adding from address;
- Sending to destination host;


The internet module maps internet addresses to local net addresses.
An address begins with a network number, followed by local address (called the "rest" field).

There are three formats or classes of internet
- class a, the high order bit is zero, the next 7 bits are the network, and the last 24 bits are the local address; 
- class b, the high order two bits are one-zero, the next 14 bits are the network and the last 16 bits are the local address;
- class c, the high order three bits are one-one-zero, the next 21 bits are the network and the last 8 bits are the local address.

Some hosts will also have several physical interfaces (multi-homing).

Fragmentation:
The receiver of the fragments uses the identification field to ensure that fragments of different datagrams
are not mixed.  The fragment offset field tells the receiver the position of a fragment in the original datagram.  The fragment offset and length determine the portion of the original datagram covered by this fragment.

To fragment a long internet datagram it creates two new internet datagrams and copies the contents of the internet header fields from the long datagram into both new internet headers.

Header format:
- Version 4 bits, format version 4;
- IHL 4 bits - header length;
- Type of service 8 bits - service quality desired;
- Total length 16 bits - in octets, recommended 576 octets, to remove fragmentation;
- Identification 16 bits - assembling fragments;
- Flags 3 bit - don't fragment, fragment;
- TTL 8 bits - every receive decreases by one.
- Header checksum - checksum;
- Source address 32 bits;
- Dest address 32 bits;

https://datatracker.ietf.org/doc/html/rfc791

[Log Index]
----------------------------------------------------------
## 16 may 22
**UDP**
UDP  is  defined  to  make  available  a datagram   mode  of  packet-switched   computer   communication  in  the environment  of  an  interconnected  set  of  computer  networks.
Provides  a procedure  for application  programs  to send messages  to other programs  with a minimum  of protocol mechanism.
Protocol  is transaction oriented, and delivery and duplicate protection are not guaranteed.

Header Format:
- source port - optional field. If not used, a value of zero is inserted;
- destination port - context of particular dest address;
- length - length in octets including header and data;
- checksum - 16 bit sum of a pseudo header of information from the IP header, the UDP header, and the
data;

The pseudo  header  conceptually prefixed to the UDP header contains the source  address,  the destination  address,  the protocol,  and the  UDP length.
- source address;
- dest address;
- protocol;
- udp length;


In .NET 5, the runtime added the concept of a Pinned Object Heap (POH) which is an area designed to hold buffers intended for native IO operations; it was initially needed for improving the performance of sockets in ASP.NET Core HTTP request handling.


https://github.com/zolotarevandrew/protocols/tree/main/Udp

https://datatracker.ietf.org/doc/html/rfc768
https://enclave.io/high-performance-udp-sockets-net6/

[Log Index]
----------------------------------------------------------
## 13 may 22
**HTTP 3**
HTTP/3 runs over QUIC – an encrypted general-purpose transport protocol that multiplexes multiple streams of data on a single connection.

QUIC - The protocol utilizes space congestion control over User Datagram Protocol (UDP).

QUIC will help fix some of HTTP/2's biggest shortcomings:
- Developing a workaround for the sluggish performance when a smartphone switches from WiFi to cellular data;
- Decreasing the effects of packet loss — when one packet of information does not make it to its destination, it will no longer block all streams of information (“head-of-line blocking”);

Benefits:
- Faster connection establishment: QUIC allows TLS version negotiation to happen at the same time as the cryptographic and transport handshakes;
- Zero round-trip time, for servers they have already connected to, clients can skip the handshake requirement;
- More comprehensive encryption: QUIC’s new approach to handshakes will provide encryption by default — a huge upgrade from HTTP/2 — and will help mitigate the risk of attacks;


[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 12 may 22
**HTTP 2**
- Multiple requests (html,css,js) in one tcp connection.
- compresses a lot of unnecessary header frames by HPACK.

Binary protocol:
Binary commands will be more difficult to read than subsequent text commands, but the network will easily generate them and parse frames.
- less network load;
- reduced network latency and increase throughput

Changes how the data is exchanged between the client and server.
- Stream: A bidirectional flow of bytes within an established connection, which may carry one or more messages.
- Message: A complete sequence of frames that map to a logical request or response message.
- Frame: The smallest unit of communication, containing a frame header.

Each stream has a unique identifier and optional priority information that is used to carry bidirectional messages.

Each message is a logical HTTP message, such as a request, or response, which consists of one or more frames.

Allows each stream to have an associated weight and dependency:
- Each stream may be assigned an integer weight between 1 and 256.
- Each stream may be given an explicit dependency on another stream.

The combination of stream dependencies and weights allows the client to construct and communicate a "prioritization tree" that expresses how it would prefer to receive responses.

Provides a set of simple building blocks that allow the client and server to implement their own stream- and connection-level flow control.
- Flow control is directional. Each receiver may choose to set any window size that it desires for each stream and the entire connection;
- Flow control is credit-based. Each receiver advertises its initial connection and stream flow control window;
- Flow control cannot be disabled;

Server push - ability of the server to send multiple responses for a single client request.

*Pluses*
- I can switch to HTTP2 to reduce number of TCP connection, and reduce latency.

*Minuses*
- There are the head of blocking problem with HTTP2;

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 11 may 22
**HTTP 1**
HTTP is an application layer protocol designed within the framework of the Internet protocol suite. Its definition presumes an underlying and reliable transport layer protocol.

HTTP resources are identified and located on the network by Uniform Resource Locators (URLs), using the Uniform Resource Identifiers (URI's).

In HTTP/1.0 a separate connection to the same server is made for every resource request.
In HTTP/1.1 instead a TCP connection can be reused to make multiple resource requests.

Persistent connections:

HTTP/1.1 has keep-alive-mechanism, so that a connection could be reused for more than one request/response. Such persistent connections reduce request latency, because the client does not need to re-negotiate the TCP handshake. TCP can put multiple requests (and responses to requests) int one TCP segment;

Since requests are pipelined, TCP segments are more efficient. The overall result is less Internet traffic and faster performance for the user

HTTP1.1 introduces the OPTIONS method, way for a client to learn about the capabilities of a server without actually requesting a resource.

Сaching:
A cache entry is fresh until it reaches its expiration time, at which point it becomes stale.
Uses the more general concept of an opaque cache validator string, known as an entity tag.

The most basic is If-None-Match, which allows a client to present one or more entity tags from its cache entries for a resource. If none of these matches the resource’s current entity tag value, the server returns a normal response; otherwise, it may return a 304 (Not Modified) response with an ETag header that indicates which cache entry is currently valid.

Adds the new Cache-Control header, uses relative expiration times, via the max-age directive.

Range requests allow a client to request portions of a resource. A client makes a range
request by including the Range header in its request, specifying one or more contiguous ranges of bytes.

Includes a new status code, 100 (Continue), to inform the client that the request body should be transmitted. When this mechanism is used, the client first sends its request headers, then waits for a response.

Resolves the problem of delimiting message bodies by introducing the Chunked transfer-coding. The sender breaks the message body into chunks of arbitrary length, and each chunk is sent with its length prepended; it marks the end of the message with a zero-length chunk. The sender uses the Transfer-Encoding: chunked header to signal the use of chunking.

*Pluses*
- I can use http1.1 caching correctly in my next projects.
- I can use http1.1 keep alive mechanism correctly to decrease latency in my next projects.

https://www.ra.ethz.ch/cdstore/www8/data/2136/pdf/pd1.pdf


[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 09-10 may 22
**TCP**
TCP provides reliable, ordered, and error-checked delivery of a stream of octets (bytes) between applications running on hosts communicating via an IP network.

Three-way handshake (active open), retransmission, and error detection adds to reliability but lengthens latency.

TCP achieves reliability using a technique known as positive acknowledgement with re-transmission. 
This requires the receiver to respond with an acknowledgement message as it receives the data. 
The sender keeps a record of each packet it sends and maintains a timer from when the packet was sent. 
The sender re-transmits a packet if the timer expires before receiving the acknowledgement. 
The timer is needed in case a packet gets lost or corrupted.

TCP protocol operations may be divided into three phases
1) Connection establishment is a multi-step handshake process
2) data transfer phase
3) connection termination closes the connection and releases all allocated resources

- Before a client attempts to connect with a server, the server must first bind to and listen at a port to open it up for connections: this is called a passive open.
- Connection termination, client transmits a FIN and ACK. After the side that sent the first FIN has responded with the final ACK, it waits for a timeout before finally closing the connection, 
during which time the local port is unavailable for new connections.

Resources:

Whenever a packet is received, the TCP must perform a lookup on table to find the destination process. 
Each entry in the table is known as a Transmission Control Block or TCB. 
It contains information about the endpoints (IP and port), status of the connection, running data about the packets that are being exchanged and buffers for sending and receiving data.

Reliable transmission:
TCP uses a sequence number to identify each byte of data.
Reliability is achieved by the sender detecting lost data and retransmitting it.

Dupack-based retransmission:
Hence the receiver acknowledges packet again on the receipt of another data packet. This duplicate acknowledgement is used as a signal for packet loss. If the sender receives three duplicate acknowledgements, it retransmits the last unacknowledged packet.

Timeout-based retransmission:
When a sender transmits a segment, it initializes a timer with a conservative estimate of the arrival time of the acknowledgement. The segment is retransmitted if the timer expires, with a new timeout threshold of twice the previous value, resulting in exponential backoff behavior.

To assure correctness a checksum field is included.

Flow control:

TCP uses a sliding window flow control protocol. In each TCP segment, the receiver specifies in the receive window field the amount of additionally received data (in bytes) that it is willing to buffer for the connection. The sending host can send only up to that amount of data before it must wait for an acknowledgement and receive window update from the receiving host.

When a receiver advertises a window size of 0, the sender stops sending data and starts its persist timer.

Congestion control:

Acknowledgments for data sent, or lack of acknowledgments, are used by senders to infer network conditions between the TCP sender and receiver. Coupled with timers, TCP senders and receivers can alter the behavior of the flow of data.

Maximum segment size:

For best performance, the MSS should be set small enough to avoid IP fragmentation, which can lead to packet loss and excessive retransmissions.


[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 05 may 22
**Kafka introduction**
Event streaming is capturing data in realtime from databases and other resources in the form of stream of events, for later retrieval, routing to 
different sources as needed.

kafka using tcp Connections for server and client.

Servers:
Cluster one more servers. Some servers forms the storage layer - brokers. Other server runs kafka connect to  import export data to integrate with other systems.

Events are durably stored in topics.
Topic can have zero or more producers and consumers. Events are not deleted after consumption.

Topics are partitioned, spread over number Of buckets located in different brokers.

Use cases:
- messaging, better than traditional rabbit, throughput, partitioning, replication, fault tolerance.
- Website activity tracking - original use case
- Metrics
- Log aggregation
- Stream processing
- Commit log

*Pluses*
- I can use kafka for projects which needed realtime processing, because it is faster than RabbitMq.

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 04 may 22
**POP3/IMAP**
Popular email providers supports both protocols.

POP - receive emails from a remote server and send to a local client. "store-and-forward" service.
Once the email is on the client, POP3 then deletes it from the server.
Users or an administrator can specify that mail be saved for some time, allowing users to download email as many times as they wish within the specified period.

Port 110 - non encrypted connection.
Port 995 - encrypted connection.

The server starts POP3 service by listening on TCP port 110. 
When a client wishes to use POP3 for email retrieval, it establishes a TCP connection with the server host. Once this connection is established, the POP3 server sends a greeting. At this point, the session enters the authorization state.

When the client issues the quit command, the session enters the update state. The POP3 server releases any resources acquired during the transaction state, and says "goodbye," which is when the TCP connection is closed. After the POP3 session enters the update state, the POP3 server deletes the message.

Pluses
- POP3 useful then users need to accesc ther email offline, and for sending and storing bulk email messages;

Minuses
- POP3 not intent to support sync with server.

IMAP - stores email messages on a mail server and enables the recipient to view and manipulate them as thoug they were stored locally on their device(s).

Enables users to organize messages into folders, flag messages for urgency or follow-up, and save draft messages on the server. Users can also have multiple email client applications that sync with the email server to consistently show which messages have been read or are still unread.

- User sign in to email client, client contact server using IMAP;
- TCP connection is made;
- The headers of all emails are displayed by the email client;
- IMAP only downloads a message to the client when the user clicks on it; attachments are not automatically downloaded.
- Email messages remain on the server unless the user explicitly deletes them;

Port 143 - non encrypted connection.
Port 993 - encrypted connection.

With POP3, email is saved for users in a single mailbox on the server. It is moved from the server to their device when the mail client opens.

Pluses
- emails accessible from multiple devices
- a single mailbox can be shared by multiple users
- users can organize emails on the server by creating folders and subfolders


[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 04 may 22
**SMTP/ESMTP**
E-Mailing system has three main parts:
- user agents (browsers, mobile devices and other)
- email servers
- SMTP protocol

User agents - read, answer, resend, send emails.
When user creates new email, agent will send it to email server (queue).

SMTP uses TEXT commands, which is not encrypted.
SMTP requires 7-bit ASCII symbols for every message header. 

Because of SMTP message can contain video images and other data.
Content-Type and Content-Tranfer-Encoding headers have to presented.

First SMTP client creates TCP connection by port 25,587 of email server. 
When client and server do handshake.

Agent delivers message to a transfer agent - TA. 
TA uses DNS to look up the MX (mail exchanger) record for the recipient's domain.
Then it selects a recipient server and connects to it to complete the mail exchange.

Once delivered to the local mail server, the mail is stored for batch retrieval by authenticated mail clients.

SMTP AUTH extension
Usually, servers reject RCPT TO commands that imply relaying unless authentication credentials have been accepted.

Encryption:
Port 587 is often used to encrypt SMTP messages using STARTTLS, which allows the email client to establish secure connections by requesting that the mail server upgrade the connection through TLS.



[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 03 may 22
**DNS**
Dns - hierarhical  distributed database and also application protocol for interaction between hosts and servers

Dns uses 53 port and work over UDP.
Http ftp and other protocol uses dns to get ip addresses.

Functions:
- Host aliases by its canonical name (eu1.host.eu - host.eu)
- Mail aliases 
- Load balancing, if web has replicas with different ip addresses, dns can contain list of ip addresses

Why not centralized?
- Single Point of failure
- Traffic volume
- Remoteness
- Service

Local dns servers - for every internet provider
If provider owns searching host it will give fast answer

Root dns servers - base dns servers about 10 worldwide.

If local dns server cant find host, it sends request to root server and becoming a client. If root server cant find host it sends address of Plenipotentiary server.

Plenipotentiary dns server - server where host has been registered.

Dns requests can be iterative of recursive.

Caching - servers  caches dns Responses

Dns servers store resource records
- Name
- Value
- Type
- TTL
Type A - Name host name, Value ip address
Type Ns - name domain, value host name or dns server
Type CNAME - value canonical name, name alias
Type MX - value canonical name, name alias

*Pluses*
- I can use correct DNS records and its properties in my DNS providers later.

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 02 may 22
**FTP**
ftp has control TCP connection and data TCP connections.
Uses authentication based on text.

Passive mode:
- Then client is behind firewall, client uses the control connection to send PASV command to server, and then receives server port and ip address to establish connection.

Active mode:
- clients starts listening for incoming data connections from the server.

FTPS -e xtension to FTP, that adds support TLS.

Implicit: 
- Negotiation is not supported with implicit FTPS configurations. A client is immediately expected to challenge the FTPS server with a TLS ClientHello message. If such a message is not received by the FTPS server, the server should drop the connection.

Explicit:
- FTPS client must "explicitly request" security from an FTPS server and then step up to a mutually agreed encryption method. If a client does not request security, the FTPS server can either allow the client to continue in insecure mode or refuse the connection.

**Pluses**
- I can use FTPS server in my simple case scenarios.

**Minuses**
- FTP deprecated in latest browser versions.
- FTP not secure;

Tried ftps by local IIS - https://www.pcwdld.com/install-secure-ftp-server-using-iis
and filezilla client.

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 27 apr 22
**Rabbitmq Streams**
Streams cover 4 use-cases that queue types can not provide:
- Large fan-outs
Users have to bind a dedicated queue for each consumer. If the number of consumers is large this becomes potentially inefficient. Streams will allow any number of consumers to consume the same messages from the same queue in a non-destructive manner. Stream consumers will also be able to read from replicas allowing read load to be spread across the cluster.

- Replay, time-travelling
Streams will allow consumers to attach at any point in the log and read from there.

- Throughput Performance
No persistent queue types are able to deliver throughput that can compete with any of the existing log based messaging systems. Streams have been designed with performance as a major goal.

- Large logs
Streams are designed to store larger amounts of data in an efficient manner with minimal in-memory overhead.

Any consumer can start reading/consuming from any point in the log.

As streams persist all data to disks before doing anything it is recommended to use the fastest disks possible.

*Minuses*
- .NET has a raw library for it;
- Kafka can be a better solution with distributed log type such as rabbit streams;


[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 27 apr 22
**Rabbitmq Priority Queues**
RabbitMq has priority queues, by using x-max-priority optional queue argument.
By default, consumers may be sent a large number of messages before they acknowledge any, limited only by network backpressure.

*Pluses*
- I can use message priorities and priority queues for some load-balancing tasks or somethis that has priorities.

*Minuses*
- There is bad documentation for consumer priorities, i need to test it more.

https://github.com/zolotarevandrew/rabbitmq/tree/main/priority-consumers
https://github.com/zolotarevandrew/rabbitmq/tree/main/priority-queues

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 26 apr 22
**Rabbitmq Consumers**
Сonsumer is a subscription for message delivery that has to be registered before deliveries begin and can be cancelled by the application.
A successful subscription operation returns a subscription identifier (consumer tag). It can later be used to cancel the consumer.

When registering a consumer applications can choose one of two delivery modes:
- Automatic
- Manual

With manual acknowledgement mode consumers have a way of limiting how many deliveries can be "in flight" by using prefetch count.

The exclusive flag can be set to true to request the consumer to be the only one on the target queue.
Consuming with only one consumer is useful when messages must be consumed and processed in the same order they arrive in the queue.

Single active consumer "x-single-active-consumer", allows to have only one consumer at a time consuming from a queue and to fail over to another registered consumer in case the active one is cancelled or dies.

Priorities - when consumer priorities are in use, messages are delivered round-robin if multiple active consumers exist with the same high priority.

.NET clients guarantee that deliveries on a single channel will be dispatched in the same order there were received regardless of the degree of concurrency. 

*Pluses*
- I can use exclusive consumers for web sockets or other transient connections in my projects;
- I can use single active consumer to support correct message ordering;

*Minuses*
- I think i will not use automatic acks, because of message loss possibility.


https://github.com/zolotarevandrew/rabbitmq/tree/main/consumers

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 26 apr 22
**Rabbitmq WorkQueues**
By default, RabbitMQ will send each message to the next consumer, in sequence. 
On average every consumer will get the same number of messages. (Round robin).

Interesting note:
"Marking messages as persistent doesn't fully guarantee that a message won't be lost. Although it tells RabbitMQ to save the message to disk, there is still a short time window when RabbitMQ has accepted a message and hasn't saved it yet. Also, RabbitMQ doesn't do fsync(2) for every message -- it may be just saved to cache and not really written to the disk. The persistence guarantees aren't strong, but it's more than enough for our simple task queue. If you need a stronger guarantee then you can use publisher confirms."

RabbitMQ dispatches a message when the message enters the queue. It doesn't look at the number of unacknowledged messages for a consumer. It just blindly dispatches every n-th message to the n-th consumer. To defeat this problem it is better to use prefetch_count, so worker will process only one message at time.

*Pluses*
- I can use work queues in some simple scenarios in projects;

*Minuses*
- There can be situations, then two workers can receive and save same message.

https://github.com/zolotarevandrew/rabbitmq/tree/main/work-queues

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 25 apr 22
**Rabbitmq Routing**
Bindings can take an extra routing_key parameter.
The fanout exchanges, simply ignored its value.

Routing keys are on messages, so the producer has a control by sending message by using correct routing key.

Exchanges compare a messages routing key to each route's binding key to determine if the message should be sent to the queue on that route.

https://github.com/zolotarevandrew/rabbitmq/tree/main/routing/DirectLogging

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 1 apr 22
**Rabbitmq Bindings**
Create binding parameters:
- queue name
- exchange name
- routing key
- arguments

There can be exchange to exchange bindings, exchange to queue bindings.

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 31 mar 22
**Rabbitmq Topics**
Internally uses trie.
Partially matching routing key.

Can be "#" - one or more words
Can be "*" - one word, faster than "#".

*Pluses*
- I can use topics for complex pub/sub scenarions;

https://github.com/zolotarevandrew/rabbitmq/tree/main/exchanges/Topic

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 30 mar 22
**Rabbitmq Queues**
Queues in RabbitMq FIFO.

Queue properties:
- Name, can max be 255 bytes
- Durable (survive on restart)
- Exclusive (will be deleted when that connection closes)
- Auto-delete (deleted when last consumer unsubscribes)


Before a queue can be used it has to be declared. 
Declaring a queue will cause it to be created if it does not already exist.

Optional arguments:
- length limitat
- mirroring settings
- max number of priorities
- consumer priorities

Ordering can be affected by the presence of multiple competing consumers, consumer priorities, message redeliveries.

Initial deliveries - redelivered(false), single consumer can process in FIFO order.
Repeated delivery - ordering can be affected by the timing of consumer acknowledgements and redeliveries.

If all of the consumers have equal priorities, they will be picked on a round-robin basis.

Durable queues will be recovered on node boot, including messages in them published as persistent. 
Messages published as transient will be discarded during recovery, even if they were stored in durable queues.

Throughput and latency of a queue is not affected by whether a queue is durable or not in most cases.
The choice between durable and transient queues comes down to the semantics of the use case.

Temporary queues:
- transient clients
- temporary WebSocket connections

Deleting temp queue automatically:
- Exclusive queues (declaring connection is closed or gone (e.g. due to underlying TCP connection loss))
(x-queue-master-locator="client-local" when declaring the queue)
- TTL 
- Auto delete queue (deleted when its last consumer is cancelled)


Queues can have their length limited. Queues and messages can have a TTL.
Queues keep messages in RAM and/or on disk.
Publishing messages as transient suggests that RabbitMQ should keep as many messages as possible in RAM.

Queues can have priorities from 1 to 10.
Publishers specify message priority using the priority field in message properties.

Delivered messages can be acknowledged by consumer explicitly or automatically as soon as a delivery is written to connection socket.

High number of unacknowledged messages will lead to higher memory usage by the broker.
Automatic acknowledgement can be problem if consumer process can't process a lot of messages.
Consumers using higher (several thousands or more) prefetch levels can experience the same overload problem.

*Pluses*
- I can configure properly queue settings for my use cases;

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 29 mar 22
**Rabbitmq Exchanges**
Direct - sends message to concrete queue by routing key.
Topic - sends message to concrete queue by routing key template.
Fanout - send message to all queues.
Headers - send message by header parameters;

*Pluses*
- I can choose the suitable exchange type in rabbitmq for my future tasks;

https://github.com/zolotarevandrew/rabbitmq/tree/main/exchanges/Exchanges
[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 28 mar 22
**Rabbitmq AMQP**
AMQP - Advanced message queueing protocol.
It is a binary application layer protocol. 

Provides Deliver guarantees:
- at-most-once (where each message is delivered once or never);
- at-least-once (where each message is certain to be delivered, but may do so multiple times);
- exactly-once (where the message will always certainly arrive and do so only once).

Support authentication and encryption. Based on TCP.

AMQP defines a self-describing encoding scheme, allowing representation of a wide range of commonly used type.

The basic unit of data in AMQP is a frame.
There are 9 AMQP frame bodies:
- open (the connection)
- begin (the session)
- attach (the link)
- transfer
- flow
- disposition
- detach (the link)
- end (the session)
- close (the connection)

link - heart of amqp.
attach - initiate new link, detach - detach new link.
links may be established in order to receive or send messages.
messages sent over a established link using the transfer frame.

each transferred message must eventually be settled. 
settlement ensures that the sender and receiver agree on the state of the transfer, providing reliability guarantees.

Ensuring the message as sent by the application is immutable allows for end-to-end message signing and/or encryption and ensures that any integrity checks (e.g. hashes or digests) remain valid.


Messages are published to exchanges (postoffice, mailbox).

Exchanges distritbute copies to queues using bindings (rules).
Then broker deliver messages to consumers subscribed to queues, or consumers pull messages from queues on demand.

Publisher can specify message metadata.
Application can fail process messages, so message acknowledgements are used.

AMQP programmable protocol, entities routing schemes defines by application, not a broker.

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 22 mar 22
**Postgres partitioning**

Types:
- By date;
- By id ranges;
- By first char of name;

By date and By Id:
- Easy to understand;
- Stable number of rows;
- Need to add partitions sometimes;
- Can be table scans if query without date or id;

Partition is separate table, it can't have primary key and be a foreign key.

**Pluses**
- I can create postgres partitions to increate my apps db queries perfomance;

**Minuses**
- In some cases, it is better to optimize db queries, rather than make partitions.
- It is need to add new partitions, based on partition types (can be leverared by DDL scripts)

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 21 mar 22
**Postgres base performance problems, replication**
Metrics:
- High cpu usage >50%;
- Transactions count > 20-30 thousands;
- I/O usage;
- Exclusive Locks count;
- Long running transactions;

Solutions:
- write only needed things;
- read only needed data;
- split writing information;
- removing distinct by joins;
- a lot of aggregations, count, sum and other remove it;
- create different users for different purposes, analytics and other.
- partial indexes;

Replication:
- First we creating async replica and Moving aggregations to them.
- Second we createing async lagging replica for long running operations;
- Third creating sync/async replica with low latency (network should be good);

Sync replica problems:
- vacuum problems on master;
- table index bloat;

Streaming replication - by tcp connection (some transactions can be lost);
- async by it manner;

Cascade replication - by other secondaries replication.
Sync replication - wal should be writed on primary and secondary.
Because of latency, can be problems with locks.


**Pluses**
- I can choose correct replica type, based on datacenters location and other factors;
- I can rely on many new metrics to increase my databases perfomance;


[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 18 mar 22
**MongoDB time series collections**
Time series collections efficiently store sequences of measurements over a period of time.
Contains:
- timeField, timestamp for document;
- metaField, should rarely changed;
- granularity, seconds by default, can be hours, minutes;
- expireAfterSeconds, ttl, automatic deletion of documents;

MongoDB treats time series collections as writable non-materialized views on internal collections that automatically organize time series data into an optimized storage format on insert.

The implementation of time series collections uses internal collections that reduce disk usage and improve query efficiency. Time series collections automatically order and index data by time.

https://github.com/zolotarevandrew/databases/blob/main/mongodb/timeSeries.js

**Pluses**
- I can use timeseries collection for some metrics, if i can't use grafana or influxdb or something;


[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 17 mar 22
**MongoDB sharding**
Mongodb doesn't have a partition mechanism such as relation databases.
Sharded cluster:
- shard - subset of sharded data, each shard can be a replica set.
- mongos - query router, can use hedged-reads (to get first response from multiple replicas)
- config servers - cluster settings

High Availability:
If one ore more shards replicas becomes unavailable, other shard can process requests.

Once a collection has been sharded, MongoDB provides no method to unshard a sharded collection.

Unsharded collections are stored on a primary shard. Each database has its own primary shard.

*Hashed sharding*
range-based queries can target more than one shard.

*Range based sharding*
Poorly considered shard keys can result in uneven distribution of data, which can negate some benefits of sharding or can cause performance bottlenecks.

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 16 mar 22
**MongoDB replication secondaries, oplog, data sync**
Secondaries can be:
- Priority 0, Prevent it from becoming a primary in an election, reside secondary or a cold standby.
- Hidden, prevent reasing from applications;
- Delayed, historical snapshot.

Priority 0:
Might be desired if the particular member is deployed in a data center that is distant from the main deployment and therefore has higher latency.

In some replica sets, it might not be possible to add a new member in a reasonable amount of time. 
A standby member keeps a current copy of the data to be able to replace an unavailable member.

Hidden:
Can vote in elections. Useful for dedicated tasks such as reporting and backups.

Delayed:
Applied operations with delay.

Arbiter:
Vote only secondary, has no data, useful just for voting.

*Oplog*
The oplog (operations log) is a special capped collection that keeps a rolling record of all operations that modify the data stored in your databases.
Retention period can be specified.
Each operation in the oplog is idempotent.

When large oplog needed:
- Updates to multiple documents at once, oplog translate multi-updates into individual operations.
- A lot of deletions and inserts;
- Significant portion of the workload is updates;

Slow oplog operations in secondaries can be found in logs.

**Pluses**
- I can choose correct replica types in mongodb to increase my app availability and resiliency;
- I can use correct combination of replicas, such as arbiters, hidden, delayed secondaries if bussiness really needs it.
- I can change oplog size then a lot of write or delete operations perfomed on collection;


**Minuses**
- I had to find slow connection creations and oplog operations in mongodb logs;

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 14 mar 22
**MongoDB replication**
A replica set in MongoDB is a group of mongod processes that maintain the same data set.
Replica sets provide redundancy and high availability.
Replication provides a level of fault tolerance against the loss of a single database server.

A replica set contains several data bearing nodes and optionally one arbiter node.
Of the data bearing nodes, one member is deemed the primary node, while the other secondary nodes.

Primary receives all write operations. Records all changes to its data sets in its operation log, i.e. oplog.
Secondaries replicate the primary's oplog and apply the operations to their data sets asynchronously.

Replication lags - amount of time that it takes to copy (i.e. replicate) a write operation on the primary to a secondary.

Failover:
When a primary does not communicate with the other members of the set for more than the configured electionTimeoutMillis period (10 seconds by default), an eligible secondary calls for an election to nominate itself as the new primary.

Read preference - by default clients read from the primary.

Multi-document transactions that contain read operations must use read preference primary. All operations in a given transaction must route to the same member.
Depending on the read concern, clients can see the results of writes before the writes are durable.

Mirrored Reads reduce the impact of primary elections following an outage or planned maintenance.

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 10 mar 22
**MongoDB connection pooling**
MongoClient provides connection pooling.
MongoClient instance per application should be used, unless the application is connecting to many separate clusters.

Tuning:
- High CPU Usage, reduce maxPoolSize;
- load small, increase maxPoolSize;
- slow connection creation, change minPoolSize, to increase connections created in before startup;

Slow connection creation, should found from logs;

Database profiler.
The profiler writes all the data it collects to a system.profile collection, a capped collection in each profiled database. Than means in has fixed amount documents by the time.

When enabled, profiling has an effect on database performance. Profiling also consumes disk space, as it logs to both the system.profile collection and also the logfile.

I think it is better to have logging metrics by some queries and commands than enable this feature.

**Pluses**
- I can profile database queries and hot paths by using database profiler sometimes;
- I can use correctly mongodbdriver knowing the fact of connection pooling is inside driver;

**Minuses**
- I should really think about using profiling in production, because it is can affect perfomance;
- It is hard to find connection startup info inside mongodb logs, to profile connection creation;


[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 10 mar 22
**MongoDB explain**
The explain results present the query plans as a tree of stages.
Each stage passes its results to the parent node.
Stages:
- COLLSCAN collection scan;
- IXSCAN scan index;
- FETCH retrieve documents;
- SHARD_MERGE merging results from shards;
- SHARDING_FILTER filtering orphan documents from shards.

When an index covers a query, mongo can return results using only index keys.
So there will be no FETCH stage as parent stage for IXSCAN. (totalDocsExamined = 0)

For index intersection parent stage can be AND_SORTED or AND_HASH.

Sort and Group stage has a additional flag usedDisk, is disk was used.

**Pluses**
- I can use combines indexes to skip documents fetching from the disk;
- I can know if there is collection scan, and add indexes;


[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 9 mar 22
**MongoDB statistics**
Collection stats has following informations:
- storage stats, indexes size, colection size, count documents and other;
- query exec stats, collection scans;
- cache stats, currently in cache, reads, writes;
- compression;
- transactions, update conflicts, rollback info;
- index details;

**Pluses**
- I can create metrics based on collection statistics to identify, caching problems, slow queries and other problems;

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 8 mar 22
**MongoDB deadlocks**
Nothing special found about deadlocks

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 7 mar 22
**MongoDB locks**
Mongo uses multi-granularity locking and global, database or collection level.
It uses reader writer locks.

Intent locks are at the top of the hierarchy and their purpose is to prevent unnecessary checking of low level locks where possible.

For most read and write operations, mongo uses optimistic concurrency control.
When the storage engine detects conflicts, one will incur a write conflict causing retry that operation.

From mongodb 5 find and aggregate queries are lock free.

Locking modes.

Shared - shared between multiple readers, in read operations.
Exclusive - resource not available for concurrent readers, in write operations.
Intent shared - indicates that the lock holder will read the resource at a granular level.
Intent exclusive - lock holder will modify the resource at a granular level.

Parallel transaction making changes to one field, for one transaction there can be a error
"Plan executor error during findAndModify :: caused by :: WriteConflict error"

**Pluses**
- I can use lock free methods from version 5 mongodb, to increase my apps perfomance;
- I can use atomic operations such as inc and others, to prevent race conditions problems;

**Minuses**
- Sometimes i need to use optimistic concurreny by using replace documents without atomic operations;
- I need know and handle writeconflict exception then using transactions;

https://github.com/zolotarevandrew/databases/blob/main/mongodb/locks.js

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 4 mar 22
**MongoDB views**
Views can be useful:
- For remove joining collection inside app;
- add computed fields or metrics;
- exclude some sensitive data;

Materialized views:
Aggregation pipeline results can be merge into collection.
Data can be replaced every time.


https://github.com/zolotarevandrew/databases/blob/main/mongodb/views.js

**Pluses**
- I can use views for some security reasons, to exclude payment cards, roles and other;
- I can use materialized views to store some frequent slow queries;

**Minuses**
- I had to store aggregation inside my applications to use materialized views;

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 3 mar 22
**MongoDB aggregations**
Aggregation pipeline consists of one or more stages.
- Each stage perform an operation on the input docs.
- Outputs are passes to then next stage;

The query planner analyzes an aggregation pipeline to determine if indexes can be used to improve pipeline performance.

Operators which can use indexes:
- match, sort, group,

https://github.com/zolotarevandrew/databases/blob/main/mongodb/aggregations.js


[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 2 mar 22
**MongoDB relations**
In mongo related data can be embed in a single document (denormalized model).

Embedded data:
- Contains relationship;
- Can update data in single atomic operation;
- Better performance for read operations;

Subset data - separate collection which contains additional, less frequently-accessed data.

Relations:
- One to one, embedded documents can be used or subset collection;
- One to many, embedded documents can be used or subset collection or references (you can also store top ten elements which needed to query);
- Many to many, embedded documents or references.

For refs lookup operator act as left outer join.

**Pluses**
- I will prefer using embedded documents in my apps, to increase apps perfomance;
- I can use subset of data, to store more needed information in separate collections;

https://github.com/zolotarevandrew/databases/blob/main/mongodb/relations.js

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 1 mar 22
**MongoDB Indexes**
Mongo indexes has sort order, but it is needed only for sorting operations in compound indexes.

Multikey indexes is used for content stored in array, it creates separate index entries for every element of the array.

Text indexes - can be only one per collection, so it should be compound text index for some scenarios. 
They.

Index properties:
- ttl indexes - can be used to automatically remove documents from a collection;
- unique indexes - can't contain duplicate values;
- partial indexes - filter expression indexes, has lower storage requirements and reduced performance costs for index creation and maintenance;
- hidden indexes - hide index from a planner;

**Pluses**
- I can use ttl indexes for some event logs collections;
- I can use partial indexes to increase some queries perfomances;

https://github.com/zolotarevandrew/databases/blob/main/mongodb/indexes.js

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 28 feb 22
**MongoDB Transactions**
From mongo 4.2 transactions can be used across multiple shards.
From mongodb 4.4 collections and indexes can be made inside transactions.

For starting one or more transaction we needed to start a session.

Read concern - by default it uses local concern.

Local read concern - returns the most recent data available from the node but can be rolled back;
Available read concern - no guarantee that the data has been written to a majority of the replica set members can getting orphaned documents;
Majority read concern - data that has been acknowledged by a majority of the replica set members;
Snapshot read concern - data from a snapshot of majority committed data;

Needs to test on cluster with replicas.

https://github.com/zolotarevandrew/databases/blob/main/mongodb/transactions_simple.js

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 25 feb 22
**Postgres connection pooling**
Postgres spawns new process on each connection and it's can be more than 2mb, 
there could be a memory problem because of a lot connections.

Connection pooling, like a thread pooling, solves this problem.

There are two popular types of poolers:
- pgbouncer
- pgpool

pgbouncer has three modes:
- session pooling (default), after client disconnects.
- transaction pooling, after transaction finishes;
- statement pooling, after statement finishes.

session - long living;
transaction - living based on transactions;
statement - short living based on statements;

npgsql - .net postgresl provider has also in memory connection pooling, (100 default).

I tried disable npgsql pooling, it a little bit reduced throghput (10% inserts);

```
SELECT count(*) FROM pg_stat_activity where query <> '<insufficient privilege>'
```
Interesting thing, there was only 30 connections pgbouncer used, even if a run more than 1000 virtual users load test.


**Pluses**
- I can use correct connection pooling by pgbouncer, for increasing my databases performance;
- I can change pooling setting in my npgsql driver, to increase my application and database perfomance and throughput;

**Minuses**
- Session pooling is the default method;

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 23 feb 22
**Postgres ef optimistic concurrency**
ef core has a simple mechanism for optimistic concurrency, i simply used rowVersion row.

**Minuses**
- Optimistic concurrency requires user to retry request;

https://github.com/zolotarevandrew/databases/tree/main/postgresql/ef-core/WebApi

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 22 feb 22
**Postgres ef core relations**
Ef core has a good mechanism for db first approach.
I have tested all relations approaches (one to one, one to many, many to many)

https://github.com/zolotarevandrew/databases/tree/main/postgresql/ef-core/BaseRelations

**Minuses**
- I should always check, which sql query ef core produces, because it can be inefficient;

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 21 feb 22
**Postgres merge joins and sorting**
merge join works only on joins where results ordered by sort condition key.

Merge join uses only one iteration for each set of internal and external rows.
It uses two pointers for internal and external rows because rows sorted.

Sort types:
- Quick sort, when rows set can be placed in  the memory;
- Top n Heap sort, when rows are limited;
- Merge sort, when rows can't be placed in the memory;

Merge sort - readed rows sorted in memory by quick sort and write to temp file, until all rows readed;

Unique values and grouping:
- distinct fields can get easily by one loop if rows are sorted;

**Pluses**
- I can write better queries with join, because i can decide which type of join database will choose, based on my query. 

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 18 feb 22
**Postgres hash joins**
Base idea is to use hash table to get correct rows.

First stage:
- sequantially read internal rows, and foreach row hash functions is calculated;
- hash key is join condition;
It is effectively working if all rows can be in ram.

Second stage:
- for each external rows, we searching internal row in a hash table by join key.
- all found data return as results;

Two-way hashing:
- on planning stage db calculating if internal rows can fit within the given memory.
- internal rows splits by on separate batches;
- batch count is powers of two.

First stage:
- reading internal rows and bulding hash table;
- if rows is within a batch it goes to a temp file, not to a ram;

Second stage:
- reading external rows;
- all external rows splitted as in a first stage, by temp files if its needed.
- start matching rows in memory;
- start matching internal and external batches;

Parallel hashing:
the hash table is not created in the local memory of the process, but in the shared dynamically allocated memory. So all parallel processes can access needed data.

- Creating hash table in parallel;
- Match external rows in parallel;

Hash join can be used in all join types, but only if join condition is equality operator;


**Pluses**
- I should get only needed fields, when my queries uses hash join, because all rows  goes to ram;


[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 17 feb 22
**Postgres nested loop joins**

nested loop:
first loop called (external) and second loop called (internal).
each found pair returns immediatly as part of result.

for cartesian product:
Nested Loop
-> Seq Scan (external set)
−> Materialize (internal set)
Materialize also uses seq scan, but upon repeated access, the node reads the previously memorized rows.

In general, the total cost of the join:
- cost of getting all external rows;
- cost of one time getting all internal rows;
- N-1 cost of repeatedly getting internal rows (N external rows);
- cost of processing each row;

Postgresql 14 has memoize node, which can cache internal rows.

Nested loop can be used in left join, but can't be used in full and right joins, 
because some rows will not be viewed.

Antijoin - returns first set rows, if there is no match for them in another set. 
(can be represented as left join with where is null).

Semijoin - return first set rows, if there is at least one match for them in the second set.

Nested loop allows parallel mode, where external nodes can be paralllel

https://github.com/zolotarevandrew/databases/blob/main/postgresql/joins/simple.sql

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
##16 feb 22
**Postgres relations**
one to one
one to many
many to many
just remembered

https://github.com/zolotarevandrew/databases/tree/main/postgresql/relations

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 15 feb 22
**Postgres mvcc**
Each relation (table, view, index and other) has a multiple forks.
Every fork has a type and it's own data.

Fork can grow up to 1gb, and then new fork will be created.

So tables, indexes more than 1gb would be stored in separate files.
Each file splitted by 8 kb pages.

Fork types:
- Initialization, exists only for UNLOGGED tables;
- Free space map, where the presence of empty space in the pages is marked, when new row is added space reduced, when cleaning started in expands
It used for adding new rows, so db should fast find place to add data.
- Visibility map, one bit marks pages that contain only actual versions of rows;

Pages:
- Header;
- Ref array to row versions (for indexes);
- empty spaces;
- row versions;

Each version of a rows should be inside one page, but if it exceeds page size, TOAST will be used.
TOAST (The Oversized Attributes Storage Technique).
TOASTR it's a separate table with it's own indexes.
If table has text or numeric field, TOAST table will be created.

https://github.com/zolotarevandrew/databases/tree/main/postgresql/mvcc

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 14 feb 22
**Db Other anomalies**

Inconsistent read:
First transaction update data, then second transaction read data, first transaction update data, then second transaction read data.
So the last read for the second transaction will be incosistent, we can solve this by sum agrregation, because we have correct state when transaction starts.

The problem can appear by functions volatility classification (VOLATILE, STABLE, IMMUTABLE), by default function has a VOLATILE behaviour.
If we calling volatile function that has a select statement inside other select operator, it can read incosistent data. So we can solve it by STABLE keyword.
(Should learn more about this)

Inconsistent write:
we have a rule to have sum by accounts greater than zero, we start two repeatable read transaction and remove amount from two separate accounts and the problem comes in.

https://github.com/zolotarevandrew/databases/blob/main/postgresql/acid/inconsistent_read.sql
https://github.com/zolotarevandrew/databases/blob/main/postgresql/acid/inconsistent_read_by_update.sql
https://github.com/zolotarevandrew/databases/blob/main/postgresql/acid/inconsistent_read_volatile_functions.sql
https://github.com/zolotarevandrew/databases/blob/main/postgresql/acid/repeatable_read_incosistent_write.sql

**Pluses**
- I can prevent some incosistent read anomalies in my projects, which use postgresql;
- I can prevent some incosistent write anomalies in my projects, which use postgresql;

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 11 feb 22
**Db MVCC/Locks**
Snapshot isolation - isolation protocol based on snapshots.
In postgresql snapshot isolation is multiversion based. 
Db can have multiple versions of the same row.
In fact only changing the same row is blocked.
Writing transaction doesn't block reading transactions.
Reading transactions doesn't block anything.

By using snapshots in postgresql it also resolve phantom read anomalia.

Solutions to some isolation problems:
- don't write code:) just use constraints;
- INSERT/UPDATE/DELETE ON CONFLICT;
- LOCK SELECT FOR UPDATE;

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 10 feb 22
**Db locks**
Exclusive lock - no one can read or change data, if some thread entered in exclusive lock.
Shared lock - lock for value, so nobody can change it inside a transaction.

Postgres has a row level lock by - FOR UPDATE statement.

I should always test parallel transactions.
So for example first transaction update somethins gets an exclusive lock, and other transaction blocked
until first transaction not committed. So postgres can refresh update statement before commit and see what row changed,
but he can also  not call refreshing the row if some fields were indexed.

https://github.com/zolotarevandrew/databases/tree/main/postgresql/locks

**Pluses**
- I can solve double/booking and some other problems, using exclusive locks in postgres;

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 9 feb 22
**Db B trees/B+ trees**
To fund a row in a large table we perform full table scan. Reading large tables is slow.
Requires many I/O to read all pages.

B tree element has a key and value. The value is usually data pointer to the row.
Data pointer point to primary key or tuple.
A node = disk page.

B tree limitations:
- Store keys and values;
- Internal node take more space this require more IO and can slow traversal;
- Range queries problem 1-5;

B+ trees:
- Store keys in internal nodes;
- Values stored in leaf nodes;
- Internal node are smaller because store only keys;
- Leaf nodes are linked;
- Great for range queries;


*OS*
Critical sections:
- Forbid interruptions, because cpu changes process by timers or other interruptions (it works only on systems which has one cpu);
- Active waiting, spin lock, consumes cpu time, can used for a fast locking;
- Peterson algorithm - cycle plus bool variables;
- TSL - cpu blocks memory bus then executing instruction;
- Semaphores - has up and down methods, up increases counter, down decreases counter, this should be atomic operations;
- Mutex - exclusive lock can use TSL and thread yield instructions to free some threads.


[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 8 feb 22
**Postgres statistics**

Query Executing process:
- creating connection (maybe using connection pool);
- parsing query, split to tokens;
- building a tree, going to rewrite system;
- using planner/optimizer;
- the more table you have the more ways to execute the query;
- for each operation estimates number of rows and execution time;
- executor comes in after getting the plan 

Number of rows or pages recalculated by:
- auto-vacuum
- auto-analyze
- created index, or some DDL operation was executed (CREATE, ALTER, DROP, TRUNCATE, COMMENT, RENAME)

inserted + updated + deleted > threshold = run autoanalyze
```threshold = analyze_threshold + reltuples(pgclass)*analyze factor```
analyze_threshold - default 50, can be changed by ALTER TABLE;
analyze_factor - default 0.1

pg_stats has most_common_vals and most_common_freqs values.
if there is a lot of frequent values, we can exclude this data from index.

ANALYZE command updates statistics in pg_statistics catalog, like update statistics in MSSQL.

https://github.com/zolotarevandrew/databases/blob/main/postgresql/statistics/statistics.sql

**Pluses**
- I can optimize my tables by sometimes changing default analyze_threshold or factor, when table grows very fast;
- I can exclude some data from index, to optimize search speed and size of the index.
- I will prefer to use jsonb over json in postgres, because postgres don't store statistics for json fields.

[Log Index]
----------------------------------------------------------

----------------------------------------------------------
## 7 feb 22
**OS**
Interrupt process:
- driver send command to controller
- Controller send interrupt signal when read or write was finished
- If interrupt controller ready to receive interruption, he send signal to cpu
- Interrupt controller send device number to bus
- Then cpu ready command counter goes to stack and then goes to kernel mode
cpu can block and continue interruption.

Processes:
Unix process has
- Text segment - program
- Data segment - variables
- Stack segment

Processes in unix has hierarchy structure and init process is on top.
Windows has no hierarchy, and child process just has parent descriptor.

Process state:
- executing, using cpu
- ready, working but stopped, for executing other process
- blocked until some event will happen
Os has table of processes like array or list.
Os has a vector of interruptions, for each device it store a address for execution procedure.
Cpu time = 1 - p^n
Where n - count of processes


*Db views*

Simple view:
- just a virtual table;
- updated every time then a table updated;
- slow processing;
- not pre-computed;
- no storage space needed;

Materialized view:
- physical copy of a table;
- updated manually or using triggers;
- fast processing;


**Pluses**:
- I will use materialized views for some statistical reports, whicn can be updated by analytics of by cache miss.
- I will use views for some frequently used queries with complex logic;


[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 4 feb 22
**Db and Data**

each row has a additionall unique id, for postgres it is tuple_id, pair (block number, tuple index).

```
select *, ctid from sales p ;
```

*Page*
- Database doesn't read a single row, it reads a page or more in a single IO and we get a lot of rows in that IO;
- Each page has a size (8KB in postgres), (8KB in MSSQL).

*I/O*
- request to the disk;
- can't read a single row;
- can goes to OS cache.

*Heap*
- data structure where the table is stored with all its pages;
- traversing expensive;
- that's why we need indexes that help tell us exactly that part of the heap we need to read.

*Index*
- data structure, which has pointers to the heap.
- has a part of data;
- one data was found, you go to the heap to get additional data;
- also store as pages and cost IO;

Learnt about explain analyze and primary and secondary indexes.
https://github.com/zolotarevandrew/databases/blob/main/postgresql/indexes/explain_analyze.sql
https://github.com/zolotarevandrew/databases/blob/main/postgresql/indexes/simple_table.sql


**Pluses**
- I can use indexes with include statements, to prevent index scan and use index only scan.
- I can create index on two or more fields, for better using where statement with AND operator;
- I can use explain, to change by indexes structure;
- I can create index concurrently, which won't block the table inserts.

**OS**

Registers - 1ns, 1kb
Cache - 2ns, 4 mb
Ram - 10ns, 1gb+
Disk - 10 ms, 100gb+

Ram splits up to cache strings by 64 bytes.
Often used strings goes to cpu cache.
It takes two tacts to get data from cache.
If string does not exist in cache, we should go to ram so it is more time expensive.
Modern Cpus has two levels of cache - l1, l2.
Intel uses shared cache, amd uses cache for each core.
I/O devices has a controllers.
Controllers has a different interfaces, thats why drivers were created. Manufacturers are creating drivers for each supported OS.
Os can load drivers dynamically like for usb or load them only then os starts.


[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 3 feb 22
**ACID**
Played some examples about ACID, serializable, phantom reads and atomicity;

https://github.com/zolotarevandrew/databases/blob/main/postgresql/acid/atomicity.sql
https://github.com/zolotarevandrew/databases/blob/main/postgresql/acid/phantom_reads.sql
https://github.com/zolotarevandrew/databases/blob/main/postgresql/acid/repeatable_read.sql
https://github.com/zolotarevandrew/databases/blob/main/postgresql/acid/serializable.sql

Interesting thing postgres resolved phantom reads even in repeatable read isolation level.

**OS**
Os can load tasks from disk to free memory space - spooling.
*Cpu*:
It is a computer brain. He chooses commands from memory and execute them. 
Selecting command, decode to find type and operands, executing.
Each cpu has its own commands, so x86 commands cant be executed on ARM. 
Registers intended for storing variables and intermediate result.
Other registers available for programmers:
- Command counter - stores memory address with next executing command;
- Stack pointer - ref to stack top;
- Program status word - bits for cond operators, bits for cpu priority,  kernel and user mode bits and other;
 Modern cpus can execute more than one command at time by using conveyors. 

Also we have superscalar cpus, which has different executing blocks (logical operators, integers).
As a result commands executing in different order


**Pluses**
- I can build better apis, knowing the facts of read replicas are eventual consistent.
- I can use correct isolation levels, to prevent some read phenomenas in my applications.

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 2 feb 22
**ACID**

Transaction is a collection of queries, one unit of work, which implements all or nothing rule.
Transactions can be readonly to guarantee you are getting consistent snapshot of data.

*Atomicity* - all queries in a transaction must succeed. 
If one query fails, all prior successful in the transaction should rollback;
If the database went down prior to a commit, all the successful queries in the transaction should rollback;
 
*Isolation* - transaction see changes made by other transactions (even running transactions concurrently).
Read phonomenas:
- Dirty reads, if you wrote something, and it is not really committed, but you see it in your transactions.
- Non-repeatable reads, then you read value, then select sum, which also has a prev readed value, and value was changed during the transaction.
- Phantom reads - when you reads aggregated data, there can be new row inserted.
- Lost updates - when you write and other transaction changes that row, then you will see lost update.

*Dirty reads*:
|PID | qnt | price   |
|:--:|:---:|:-------:|
| P1 | 15  | 5       |
| P2 | 20  | 4       |
```
select pid, qnt*price from sales
p1, 50
p2, 80

then concurrently
update sales set qnt = qnt + 5 where pid = 1
but not commit

select sum(qnt*price) from sales

we get $155, when it should be $130, we read a dirty value
```

*Non repeatable read*:
same table as in dirty reads.
```
select pid, qnt*price from sales
p1, 50
p2, 80

then concurrently

update sales set qnt = qnt + 5 where pid = 1
commit


select sum(qnt*price) from sales

we get $155, when it should be $130, we got non repeatable read, because parallel transaction was committed.
```

*Phantom read*:
|PID | qnt | price   |
|:--:|:---:|:-------:|
| P1 | 10  | 5       |
| P2 | 20  | 4       |
```
select pid, qnt*price from sales
p1, 50
p2, 80

then concurrently
insert into sales('Product3', 10, 1)
and commit

select sum(qnt*price) from sales

we get $140, when it should be $130, we had a phantom read.
```

*Lost updates*
|PID | qnt | price   |
|:--:|:---:|:-------:|
| P1 | 10  | 5       |
| P2 | 20  | 4       |
```
update sales set qnt = qnt + 10 where pid = 1

then concurrently
update sales set qnt = qnt + 5 where pid = 1
and commit
so qnt will be 15, not 20.

select sum(qnt*price) from sales

we get $155, when it should be $180, we had a lost update.
```

Isolation levels:
Read uncommitted - no isolation.
Read committed - only see committed changes by other transactions.
Repeatable read - transaction will make sure than when a query reads a row,
that row will remain unchanged the transaction while it's running.
Snapshot - each query see changes that committed up to the start of the transaction.
Serializable - each transactions run one by one, no concurrency.

Isolation levels vs read phenomena:

|Level              | Dirty | Lost upd | Non repeatable | Phantom |
|:-----------------:|:-----:|:--------:|:--------------:|:-------:|
| Read uncommitted  | +     |  +       | +              | +       |
| Read committed    | -     | +        | +              | +       |
| Repeatable read   | -     | -        | -              | +       |
| Serializable      | -     | -        | -              | -       |


*Consistency* - in reads, in data.
In data:
- defined by user;
- foreign keys;
incorrect isolation level can lead to incosistent data.
replicas leads to eventual consistency.

*Durability* - changes made by committed transactions must be persisted in a durable storage.
Techniques:
- write ahead log (OS cache problem, can be possible data loss).
- Async snapshots.
- Append only file.

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 1 feb 22
**.net Exceptions**
Managed exceptions in .NET are implemented on top of the Win32 structured exception handling mechanism.
SEH has two mechanisms, exception handlers and termination handlers.
Nothing interesting was found about exceptions.

**net ThreadPool**
Thread pool has a global task queue and each thread has its own queue.
Then thread queue is empty, it can get tasks from other thread, but its requires synchronization mechanism.

If there is no tasks in all queues, thread goes to sleeping mode. If it lasts long, thread awakes and self destructs.

If some thread locked by sync mechanism, thread pool will create new thread.

https://github.com/zolotarevandrew/.net-internals/blob/main/Concurrent/ThreadPoolInternals.cs

**Pluses**:
- Another point to avoid locks if possible in my applications, so threads count will be stable (gc will be faster);

**Minuses**:
- Locks increases threads in thread pool.

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 31 Jan 22
**.net Threads**
Each thread:
Thread kernel object - os creates internal structure
Thread environment block - address space created in user mode. Local storage for thread. It is only 4 kbs. 
User mode stack - local args and variables, by default 1 mb.
Kernel mode stack - for safety when code goes to kernel mode, all data copied from user mode stack.
Before thread created - windows load each dll by dll main.

Windows executed threads  with highest priority fiest. So there can be thread starvation.
Priority calculated by process priority and thread priority.

There are two types of threads in clr: foreground and background.
Thread poll threads are background.

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 28 Jan 22
**.net GC**
GC modes: 
- low latency mode, which ignores 2-nd generation.
- batch, disables parallel execution, for maximum throughput;
- interactive, enabled parallel execution;

GC:
- pause all threads;
- Marking - Go through all heap roots that has objects ref and mark them By Sync block index, so marked objects will stay in heap;
- Compacting - objects which stays at heap moving to near memory blocks;

**Finalization**
If object type has finalizator, before calling ctor, object ref goes to finalization list.
Clr has high priority thread for finalization.
When object added to finalizaition queue this thread awakes.
Object that has finalizers lives more than 2 gcs.

**Minuses**:
- Should investigate each mode by concrete app types;


[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 27 Jan 22
**.net GC**
.net GC uses two strategies. 
First, by size - small and large object heap.
Compact - squeeze heap, objects should be copied and will be compacted.
Sweep - use free parts of the heap.
SOH < 85000 bytes, LOH > 85000 bytes.
SOH uses compacting and sweep. LOH uses only sweep.

Because a lot of objects less than 85000, they goes to SOH and that's why generations came.
0 gen - time between creation and nearest GC.
1 gen - relatively long living objects;
2 gen - long living objects;


https://www.youtube.com/watch?v=DVnmGW6964o&t=429s&ab_channel=%D0%9C%D0%B8%D0%BD%D0%B8-%D0%BA%D0%BE%D0%BD%D1%84%D0%B5%D1%80%D0%B5%D0%BD%D1%86%D0%B8%D0%B8CLRium

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 26 Jan 22
**.net SpinWait**
SpinWait internally uses Thread.Yield and Thread.Sleep methods and CLR internall Thread.SpinWait.
Thread.Yield will interrupt the current thread to allow other threads to do work.
Spinning should only be attempted when you have good guarantee thad doing so is more efficient than a context switch,
because is hust utilizes CPU time.

**.net SpinLock**
Microsoft says that SpinLock should only be used for improving performance. It is good for quick locks.

**.net Interlocked**


**Pluses**:
- I can implement some lock free algorithms and structures by using SpinLock and SpinWait.
- I can use Interlocked class for async code, which has stateful data using by many threads.

**Minuses**:
- I think i will rarely use SpinWait in my apps, but it is good to know structure.
- SpinLock internally uses while cycle, so it utilizes CPU time, that is not good.
- Can't use spinlock with async/await because it is exlusive kind of lock.

https://github.com/zolotarevandrew/.net-internals/blob/main/Concurrent/SpinLockInternals.cs
https://github.com/zolotarevandrew/.net-internals/blob/main/Concurrent/InterlockedInternals.cs

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 25 Jan 22
**.net CountdownEvent**
Internally it just uses ManualResetEventSlim. On each signal it just uses Interlocked.Decrement, 
when count goes to zero, it calls Set method of ManualResetEventSlim.
It is have wait method which support cancellation token.

https://github.com/zolotarevandrew/.net-internals/blob/main/Concurrent/CountDownEventInternals.cs


Minuses:
- Simply i can use await Task.WhenAll for simple fork/join scenarios;
- Main thread start hanging, when task completed with exception, Task.WhenAll solves this problem..;

**.net Barrier**:
Internally it also uses ManualResetEventSlim. I think it is very useful primitive, to split jobs between multiple tasks(threads).
It is also has post callback when all jobs has done.

**Pluses**:
- I can use Barrier in cases then jobs count are determined in execution time and can be decreased or increased.

**Minuses**:
- Sometimes simple tasks approach can be a better solution, should do some benchmarks;

https://github.com/zolotarevandrew/.net-internals/blob/main/Concurrent/BarrierInternals.cs

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 24 Jan 22
**.net ReaderWriterLockSlim**
Unfortunately readerwriterlock has no support for async/await statements.
So i found Microsoft.Extensions.Threading which has Async implementation for reader writer lock, but internally it uses lock.

https://github.com/zolotarevandrew/.net-internals/blob/main/Concurrent/AsyncReaderWriterLockSlimInternals.cs
https://github.com/zolotarevandrew/.net-internals/blob/main/Concurrent/ReaderWriterLockSlimInternals.cs

**Minuses**:
- I don't think i will use readerwritelock slim, because of i always using async/await.
- I should compare benchmarks with simple lock and Microsoft.Extensions.Threading.AsyncReaderWriterLock.
- I should learn more about implementation details of Microsoft.Extensions.Threading.AsyncReaderWriterLock.
- WTF when used same code with tasks and AsyncReaderWriterLock, my main thread starts hanging.
- I think there should better option for case a lot of reads and single write, and maybe simple lock would be the case.

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 22 Jan 22
**.net strings**
in .net5/6 has optimized equality == operator, which uses spans and it is more faster than Equals method, richter book was outdated:)
In benchmarks slowest was compareTo method with current culture param. So i am going to try avoid this comparisons.

.net 6 has improved interpolated string by DefaultInterpolatedStringHandler, so it is more memory efficient than stringbuilder.
https://github.com/zolotarevandrew/.net-internals/blob/main/Strings/StringComparisonBenchmarks.cs
https://github.com/zolotarevandrew/.net-internals/blob/main/Strings/StringCreationBenchmarks.cs

**Pluses**:
- I should avoid strings comparison with Culture for increase my app perfomance;
- I should use string interpolation instead of strinbguiler for lesser memory allocations in my apps (.net5/6).
- I should use string concat method, because of value types can be boxed, for lesser memory allocations in my apps.



[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 21 Jan 22
**.net EventWaitHandle**
AutoResetEvent and ManualResetEvent just using EventWaitHandle with some enum and nothing else.
It's good thread signaling mechanism.

AutoResetEvent works as exclusive lock, so only one thread can get access to resource at point of time. 
Then thread job has done, we can call set to signal one or more thread, so they can start work.

ManualResetEvent works differently from autoresetevent. In signaled state it allow all threads to execute some work.
ManualResetEventSlim has the cancellation token support. And it has spinning with Monitor implementation.


**Pluses**:
- I can use AutoResetEvent for concurrent operations, if only one thread can get access for a point of time (write to file of something).
- I am going to ManualResetEventSlim for sort of thread signaling operations, because of it has cancellation tokens support.

**Minuses**:
- Should learn more about sleeping state of thread, and how it affect on performance.

https://github.com/zolotarevandrew/.net-internals/blob/main/Concurrent/AutoResetEventInternals.cs
https://github.com/zolotarevandrew/.net-internals/blob/main/Concurrent/ManualResetEventInternals.cs
https://github.com/zolotarevandrew/.net-internals/blob/main/Concurrent/ManualResetEventSlimInternals.cs

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 20 Jan 22
**.net Mutex**
I should learn more about implementation of mutex, but a lot of resources says about it uses kernel mode.
And it so expensive to use. We can use named mutex for inteprocess communication.
The main problem with mutex with async/await code. It is because async continuation can be executed by other thread.
But in mutex release should be called by thread, which called waitone. And this is the problem.

https://github.com/zolotarevandrew/.net-internals/blob/main/Concurrent/MutexInternals.cs

**.net SemaphoreSlim**
Internal implementation of SemaphoreSlim uses lock and has methods for async code.
Should investigate more about that.

https://github.com/zolotarevandrew/.net-internals/blob/main/Concurrent/SemaphoreSlimInternals.cs

**Pluses**:
- I can use SemaphoreSlim as my mutex, semaphore for async/await methods.

**Minuses**:
- As because i always using async/await i will not use Mutex, because of its problems with async code.

To my surpise, creating SemaphoreSlim executed longer than creating Semaphore. Should investigate more about that.

[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 19 Jan 22
**.net Monitor**
Watched CLRium video about monitor - 
https://www.youtube.com/watch?v=GkWDcsVHh0M&ab_channel=%D0%9C%D0%B8%D0%BD%D0%B8-%D0%BA%D0%BE%D0%BD%D1%84%D0%B5%D1%80%D0%B5%D0%BD%D1%86%D0%B8%D0%B8CLRium

Monitor.TryEnter works only in os user mode and can use spin lock.
Thread.Sleep uses timer, thread awakes with higher priority.

Monitor.Enter first tries to spinwait. In worst case kernel mode lock allocated.
After allocating we use kernel lock, waiting for specified timeout.
Then it tries use spinwait to get a lock. 
Thread can awakes before timeout elapsed, so it uses while true cycle and reduces timeout on each iteration.

**Pluses**:
- I can reduce locks duration to increase the throughput of my backends;
- I can reduce threads waiting time in kernel mode by timeouts, to increase the throughput of my backends;

**Minuses**:
- I  Need to study the Monitor in more detail, for better use in practice.


[Log Index]
----------------------------------------------------------
----------------------------------------------------------
## 18 Jan 22
**.net concurrent dictionary**
internal classes are sealed, because of performance improvements.
all nodes stored in wrapper class Table, which has array of locks and nodes.
Node has next pointer, so it is implemented in linked list manner.
Table variable uses volatile keyword. Should learn more about this keyword.
Has the bool static variable isValueWriteAtomic, which checks is generic type a value type.

On add tables and locks copied in local variables, because of other threads can change size of this arrays.
Using locks array helps different threads, change different buckets. 
Size of locks array can be changed by concurrency level in constructor.
Index is returned along with lockNo - index in array of locks.
After got index, exclusive lock acquired by lockNo.

It is interesting to use an array of locks so that more threads can do more work.

https://github.com/zolotarevandrew/.net-internals/blob/main/DataStructuresInternals/ConcurrentDictionaryInternal.cs

**.net concurrent stack**
Internal classes are also sealed.
Node has next pointer, so it is implemented in linked list manner.
Has a pointer to head Node, variable uses volatile keyword.
Push and pop simply implemented with using spinwait and compare exchange swap.
So it is lock free implementation.

https://github.com/zolotarevandrew/.net-internals/blob/main/DataStructuresInternals/ConcurrentStack.cs

**Pluses**:
- I can build effective concurrent cache, or pool of locks by objects;
- I can use technique array of locks, to build more responsive apps;
- I can implement simple lock free implementation for some data structures using learnt techniques;

**Minuses**:
- I didn't quite get use cases for volatile keyword in concurrent environment;

**.ElasticSearch**
Yesterday's problem with term frequency solved by changing index options.
I deployed current solution and we should give this first version to clients.


[Log Index]
----------------------------------------------------------
## 17 Jan 22
**.ElasticSearch**

Edge n gram has issues with exact matches in my analyzer setup.

So when I typed: “Surf”, it is not returned exact match “surf” with highest score.

- "SURFPRIZE, SURFPRIZE, SURFRIZE, SURF, PRIZE, SURFRIZE, RIZE"
- "SURF COFFEE SURFING NEVER ALONE"
- "КАЙТВИНД, КАЙТВИНД, КАЙТ, ВИНД, СЁРФИНГ, MYSURF, SURF, SURFLIFE, MYSURFLIFE"
- "SURFJAZZ, СЁРФДЖАЗ, SURF, JAZZ, СЁРФ, ДЖАЗ"
- "SURF'N'FRIES ORIGINAL NATURAL ORIGINAL FRIES"

So i learnt about similarity models and why it is happened.

Default model - tf/idf. 
Position of words in document are not used, doc is just a bag of words.
Tf - It uses logarithm term frequency.
Idf - total number of docs divided by the number docs contains the word. It also uses logarithm.

But default model in elasticsearch is BM25.
It is probabilistic model and it also uses tf and idf.

Found this:
"Indeed, this is due to the fact that the ngram FILTER writes terms at the
same position (like synonyms) while the TOKENIZER generates a stream of
tokens which have consecutive positions. This gives blablablafoobarbarbar a
larger number of positions and thus a smaller length normalization."

So after changing from filter to tokenizer, problem with scores was solved. 
Also was used multi_match query with most_fields. Most_fields - "useful when querying multiple fields that contain the same text analyzed in different ways". 
For that i added multi_field with standard analyzer to my text field.
But there is problem with space-ignoring search.

Found this good approach, which uses keyword tokenizer.

https://medium.com/@davedash/writing-a-space-ignoring-autocompleter-with-elasticsearch-6c3c28e3a974

But is also has a problem with score (then text contains search term twice or higher).

**.net Priority Queue**
Elements is array of tuples (struct) with Value and its priority.
Uses standard grow factors, starts with 4, then multiplying by 2.
Uses standard heapify algorithm (non-recursive). Parent and child indexes calculated by bitwise operators (index - 1 >> 2).
Uses tuple because it allows simple swap without using third variable. (nodes[nodeIndex] = tuple).

https://github.com/zolotarevandrew/.net-internals/blob/main/DataStructuresInternals/PriorityQueueInternal.cs

**.net Object pool**
New feature, very cool that you don't have to write from scratch.
Has a thread safe implementation, using Interlocked.CompareExchange.
Always stores first item.
Elements stored in array of objectwrapper struct. (// PERF: the struct wrapper avoids array-covariance-checks from the runtime when assigning to elements of the array.)
Learnt about array covariance checks.
https://codeblog.jonskeet.uk/2013/06/22/array-covariance-not-just-ugly-but-slow-too/

https://github.com/zolotarevandrew/.net-internals/blob/main/DataStructuresInternals/DefaultObjectPoolInternal.cs

**Pluses**:
- I can build simple thread safe object pool using implemented default object pool;
- I can build custom search analyzers for elastic search for a better user experience;
- I can build custom search queries for elastic search for a better user experience;

**Minuses**:
- I can't use PriorityQueue's in concurrent environment;

[Log Index]
----------------------------------------------------------
----------------------------------------------------------

## 15 Jan 22
**.net**

**Events:**
If event delegate used in multithread scenarios, there can be some problems.  

(Good use case, should learn more about memory barriers)

- event handler can be copied to temp var by ref, but compiler can remove this var;
- So it is better to use memory barrier and call Volatile.Read. Processor can not move calling delegate before assign to variable;

For registering and removing event handlers, compilers add methods add, remove, which using interlocked.compareexchange for thread safe swap, old and new handler.

**Generics.**

Each type has object type. 

CLR optimizes generated IL code to not overgrow, Generic that has a ref type can use same code.
But its not true for value types.

[Log Index]
----------------------------------------------------------
----------------------------------------------------------

## 14 Jan 22
**.net**

Default parameters stores in metadata, so compiler can add them at execution.
params always allocate some space in heap, so better not to use it.

**ElasticSearch:**

Finally implemented search, still dont understand why edge n gram did not solve the problem for contains queries.
Instead wilcard queries with boost was used.

surf coff - (surf* and coff*)^3 or (surf*)^2 or (coff*)^1.

So the problem was with docs in elastic search, they don't put this "Sometimes, though, it can make sense to use a different analyzer at search time, such as when using the edge_ngram tokenizer for autocomplete or when using search-time synonyms."
into edge n gram docs. 

I just changed search_analyzer to standard. And my query became simple - "query": "surf coff".
So now search became faster, because of using index correctly.


**.net stack**

has a simple array implementation.
Capacity started from 4, and then multiply by 2.

https://github.com/zolotarevandrew/.net-internals/blob/main/DataStructuresInternals/StackInternal.cs

**.net queue**

has a simple array implementation.
has _head and _tail index.
Capacity started from 4, and then multiply by 2.
On enqueue, they increment _tail my method with ref index parameter. 
Has a comment (JIT produces better code than with ternary operator ?:) - i think it is  branch to target operator, should learn more about this.

https://github.com/zolotarevandrew/.net-internals/blob/main/DataStructuresInternals/QueueInternal.cs

**.net list**

has a simple array implementation.
Capacity started from 4, and then multiply by 2.
Interesting, they avoid the extra generic instantiation for Array.Empty<T>() - by static instantiation of new T[0];

https://github.com/zolotarevandrew/.net-internals/blob/main/DataStructuresInternals/ListInternal.cs

[Log Index]
----------------------------------------------------------
----------------------------------------------------------

## 13 Jan 22

**.net hashtable**

Hashtable was implemented a long ago. It is because bucket key and value has object type.
Bucket is struct.
Then there was no generics, and struct type was boxed.
It is using double hashing technique - h(key, n) = h1(key) + n*h2(key).
h1 - can be any number, h2 - can be only from to table size.

They called func to get hash - InitHash, wtf ?:) 
Because of Bucket implemented as struct.

Was used load factor (0.72 by perfomance tests).
Was used _occupancy - total number of collision bits set.

InitHash - has two out parameters, for h1 and h2. h2 using only then there is collision.

For contains key, buckets copied in stack (Take a snapshot of buckets, in case another thread resizes table)

Insert - Has a lot of strange conditions because of optimizations and collision bits.

Has a lot of complex details, worth a look later. 

https://github.com/zolotarevandrew/.net-internals/blob/main/DataStructuresInternals/HashTableInternal.cs


**.net dictionary**
Has a bucket and entry arrays.
Bucket array stores indexes, and entry array stores entries.
Entry struct - hash code, key, value, next - next entry in chain.
Size Expanding uses prime number close to count.
For insertions entries copied in new stack frame (same as for hastable case another thread resizes table).
Collision resolves by chaining, because of entry has a next field.

https://github.com/zolotarevandrew/.net-internals/blob/main/DataStructuresInternals/DictionaryInternal.cs

**Pluses**:
- I can use technique to copy my data structs in new stack frame for concurrent environments;
- I can use dictionary more smart, to reduce the use of extra memory;

**Minuses**:
- I am not going to use HashSet, because it is outdated;

[Log Index]
----------------------------------------------------------
----------------------------------------------------------

## 12 Jan 22

**.net expressions**

Interesting topic. But looks very unusual and unreadable.
I have never used even once in production.

Read more about this in enterpise development - https://habr.com/ru/company/jugru/blog/423891/
It is good that expressions way more faster than Activator and other Reflection tools for creating objects.

Should warmup expression because of slow compilation (start from 300 milliseconds).

Should learn more about projections by expressions (how it works in automapper and other providers).

Topic not interesting to me, for now.

https://github.com/zolotarevandrew/.net-internals/blob/main/Expressions/Program.cs

[Log Index]
----------------------------------------------------------
----------------------------------------------------------

## 11 Jan 22

**Docker:**

Looking for a problem - resource temporarily unavailable, cant fork process.

So the problem was with the docker container and chrome in it.

The docker container has 512 default pids. But my container created over 1000, because of chrome driver starting by each request.
After execution stops, a lot of pids, more than 500 just hang.  

Tested on local docker.

So should never span a lot of processes inside container. Knew it, but just thought it would be okay.

Instead selenoid was used and problem was solved.

**c# records, structs, classes**:

record is just wrapper around class, has some additional methods, and implements iequatable.
record created like this: record MyRecord(string Name) - has a deconstruct method, and init only setters.

simple struct has no equality operator ==, should only use Equals (no boxing).
record struct implements IEquatable and has equality operator ==, useful for DDD ValueObjects.

operator with for structs - copy struct and changes values.
operator with for classes, records - copy class by Clone Method and changes values.

readonly ref record struct - useful for immutable DDD ValueObjects.
ref struct - lives only stack, useful for low optimizations.

https://github.com/zolotarevandrew/.net-internals/tree/main/ClassesStructsRecords

**.net**

IL has call and callvirt methods. callvirt called polymorphically.

Sealed class cant use inheritance. So compiler can optimize virtual methods, such as ToString.

**Pluses**:
- I can use sealed .net classes to improve my applications performance;
- I can use readonly record structs as my DDD Value objects.
- I can use ref structs to create low-level optimizations.

**Minuses**:
- I will never use docker container to create a lot of processes inside of it;

[Log Index]
----------------------------------------------------------
----------------------------------------------------------

## 10 Jan 22

Should learn more about LayoutKind for struct.

**Struct Boxing mechanism:**
- Memory allocated in heap;
- Memory size = length of struct + object type ref + sync block indes;
- Fields copied to heap;
- Ref for object returned.

**Struct unboxing mechanism:**
- get object ref;
- copy fields from heap to stack.

String concat uses a objects in args - so should be very attentive to writing code with boxing.

GetType - method from object, so value type be boxed.

Сasting to interface - value type be boxed.

Using ILDasm/sharplab.io to find boxing - good solution.

https://github.com/zolotarevandrew/.net-internals/blob/main/BoxingUnboxing/Program.cs

**ElasticSearch:**

Found and used word delimeter graph token filter, which splits tokens by non alphanumeric characters and by other good stuff.
for example: favorit—42+ to favorit, 42

Found and used shingle filter to produce n grams by concatenating.
for example:  ka - favorit, favoritka, ka.

Used wildcard search for contains string filter:
(*Query*) OR (Query*) OR (Query)

**.net LinkedList internal structure**

list is Doubly linked circular list, node has next and prev ref.

has mutable and immutable methods.

adding, deleting, searching - nothing special.

https://github.com/zolotarevandrew/.net-internals/blob/main/DataStructuresInternals/LinkedListInternal.cs


**.net SortedList internal structure**

Default capacity - zero. Then 4, then multiplied by 2.

Contains arrays of keys and values. Keys are unique.

Adding is done using a binary search. BinarySearch gives index to insert.

Binary search has interesting implementation with bitwise operators.


https://github.com/zolotarevandrew/.net-internals/blob/main/DataStructuresInternals/SortedListInternal.cs

**.net HashSet internal structure**

Has slots array (hashcode, next entry index and value) and array of indexes.
Hash code using get hash code and bitwise and operator to store lower 31 bits of hash code. (need learn more)

Contains - gets hash code and iterate over slots.next items until found equal value.

Remove - same approach like contains.

Add - add value if its not present.

Additionally has a variable m_freeList, for fast insertion into free part of array.

Has method trim excess - Sets the capacity of this list to the size of the list (rounded up to nearest prime)

https://github.com/zolotarevandrew/.net-internals/blob/main/DataStructuresInternals/HashSetInternal.cs

**Pluses**:
- I can create more optimized classes under .net;
- I can search unnecessary boxing by ILDasm, to improve memory allocations.
- I can create search with ignoring spaces by elastic search shingle token filter.

[Log Index]

----------------------------------------------------------
----------------------------------------------------------

## 09 Jan 22

prologue code - set ups the stack frame of the called function.

epilogue code - restore the stack frame of calling parent function.

Another point to learn assembler.

**CLR simple method execution**
M1()
string name
M2(name)
- Process thread gets a 1 mb size stack;
- Name address pushed to stack - m1 local var;
- S address pushed to stack - m2 name parameter;
- Return address to M1 pushed to stack;
- M2 local variables pushed to stack;
- M2 code executed;
- Write return address into cpu instruction pointer.

**CLR method execution with objects.**
We have Employee and Manager class.
M3()
Employee e = new Manager()
e = Employee.Lookup()
e.GetProgressReport

**Then JIT stage comes**
- Found all types which M3 has;
- CLR load all assemblies for found types;
- Using metadata CLR creates objects types in heap;
- Each object type has - sync block index, object type ref, static fields, methods table. M3 compiles and then execution starts;
- Employee object created in heap;
- Object has sync block index, object type ref, bytes for fields and bytes for base type fields;
- Employee ref pushed to stack - variable e.

**Calling static method lookup**
- Search object type by ref;
- Search entrypoint to method by method table.

**Calling virtual method GetProgressReport**
- using variable go to address;
- search object type by ref;
- Search entrypoint to method by method table.

Object types has ref to System.Type which CLR creates at startup.
System.Type object type too and it refers to itself.

compiler has flag /checked+ prevent number overflows.

![clrexecution](https://user-images.githubusercontent.com/49956820/148688808-8a5cc92c-e986-4678-b532-cb259c82584b.png)

[Log Index]

----------------------------------------------------------

## 08 Jan 22

Put my github in order

[Log Index]

----------------------------------------------------------

[Log Index]: https://github.com/zolotarevandrew/my_learning_tracker/blob/main/log-index.md#log-index